{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mHarvey52/com4513_assignment/blob/main/BaselineGoogleColab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install conda with python 3.6"
      ],
      "metadata": {
        "id": "_IAjeUCRzbr0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%env PYTHONPATH = # /env/python\n",
        "!conda config --set ssl_verify no\n",
        "!wget https://repo.anaconda.com/miniconda/Miniconda3-4.3.21-Linux-x86_64.sh\n",
        "!chmod +x Miniconda3-4.3.21-Linux-x86_64.sh\n",
        "!./Miniconda3-4.3.21-Linux-x86_64.sh -b -f -p /usr/local\n",
        "!conda update conda"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "febSuVik8Rue",
        "outputId": "8dc19bc4-f6ac-4ffc-cf7b-615240222842"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: PYTHONPATH=# /env/python\n",
            "--2024-04-21 21:20:54--  https://repo.anaconda.com/miniconda/Miniconda3-4.3.21-Linux-x86_64.sh\n",
            "Resolving repo.anaconda.com (repo.anaconda.com)... 104.16.32.241, 104.16.191.158, 2606:4700::6810:bf9e, ...\n",
            "Connecting to repo.anaconda.com (repo.anaconda.com)|104.16.32.241|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 35074768 (33M) [application/x-sh]\n",
            "Saving to: ‘Miniconda3-4.3.21-Linux-x86_64.sh.1’\n",
            "\n",
            "Miniconda3-4.3.21-L 100%[===================>]  33.45M  29.1MB/s    in 1.1s    \n",
            "\n",
            "2024-04-21 21:20:56 (29.1 MB/s) - ‘Miniconda3-4.3.21-Linux-x86_64.sh.1’ saved [35074768/35074768]\n",
            "\n",
            "PREFIX=/usr/local\n",
            "installing: python-3.6.1-2 ...\n",
            "installing: asn1crypto-0.22.0-py36_0 ...\n",
            "installing: cffi-1.10.0-py36_0 ...\n",
            "installing: conda-env-2.6.0-0 ...\n",
            "installing: cryptography-1.8.1-py36_0 ...\n",
            "installing: idna-2.5-py36_0 ...\n",
            "installing: libffi-3.2.1-1 ...\n",
            "installing: openssl-1.0.2l-0 ...\n",
            "installing: packaging-16.8-py36_0 ...\n",
            "installing: pycosat-0.6.2-py36_0 ...\n",
            "installing: pycparser-2.17-py36_0 ...\n",
            "installing: pyopenssl-17.0.0-py36_0 ...\n",
            "installing: pyparsing-2.1.4-py36_0 ...\n",
            "installing: readline-6.2-2 ...\n",
            "installing: requests-2.14.2-py36_0 ...\n",
            "installing: ruamel_yaml-0.11.14-py36_1 ...\n",
            "installing: setuptools-27.2.0-py36_0 ...\n",
            "installing: six-1.10.0-py36_0 ...\n",
            "installing: sqlite-3.13.0-0 ...\n",
            "installing: tk-8.5.18-0 ...\n",
            "installing: xz-5.2.2-1 ...\n",
            "installing: yaml-0.1.6-0 ...\n",
            "installing: zlib-1.2.8-3 ...\n",
            "installing: conda-4.3.21-py36_0 ...\n",
            "installing: pip-9.0.1-py36_1 ...\n",
            "installing: wheel-0.29.0-py36_0 ...\n",
            "mkdir: cannot create directory ‘/usr/local/envs’: File exists\n",
            "Python 3.6.1 :: Continuum Analytics, Inc.\n",
            "creating default environment...\n",
            "using -f (force) option\n",
            "installation finished.\n",
            "WARNING:\n",
            "    You currently have a PYTHONPATH environment variable set. This may cause\n",
            "    unexpected behavior when running the Python interpreter in Miniconda3.\n",
            "    For best results, please verify that your PYTHONPATH only points to\n",
            "    directories of packages that are compatible with the Python interpreter\n",
            "    in Miniconda3: /usr/local\n",
            "Fetching package metadata .........\n",
            "Solving package specifications: .\n",
            "\n",
            "Package plan for installation in environment /usr/local:\n",
            "\n",
            "The following packages will be UPDATED:\n",
            "\n",
            "    conda: 4.3.21-py36_0 --> 4.3.30-py36h5d9f9f4_0\n",
            "\n",
            "Proceed ([y]/n)? y\n",
            "\n",
            "conda-4.3.30-p 100% || Time: 0:00:00  10.39 MB/s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('/usr/local/lib/python3.10/site-packages')\n",
        "!conda create -y -n myenv python=3.6"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Tqb2WKv8XoW",
        "outputId": "58989958-2b38-480c-ecd4-39fb7fd92c73"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching package metadata ...........\n",
            "Solving package specifications: .\n",
            "\n",
            "Package plan for installation in environment /usr/local/envs/myenv:\n",
            "\n",
            "The following NEW packages will be INSTALLED:\n",
            "\n",
            "    _libgcc_mutex:    0.1-main                \n",
            "    ca-certificates:  2024.3.11-h06a4308_0    \n",
            "    certifi:          2021.5.30-py36h06a4308_0\n",
            "    ld_impl_linux-64: 2.38-h1181459_1         \n",
            "    libffi:           3.3-he6710b0_2          \n",
            "    libgcc-ng:        9.1.0-hdf63c60_0        \n",
            "    libstdcxx-ng:     9.1.0-hdf63c60_0        \n",
            "    ncurses:          6.3-h7f8727e_2          \n",
            "    openssl:          1.1.1w-h7f8727e_0       \n",
            "    pip:              21.2.2-py36h06a4308_0   \n",
            "    python:           3.6.13-h12debd9_1       \n",
            "    readline:         8.1.2-h7f8727e_1        \n",
            "    setuptools:       58.0.4-py36h06a4308_0   \n",
            "    sqlite:           3.38.5-hc218d9a_0       \n",
            "    tk:               8.6.12-h1ccaba5_0       \n",
            "    wheel:            0.37.1-pyhd3eb1b0_0     \n",
            "    xz:               5.2.5-h7f8727e_1        \n",
            "    zlib:             1.2.12-h7f8727e_2       \n",
            "\n",
            "_libgcc_mutex- 100% || Time: 0:00:00   4.01 MB/s\n",
            "ca-certificate 100% || Time: 0:00:00   7.58 MB/s\n",
            "ld_impl_linux- 100% || Time: 0:00:00  27.91 MB/s\n",
            "libstdcxx-ng-9 100% || Time: 0:00:00  49.96 MB/s\n",
            "libgcc-ng-9.1. 100% || Time: 0:00:00  55.13 MB/s\n",
            "libffi-3.3-he6 100% || Time: 0:00:00  30.99 MB/s\n",
            "ncurses-6.3-h7 100% || Time: 0:00:00  40.84 MB/s\n",
            "openssl-1.1.1w 100% || Time: 0:00:00  25.55 MB/s\n",
            "xz-5.2.5-h7f87 100% || Time: 0:00:00   7.80 MB/s\n",
            "zlib-1.2.12-h7 100% || Time: 0:00:00  46.31 MB/s\n",
            "readline-8.1.2 100% || Time: 0:00:00  48.00 MB/s\n",
            "tk-8.6.12-h1cc 100% || Time: 0:00:00  47.95 MB/s\n",
            "sqlite-3.38.5- 100% || Time: 0:00:00  51.64 MB/s\n",
            "python-3.6.13- 100% || Time: 0:00:00  58.25 MB/s\n",
            "certifi-2021.5 100% || Time: 0:00:00  48.37 MB/s\n",
            "wheel-0.37.1-p 100% || Time: 0:00:00  35.01 MB/s\n",
            "setuptools-58. 100% || Time: 0:00:00  42.79 MB/s\n",
            "pip-21.2.2-py3 100% || Time: 0:00:00  57.93 MB/s\n",
            "#\n",
            "# To activate this environment, use:\n",
            "# > source activate myenv\n",
            "#\n",
            "# To deactivate an active environment, use:\n",
            "# > source deactivate\n",
            "#\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!source activate myenv\n",
        "!conda install -y python=3.6.6"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ax07h7pq8fJN",
        "outputId": "9ef0be54-a5d1-43b9-c602-fbc9e9079362"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching package metadata ...........\n",
            "Solving package specifications: .\n",
            "\n",
            "Package plan for installation in environment /usr/local:\n",
            "\n",
            "The following NEW packages will be INSTALLED:\n",
            "\n",
            "    _libgcc_mutex:          0.1-main                \n",
            "    brotlipy:               0.7.0-py36h27cfd23_1003 \n",
            "    ca-certificates:        2024.3.11-h06a4308_0    \n",
            "    certifi:                2021.5.30-py36h06a4308_0\n",
            "    charset-normalizer:     2.0.4-pyhd3eb1b0_0      \n",
            "    colorama:               0.4.4-pyhd3eb1b0_0      \n",
            "    conda-package-handling: 1.7.3-py36h27cfd23_1    \n",
            "    libedit:                3.1.20210910-h7f8727e_0 \n",
            "    libgcc-ng:              9.1.0-hdf63c60_0        \n",
            "    libstdcxx-ng:           9.1.0-hdf63c60_0        \n",
            "    ncurses:                6.3-h7f8727e_2          \n",
            "    pysocks:                1.7.1-py36h06a4308_0    \n",
            "    tqdm:                   4.63.0-pyhd3eb1b0_0     \n",
            "    urllib3:                1.26.8-pyhd3eb1b0_0     \n",
            "\n",
            "The following packages will be UPDATED:\n",
            "\n",
            "    conda:                  4.3.30-py36h5d9f9f4_0    --> 4.10.3-py36h06a4308_0\n",
            "    conda-env:              2.6.0-0                  --> 2.6.0-1              \n",
            "    openssl:                1.0.2l-0                 --> 1.0.2u-h7b6447c_0    \n",
            "    pycosat:                0.6.2-py36_0             --> 0.6.3-py36h27cfd23_0 \n",
            "    python:                 3.6.1-2                  --> 3.6.6-h6e4f718_2     \n",
            "    readline:               6.2-2                    --> 7.0-h7b6447c_5       \n",
            "    requests:               2.14.2-py36_0            --> 2.27.1-pyhd3eb1b0_0  \n",
            "    setuptools:             27.2.0-py36_0            --> 58.0.4-py36h06a4308_0\n",
            "    sqlite:                 3.13.0-0                 --> 3.33.0-h62c20be_0    \n",
            "    tk:                     8.5.18-0                 --> 8.6.12-h1ccaba5_0    \n",
            "    xz:                     5.2.2-1                  --> 5.2.5-h7f8727e_1     \n",
            "    zlib:                   1.2.8-3                  --> 1.2.12-h7f8727e_2    \n",
            "\n",
            "conda-env-2.6. 100% || Time: 0:00:00   3.36 MB/s\n",
            "openssl-1.0.2u 100% || Time: 0:00:00  26.09 MB/s\n",
            "libedit-3.1.20 100% || Time: 0:00:00  56.23 MB/s\n",
            "readline-7.0-h 100% || Time: 0:00:00   2.76 MB/s\n",
            "sqlite-3.33.0- 100% || Time: 0:00:00   7.42 MB/s\n",
            "python-3.6.6-h 100% || Time: 0:00:00  59.30 MB/s\n",
            "charset-normal 100% || Time: 0:00:00  32.85 MB/s\n",
            "colorama-0.4.4 100% || Time: 0:00:00  19.17 MB/s\n",
            "pycosat-0.6.3- 100% || Time: 0:00:00  38.75 MB/s\n",
            "pysocks-1.7.1- 100% || Time: 0:00:00  35.49 MB/s\n",
            "tqdm-4.63.0-py 100% || Time: 0:00:00  32.09 MB/s\n",
            "brotlipy-0.7.0 100% || Time: 0:00:00   5.29 MB/s\n",
            "conda-package- 100% || Time: 0:00:00   6.42 MB/s\n",
            "urllib3-1.26.8 100% || Time: 0:00:00  48.55 MB/s\n",
            "requests-2.27. 100% || Time: 0:00:00  35.29 MB/s\n",
            "conda-4.10.3-p 100% || Time: 0:00:00  13.08 MB/s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking the python version is 3.6.6\n",
        "!python --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zNTuz0eP8m2Z",
        "outputId": "1035358a-289f-4ab5-925b-18ac3e8696d1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.6.6 :: Anaconda, Inc.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install dependencies"
      ],
      "metadata": {
        "id": "7hrXyop1zsBJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!conda install -y ipykernel"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DB8mkCQS72kX",
        "outputId": "13d32fe9-afdb-4040-e53e-6f4d3e63cf80"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting package metadata (current_repodata.json): - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\bdone\n",
            "Solving environment: - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\bfailed with initial frozen solve. Retrying with flexible solve.\n",
            "Solving environment: - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\bfailed with repodata from current_repodata.json, will retry with next repodata source.\n",
            "Collecting package metadata (repodata.json): / \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\bdone\n",
            "Solving environment: \\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \n",
            "Warning: >10 possible package resolutions (only showing differing packages):\n",
            "  - https://repo.continuum.io/pkgs/main/linux-64/linux-64::conda-4.10.3-py36h06a4308_0, https://repo.continuum.io/pkgs/main/linux-64/linux-64::conda-env-2.6.0-1, https://repo.continuum.io/pkgs/main/linux-64/linux-64::libedit-3.1.20210910-h7f8727e_0, https://repo.continuum.io/pkgs/main/noarch/noarch::colorama-0.4.4-pyhd3eb1b0_0\n",
            "  - defaults/linux-64::conda-4.10.3-py36h06a4308_0, https://repo.continuum.io/pkgs/main/linux-64/linux-64::conda-env-2.6.0-1, https://repo.continuum.io/pkgs/main/linux-64/linux-64::libedit-3.1.20210910-h7f8727e_0, https://repo.continuum.io/pkgs/main/noarch/noarch::colorama-0.4.4-pyhd3eb1b0_0\n",
            "  - defaults/linux-64::conda-env-2.6.0-1, https://repo.continuum.io/pkgs/main/linux-64/linux-64::conda-4.10.3-py36h06a4308_0, https://repo.continuum.io/pkgs/main/linux-64/linux-64::libedit-3.1.20210910-h7f8727e_0, https://repo.continuum.io/pkgs/main/noarch/noarch::colorama-0.4.4-pyhd3eb1b0_0\n",
            "  - defaults/linux-64::conda-4.10.3-py36h06a4308_0, defaults/linux-64::conda-env-2.6.0-1, https://repo.continuum.io/pkgs/main/linux-64/linux-64::libedit-3.1.20210910-h7f8727e_0, https://repo.continuum.io/pkgs/main/noarch/noarch::colorama-0.4.4-pyhd3eb1b0_0\n",
            "  - defaults/linux-64::conda-env-2.6.0-1, defaults/noarch::colorama-0.4.4-pyhd3eb1b0_0, https://repo.continuum.io/pkgs/main/linux-64/linux-64::conda-4.10.3-py36h06a4308_0, https://repo.continuum.io/pkgs/main/linux-64/linux-64::libedit-3.1.20210910-h7f8727e_0\n",
            "  - defaults/linux-64::conda-4.10.3-py36h06a4308_0, defaults/linux-64::conda-env-2.6.0-1, defaults/noarch::colorama-0.4.4-pyhd3eb1b0_0, https://repo.continuum.io/pkgs/main/linux-64/linux-64::libedit-3.1.20210910-h7f8727e_0\n",
            "  - defaults/noarch::colorama-0.4.4-pyhd3eb1b0_0, https://repo.continuum.io/pkgs/main/linux-64/linux-64::conda-4.10.3-py36h06a4308_0, https://repo.continuum.io/pkgs/main/linux-64/linux-64::conda-env-2.6.0-1, https://repo.continuum.io/pkgs/main/linux-64/linux-64::libedit-3.1.20210910-h7f8727e_0\n",
            "  - defaults/linux-64::conda-4.10.3-py36h06a4308_0, defaults/noarch::colorama-0.4.4-pyhd3eb1b0_0, https://repo.continuum.io/pkgs/main/linux-64/linux-64::conda-env-2.6.0-1, https://repo.continuum.io/pkgs/main/linux-64/linux-64::libedit-3.1.20210910-h7f8727e_0\n",
            "  - defaults/linux-64::libedit-3.1.20210910-h7f8727e_0, defaults/noarch::colorama-0.4.4-pyhd3eb1b0_0, https://repo.continuum.io/pkgs/main/linux-64/linux-64::conda-4.10.3-py36h06a4308_0, https://repo.continuum.io/pkgs/main/linux-64/linux-64::conda-env-2.6.0-1\n",
            "  - defaults/linux-64::libedit-3.1.20210910-h7f8727e_0, https://repo.continuum.io/pkgs/main/linux-64/linux-64::conda-4.10.3-py36h06a4308_0, https://repo.continuum.io/pkgs/main/linux-64/linux-64::conda-env-2.6.0-1, https://repo.continuum.io/pkgs/main/noarch/noarch::colorama-0.4.4-pyhd3eb1b0_0\n",
            "  ... and others\b\b| \b\bdone\n",
            "\n",
            "\n",
            "==> WARNING: A newer version of conda exists. <==\n",
            "  current version: 4.10.3\n",
            "  latest version: 24.3.0\n",
            "\n",
            "Please update conda by running\n",
            "\n",
            "    $ conda update -n base conda\n",
            "\n",
            "\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local\n",
            "\n",
            "  added / updated specs:\n",
            "    - ipykernel\n",
            "\n",
            "\n",
            "The following packages will be downloaded:\n",
            "\n",
            "    package                    |            build\n",
            "    ---------------------------|-----------------\n",
            "    backcall-0.2.0             |     pyhd3eb1b0_0          13 KB\n",
            "    decorator-5.1.1            |     pyhd3eb1b0_0          12 KB\n",
            "    entrypoints-0.3            |           py36_0          12 KB\n",
            "    ipykernel-5.3.4            |   py36h5ca1d4c_0         181 KB\n",
            "    ipython-7.16.1             |   py36h5ca1d4c_0         999 KB\n",
            "    ipython_genutils-0.2.0     |     pyhd3eb1b0_1          27 KB\n",
            "    jedi-0.17.0                |           py36_0         780 KB\n",
            "    jupyter_client-7.1.2       |     pyhd3eb1b0_0          93 KB\n",
            "    jupyter_core-4.8.1         |   py36h06a4308_0          74 KB\n",
            "    libsodium-1.0.18           |       h7b6447c_0         244 KB\n",
            "    nest-asyncio-1.5.1         |     pyhd3eb1b0_0          10 KB\n",
            "    parso-0.8.3                |     pyhd3eb1b0_0          70 KB\n",
            "    pexpect-4.8.0              |     pyhd3eb1b0_3          53 KB\n",
            "    pickleshare-0.7.5          |  pyhd3eb1b0_1003          13 KB\n",
            "    prompt-toolkit-3.0.20      |     pyhd3eb1b0_0         259 KB\n",
            "    ptyprocess-0.7.0           |     pyhd3eb1b0_2          17 KB\n",
            "    pygments-2.11.2            |     pyhd3eb1b0_0         759 KB\n",
            "    python-dateutil-2.8.2      |     pyhd3eb1b0_0         233 KB\n",
            "    pyzmq-22.2.1               |   py36h295c915_1         454 KB\n",
            "    tornado-6.1                |   py36h27cfd23_0         581 KB\n",
            "    traitlets-4.3.3            |   py36h06a4308_0         138 KB\n",
            "    wcwidth-0.2.5              |     pyhd3eb1b0_0          26 KB\n",
            "    zeromq-4.3.4               |       h2531618_0         331 KB\n",
            "    ------------------------------------------------------------\n",
            "                                           Total:         5.3 MB\n",
            "\n",
            "The following NEW packages will be INSTALLED:\n",
            "\n",
            "  backcall           pkgs/main/noarch::backcall-0.2.0-pyhd3eb1b0_0\n",
            "  decorator          pkgs/main/noarch::decorator-5.1.1-pyhd3eb1b0_0\n",
            "  entrypoints        pkgs/main/linux-64::entrypoints-0.3-py36_0\n",
            "  ipykernel          pkgs/main/linux-64::ipykernel-5.3.4-py36h5ca1d4c_0\n",
            "  ipython            pkgs/main/linux-64::ipython-7.16.1-py36h5ca1d4c_0\n",
            "  ipython_genutils   pkgs/main/noarch::ipython_genutils-0.2.0-pyhd3eb1b0_1\n",
            "  jedi               pkgs/main/linux-64::jedi-0.17.0-py36_0\n",
            "  jupyter_client     pkgs/main/noarch::jupyter_client-7.1.2-pyhd3eb1b0_0\n",
            "  jupyter_core       pkgs/main/linux-64::jupyter_core-4.8.1-py36h06a4308_0\n",
            "  libsodium          pkgs/main/linux-64::libsodium-1.0.18-h7b6447c_0\n",
            "  nest-asyncio       pkgs/main/noarch::nest-asyncio-1.5.1-pyhd3eb1b0_0\n",
            "  parso              pkgs/main/noarch::parso-0.8.3-pyhd3eb1b0_0\n",
            "  pexpect            pkgs/main/noarch::pexpect-4.8.0-pyhd3eb1b0_3\n",
            "  pickleshare        pkgs/main/noarch::pickleshare-0.7.5-pyhd3eb1b0_1003\n",
            "  prompt-toolkit     pkgs/main/noarch::prompt-toolkit-3.0.20-pyhd3eb1b0_0\n",
            "  ptyprocess         pkgs/main/noarch::ptyprocess-0.7.0-pyhd3eb1b0_2\n",
            "  pygments           pkgs/main/noarch::pygments-2.11.2-pyhd3eb1b0_0\n",
            "  python-dateutil    pkgs/main/noarch::python-dateutil-2.8.2-pyhd3eb1b0_0\n",
            "  pyzmq              pkgs/main/linux-64::pyzmq-22.2.1-py36h295c915_1\n",
            "  tornado            pkgs/main/linux-64::tornado-6.1-py36h27cfd23_0\n",
            "  traitlets          pkgs/main/linux-64::traitlets-4.3.3-py36h06a4308_0\n",
            "  wcwidth            pkgs/main/noarch::wcwidth-0.2.5-pyhd3eb1b0_0\n",
            "  zeromq             pkgs/main/linux-64::zeromq-4.3.4-h2531618_0\n",
            "\n",
            "\n",
            "\n",
            "Downloading and Extracting Packages\n",
            "wcwidth-0.2.5        | 26 KB     | : 100% 1.0/1 [00:00<00:00,  4.79it/s]\n",
            "backcall-0.2.0       | 13 KB     | : 100% 1.0/1 [00:00<00:00, 25.46it/s]\n",
            "ptyprocess-0.7.0     | 17 KB     | : 100% 1.0/1 [00:00<00:00,  5.26it/s]\n",
            "decorator-5.1.1      | 12 KB     | : 100% 1.0/1 [00:00<00:00,  6.06it/s]\n",
            "ipython_genutils-0.2 | 27 KB     | : 100% 1.0/1 [00:00<00:00,  6.15it/s]\n",
            "prompt-toolkit-3.0.2 | 259 KB    | : 100% 1.0/1 [00:00<00:00,  4.97it/s]\n",
            "ipykernel-5.3.4      | 181 KB    | : 100% 1.0/1 [00:00<00:00,  3.13it/s]\n",
            "pickleshare-0.7.5    | 13 KB     | : 100% 1.0/1 [00:00<00:00,  5.32it/s]\n",
            "zeromq-4.3.4         | 331 KB    | : 100% 1.0/1 [00:00<00:00,  5.91it/s]\n",
            "ipython-7.16.1       | 999 KB    | : 100% 1.0/1 [00:00<00:00,  2.41it/s]\n",
            "jupyter_core-4.8.1   | 74 KB     | : 100% 1.0/1 [00:00<00:00,  4.26it/s]\n",
            "traitlets-4.3.3      | 138 KB    | : 100% 1.0/1 [00:00<00:00,  5.93it/s]\n",
            "libsodium-1.0.18     | 244 KB    | : 100% 1.0/1 [00:00<00:00,  5.98it/s]\n",
            "nest-asyncio-1.5.1   | 10 KB     | : 100% 1.0/1 [00:00<00:00,  6.85it/s]\n",
            "tornado-6.1          | 581 KB    | : 100% 1.0/1 [00:00<00:00,  5.34it/s]\n",
            "pygments-2.11.2      | 759 KB    | : 100% 1.0/1 [00:00<00:00,  4.63it/s]\n",
            "python-dateutil-2.8. | 233 KB    | : 100% 1.0/1 [00:00<00:00,  6.30it/s]\n",
            "jupyter_client-7.1.2 | 93 KB     | : 100% 1.0/1 [00:00<00:00,  6.18it/s]\n",
            "parso-0.8.3          | 70 KB     | : 100% 1.0/1 [00:00<00:00,  6.78it/s]\n",
            "entrypoints-0.3      | 12 KB     | : 100% 1.0/1 [00:00<00:00,  4.66it/s]\n",
            "pexpect-4.8.0        | 53 KB     | : 100% 1.0/1 [00:00<00:00,  6.44it/s]\n",
            "pyzmq-22.2.1         | 454 KB    | : 100% 1.0/1 [00:00<00:00,  3.28it/s]\n",
            "jedi-0.17.0          | 780 KB    | : 100% 1.0/1 [00:00<00:00,  2.29it/s]\n",
            "Preparing transaction: - \b\b\\ \b\bdone\n",
            "Verifying transaction: / \b\b- \b\b\\ \b\b| \b\bdone\n",
            "Executing transaction: - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\bdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers==2.10.0\n",
        "!pip install protobuf==3.6.0|\n",
        "!pip install pytorch-lightning==0.8.1\n",
        "!pip install torch==1.5.1\n",
        "!pip install bert-score==0.3.3\n",
        "!pip install sacrebleu==1.4.12\n",
        "!pip install moverscore==1.0.3\n",
        "!pip install pyemd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MpdxduO7_eSe",
        "outputId": "301602a4-f864-4ab6-a181-f9b19fd13a1b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers==2.10.0\n",
            "  Downloading https://files.pythonhosted.org/packages/12/b5/ac41e3e95205ebf53439e4dd087c58e9fd371fd8e3724f2b9b4cdb8282e5/transformers-2.10.0-py3-none-any.whl (660kB)\n",
            "\u001b[K    100% |████████████████████████████████| 665kB 1.6MB/s \n",
            "\u001b[?25hCollecting tokenizers==0.7.0 (from transformers==2.10.0)\n",
            "  Downloading https://files.pythonhosted.org/packages/14/e5/a26eb4716523808bb0a799fcfdceb6ebf77a18169d9591b2f46a9adb87d9/tokenizers-0.7.0-cp36-cp36m-manylinux1_x86_64.whl (3.8MB)\n",
            "\u001b[K    100% |████████████████████████████████| 3.8MB 373kB/s \n",
            "\u001b[?25hCollecting regex!=2019.12.17 (from transformers==2.10.0)\n",
            "  Downloading https://files.pythonhosted.org/packages/fd/75/ed64d258199c5ea90be0b6f772a7e3d408e794f54236afa0f28e1db384d6/regex-2023.8.8-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (678kB)\n",
            "\u001b[K    100% |████████████████████████████████| 686kB 2.1MB/s \n",
            "\u001b[?25hCollecting dataclasses; python_version < \"3.7\" (from transformers==2.10.0)\n",
            "  Downloading https://files.pythonhosted.org/packages/fe/ca/75fac5856ab5cfa51bbbcefa250182e50441074fdc3f803f6e76451fab43/dataclasses-0.8-py3-none-any.whl\n",
            "Collecting sentencepiece (from transformers==2.10.0)\n",
            "  Downloading https://files.pythonhosted.org/packages/c9/d2/b9c7ca067c26d8ff085d252c89b5f69609ca93fb85a00ede95f4857865d4/sentencepiece-0.2.0.tar.gz (2.6MB)\n",
            "\u001b[K    100% |████████████████████████████████| 2.6MB 520kB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/site-packages (from transformers==2.10.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/site-packages (from transformers==2.10.0)\n",
            "Collecting numpy (from transformers==2.10.0)\n",
            "  Downloading https://files.pythonhosted.org/packages/45/b2/6c7545bb7a38754d63048c7696804a0d947328125d81bf12beaa692c3ae3/numpy-1.19.5-cp36-cp36m-manylinux1_x86_64.whl (13.4MB)\n",
            "\u001b[K    100% |████████████████████████████████| 13.4MB 100kB/s \n",
            "\u001b[?25hCollecting sacremoses (from transformers==2.10.0)\n",
            "  Downloading https://files.pythonhosted.org/packages/28/78/fef8d089db5b97546fd6d1ff2e813b8544e85670bf3a8c378c9d0250b98d/sacremoses-0.0.53.tar.gz (880kB)\n",
            "\u001b[K    100% |████████████████████████████████| 880kB 2.0MB/s \n",
            "\u001b[?25hCollecting filelock (from transformers==2.10.0)\n",
            "  Downloading https://files.pythonhosted.org/packages/84/ce/8916d10ef537f3f3b046843255f9799504aa41862bfa87844b9bdc5361cd/filelock-3.4.1-py3-none-any.whl\n",
            "Collecting importlib-resources; python_version < \"3.7\" (from tqdm>=4.27->transformers==2.10.0)\n",
            "  Downloading https://files.pythonhosted.org/packages/24/1b/33e489669a94da3ef4562938cd306e8fa915e13939d7b8277cb5569cb405/importlib_resources-5.4.0-py3-none-any.whl\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/site-packages (from requests->transformers==2.10.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.6/site-packages (from requests->transformers==2.10.0)\n",
            "Requirement already satisfied: idna<4,>=2.5; python_version >= \"3\" in /usr/local/lib/python3.6/site-packages (from requests->transformers==2.10.0)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0; python_version >= \"3\" in /usr/local/lib/python3.6/site-packages (from requests->transformers==2.10.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/site-packages (from sacremoses->transformers==2.10.0)\n",
            "Collecting click (from sacremoses->transformers==2.10.0)\n",
            "  Downloading https://files.pythonhosted.org/packages/4a/a8/0b2ced25639fb20cc1c9784de90a8c25f9504a7f18cd8b5397bd61696d7d/click-8.0.4-py3-none-any.whl (97kB)\n",
            "\u001b[K    100% |████████████████████████████████| 102kB 11.7MB/s \n",
            "\u001b[?25hCollecting joblib (from sacremoses->transformers==2.10.0)\n",
            "  Downloading https://files.pythonhosted.org/packages/7c/91/d3ba0401e62d7e42816bc7d97b82d19c95c164b3e149a87c0a1c026a735e/joblib-1.1.1-py2.py3-none-any.whl (309kB)\n",
            "\u001b[K    100% |████████████████████████████████| 317kB 4.4MB/s \n",
            "\u001b[?25hCollecting zipp>=3.1.0; python_version < \"3.10\" (from importlib-resources; python_version < \"3.7\"->tqdm>=4.27->transformers==2.10.0)\n",
            "  Downloading https://files.pythonhosted.org/packages/bd/df/d4a4974a3e3957fd1c1fa3082366d7fff6e428ddb55f074bf64876f8e8ad/zipp-3.6.0-py3-none-any.whl\n",
            "Collecting importlib-metadata; python_version < \"3.8\" (from click->sacremoses->transformers==2.10.0)\n",
            "  Downloading https://files.pythonhosted.org/packages/a0/a1/b153a0a4caf7a7e3f15c2cd56c7702e2cf3d89b1b359d1f1c5e59d68f4ce/importlib_metadata-4.8.3-py3-none-any.whl\n",
            "Collecting typing-extensions>=3.6.4; python_version < \"3.8\" (from importlib-metadata; python_version < \"3.8\"->click->sacremoses->transformers==2.10.0)\n",
            "  Downloading https://files.pythonhosted.org/packages/45/6b/44f7f8f1e110027cf88956b59f2fad776cca7e1704396d043f89effd3a0e/typing_extensions-4.1.1-py3-none-any.whl\n",
            "Building wheels for collected packages: sentencepiece, sacremoses\n",
            "  Running setup.py bdist_wheel for sentencepiece ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/e3/52/7c/94e4e19ae162d72e59e6346cd544ecb767c95751826e841365\n",
            "  Running setup.py bdist_wheel for sacremoses ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/56/d5/b2/bc878b2bbddfbcc8fd62ca73c4fd842bd28c1fd3dbdf424c74\n",
            "Successfully built sentencepiece sacremoses\n",
            "Installing collected packages: tokenizers, regex, dataclasses, sentencepiece, numpy, zipp, typing-extensions, importlib-metadata, click, joblib, sacremoses, filelock, transformers, importlib-resources\n",
            "Successfully installed click-8.0.4 dataclasses-0.8 filelock-3.4.1 importlib-metadata-4.8.3 importlib-resources-5.4.0 joblib-1.1.1 numpy-1.19.5 regex-2023.8.8 sacremoses-0.0.53 sentencepiece-0.2.0 tokenizers-0.7.0 transformers-2.10.0 typing-extensions-4.1.1 zipp-3.6.0\n",
            "/bin/bash: -c: line 2: syntax error: unexpected end of file\n",
            "Collecting pytorch-lightning==0.8.1\n",
            "  Downloading https://files.pythonhosted.org/packages/68/8e/d1bb6f3696aaed40bf8263c0d9be95593dbc6a63921cca68f0e7f60e7893/pytorch_lightning-0.8.1-py3-none-any.whl (293kB)\n",
            "\u001b[K    100% |████████████████████████████████| 296kB 3.8MB/s \n",
            "\u001b[?25hCollecting torch>=1.3 (from pytorch-lightning==0.8.1)\n",
            "  Downloading https://files.pythonhosted.org/packages/a4/54/81b1c3c574a1ffde54b0c82ed2a37d81395709cdd5f50e59970aeed5d95e/torch-1.10.2-cp36-cp36m-manylinux1_x86_64.whl (881.9MB)\n",
            "\u001b[K    100% |████████████████████████████████| 881.9MB 1.4kB/s \n",
            "\u001b[?25hCollecting PyYAML>=5.1 (from pytorch-lightning==0.8.1)\n",
            "  Downloading https://files.pythonhosted.org/packages/cd/e5/af35f7ea75cf72f2cd079c95ee16797de7cd71f29ea7c68ae5ce7be1eda0/PyYAML-6.0.1.tar.gz (125kB)\n",
            "\u001b[K    100% |████████████████████████████████| 133kB 6.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.6/site-packages (from pytorch-lightning==0.8.1)\n",
            "Collecting future>=0.17.1 (from pytorch-lightning==0.8.1)\n",
            "  Downloading https://files.pythonhosted.org/packages/da/71/ae30dadffc90b9006d77af76b393cb9dfbfc9629f339fc1574a1c52e6806/future-1.0.0-py3-none-any.whl (491kB)\n",
            "\u001b[K    100% |████████████████████████████████| 491kB 2.7MB/s \n",
            "\u001b[?25hCollecting tensorboard>=1.14 (from pytorch-lightning==0.8.1)\n",
            "  Downloading https://files.pythonhosted.org/packages/80/49/a5ec29886ef823718c8ae54ed0b3ad7e19066b5bf21cec5038427e6a04c4/tensorboard-2.10.1-py3-none-any.whl (5.9MB)\n",
            "\u001b[K    100% |████████████████████████████████| 5.9MB 226kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.6/site-packages (from pytorch-lightning==0.8.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/site-packages (from torch>=1.3->pytorch-lightning==0.8.1)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/site-packages (from torch>=1.3->pytorch-lightning==0.8.1)\n",
            "Requirement already satisfied: importlib-resources; python_version < \"3.7\" in /usr/local/lib/python3.6/site-packages (from tqdm>=4.41.0->pytorch-lightning==0.8.1)\n",
            "Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard>=1.14->pytorch-lightning==0.8.1)\n",
            "  Downloading https://files.pythonhosted.org/packages/b1/0e/0636cc1448a7abc444fb1b3a63655e294e0d2d49092dc3de05241be6d43c/google_auth_oauthlib-0.4.6-py2.py3-none-any.whl\n",
            "Collecting tensorboard-data-server<0.7.0,>=0.6.0 (from tensorboard>=1.14->pytorch-lightning==0.8.1)\n",
            "  Downloading https://files.pythonhosted.org/packages/74/69/5747a957f95e2e1d252ca41476ae40ce79d70d38151d2e494feb7722860c/tensorboard_data_server-0.6.1-py3-none-any.whl\n",
            "Collecting absl-py>=0.4 (from tensorboard>=1.14->pytorch-lightning==0.8.1)\n",
            "  Downloading https://files.pythonhosted.org/packages/dd/87/de5c32fa1b1c6c3305d576e299801d8655c175ca9557019906247b994331/absl_py-1.4.0-py3-none-any.whl (126kB)\n",
            "\u001b[K    100% |████████████████████████████████| 133kB 9.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/site-packages (from tensorboard>=1.14->pytorch-lightning==0.8.1)\n",
            "Collecting markdown>=2.6.8 (from tensorboard>=1.14->pytorch-lightning==0.8.1)\n",
            "  Downloading https://files.pythonhosted.org/packages/f3/df/ca72f352e15b6f8ce32b74af029f1189abffb906f7c137501ffe69c98a65/Markdown-3.3.7-py3-none-any.whl (97kB)\n",
            "\u001b[K    100% |████████████████████████████████| 102kB 11.9MB/s \n",
            "\u001b[?25hCollecting grpcio>=1.24.3 (from tensorboard>=1.14->pytorch-lightning==0.8.1)\n",
            "  Downloading https://files.pythonhosted.org/packages/81/9a/6b33e8d15850356772f0ee6489bc8346a7aa90f0c86733283e139740865e/grpcio-1.48.2.tar.gz (22.0MB)\n",
            "\u001b[K    100% |████████████████████████████████| 22.0MB 63kB/s \n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/site-packages (from tensorboard>=1.14->pytorch-lightning==0.8.1)\n",
            "Collecting werkzeug>=1.0.1 (from tensorboard>=1.14->pytorch-lightning==0.8.1)\n",
            "  Downloading https://files.pythonhosted.org/packages/f4/f3/22afbdb20cc4654b10c98043414a14057cd27fdba9d4ae61cea596000ba2/Werkzeug-2.0.3-py3-none-any.whl (289kB)\n",
            "\u001b[K    100% |████████████████████████████████| 296kB 3.3MB/s \n",
            "\u001b[?25hCollecting protobuf<3.20,>=3.9.2 (from tensorboard>=1.14->pytorch-lightning==0.8.1)\n",
            "  Downloading https://files.pythonhosted.org/packages/32/27/1141a8232723dcb10a595cc0ce4321dcbbd5215300bf4acfc142343205bf/protobuf-3.19.6-py2.py3-none-any.whl (162kB)\n",
            "\u001b[K    100% |████████████████████████████████| 163kB 5.5MB/s \n",
            "\u001b[?25hCollecting tensorboard-plugin-wit>=1.6.0 (from tensorboard>=1.14->pytorch-lightning==0.8.1)\n",
            "  Downloading https://files.pythonhosted.org/packages/e0/68/e8ecfac5dd594b676c23a7f07ea34c197d7d69b3313afdf8ac1b0a9905a2/tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781kB)\n",
            "\u001b[K    100% |████████████████████████████████| 788kB 1.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/site-packages (from tensorboard>=1.14->pytorch-lightning==0.8.1)\n",
            "Collecting google-auth<3,>=1.6.3 (from tensorboard>=1.14->pytorch-lightning==0.8.1)\n",
            "  Downloading https://files.pythonhosted.org/packages/9c/8d/bff87fc722553a5691d8514da5523c23547f3894189ba03b57592e37bdc2/google_auth-2.22.0-py2.py3-none-any.whl (181kB)\n",
            "\u001b[K    100% |████████████████████████████████| 184kB 7.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: zipp>=3.1.0; python_version < \"3.10\" in /usr/local/lib/python3.6/site-packages (from importlib-resources; python_version < \"3.7\"->tqdm>=4.41.0->pytorch-lightning==0.8.1)\n",
            "Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.14->pytorch-lightning==0.8.1)\n",
            "  Downloading https://files.pythonhosted.org/packages/3b/5d/63d4ae3b9daea098d5d6f5da83984853c1bbacd5dc826764b249fe119d24/requests_oauthlib-2.0.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: importlib-metadata>=4.4; python_version < \"3.10\" in /usr/local/lib/python3.6/site-packages (from markdown>=2.6.8->tensorboard>=1.14->pytorch-lightning==0.8.1)\n",
            "Requirement already satisfied: six>=1.5.2 in /usr/local/lib/python3.6/site-packages (from grpcio>=1.24.3->tensorboard>=1.14->pytorch-lightning==0.8.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard>=1.14->pytorch-lightning==0.8.1)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0; python_version >= \"3\" in /usr/local/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard>=1.14->pytorch-lightning==0.8.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard>=1.14->pytorch-lightning==0.8.1)\n",
            "Requirement already satisfied: idna<4,>=2.5; python_version >= \"3\" in /usr/local/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard>=1.14->pytorch-lightning==0.8.1)\n",
            "Collecting cachetools<6.0,>=2.0.0 (from google-auth<3,>=1.6.3->tensorboard>=1.14->pytorch-lightning==0.8.1)\n",
            "  Downloading https://files.pythonhosted.org/packages/ea/c1/4740af52db75e6dbdd57fc7e9478439815bbac549c1c05881be27d19a17d/cachetools-4.2.4-py3-none-any.whl\n",
            "Collecting pyasn1-modules>=0.2.1 (from google-auth<3,>=1.6.3->tensorboard>=1.14->pytorch-lightning==0.8.1)\n",
            "  Downloading https://files.pythonhosted.org/packages/cd/8e/bea464350e1b8c6ed0da3a312659cb648804a08af6cacc6435867f74f8bd/pyasn1_modules-0.3.0-py2.py3-none-any.whl (181kB)\n",
            "\u001b[K    100% |████████████████████████████████| 184kB 7.5MB/s \n",
            "\u001b[?25hCollecting rsa<5,>=3.1.4 (from google-auth<3,>=1.6.3->tensorboard>=1.14->pytorch-lightning==0.8.1)\n",
            "  Downloading https://files.pythonhosted.org/packages/49/97/fa78e3d2f65c02c8e1268b9aba606569fe97f6c8f7c2d74394553347c145/rsa-4.9-py3-none-any.whl\n",
            "Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.14->pytorch-lightning==0.8.1)\n",
            "  Downloading https://files.pythonhosted.org/packages/7e/80/cab10959dc1faead58dc8384a781dfbf93cb4d33d50988f7a69f1b7c9bbe/oauthlib-3.2.2-py3-none-any.whl (151kB)\n",
            "\u001b[K    100% |████████████████████████████████| 153kB 8.9MB/s \n",
            "\u001b[?25hCollecting pyasn1<0.6.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=1.14->pytorch-lightning==0.8.1)\n",
            "  Downloading https://files.pythonhosted.org/packages/d1/75/4686d2872bf2fc0b37917cbc8bbf0dd3a5cdb0990799be1b9cbf1e1eb733/pyasn1-0.5.1-py2.py3-none-any.whl (84kB)\n",
            "\u001b[K    100% |████████████████████████████████| 92kB 9.2MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: PyYAML, grpcio\n",
            "  Running setup.py bdist_wheel for PyYAML ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/e6/c6/ef/4e8ee93f1b79fc90562f1600d47189799f8213023d9dadafa2\n",
            "  Running setup.py bdist_wheel for grpcio ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/71/8a/35/35a0c3898eb788c68c9ec81b41b840f7165375ede46eb3e5cc\n",
            "Successfully built PyYAML grpcio\n",
            "Installing collected packages: torch, PyYAML, future, cachetools, pyasn1, pyasn1-modules, rsa, google-auth, oauthlib, requests-oauthlib, google-auth-oauthlib, tensorboard-data-server, absl-py, markdown, grpcio, werkzeug, protobuf, tensorboard-plugin-wit, tensorboard, pytorch-lightning\n",
            "Successfully installed PyYAML-6.0.1 absl-py-1.4.0 cachetools-4.2.4 future-1.0.0 google-auth-2.22.0 google-auth-oauthlib-0.4.6 grpcio-1.48.2 markdown-3.3.7 oauthlib-3.2.2 protobuf-3.19.6 pyasn1-0.5.1 pyasn1-modules-0.3.0 pytorch-lightning-0.8.1 requests-oauthlib-2.0.0 rsa-4.9 tensorboard-2.10.1 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 torch-1.10.2 werkzeug-2.0.3\n",
            "Collecting torch==1.5.1\n",
            "  Downloading https://files.pythonhosted.org/packages/62/01/457b49d790b6c4b9720e6f9dbbb617692f6ce8afdaadf425c055c41a7416/torch-1.5.1-cp36-cp36m-manylinux1_x86_64.whl (753.2MB)\n",
            "\u001b[K    100% |████████████████████████████████| 753.2MB 1.5kB/s \n",
            "\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.6/site-packages (from torch==1.5.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/site-packages (from torch==1.5.1)\n",
            "Installing collected packages: torch\n",
            "  Found existing installation: torch 1.10.2\n",
            "    Uninstalling torch-1.10.2:\n",
            "      Successfully uninstalled torch-1.10.2\n",
            "Successfully installed torch-1.5.1\n",
            "Collecting bert-score==0.3.3\n",
            "  Downloading https://files.pythonhosted.org/packages/95/66/a6210213e3f8e5d52f665d35a435310c11e6d69229b4eb90022e89f3e5d2/bert_score-0.3.3-py3-none-any.whl (52kB)\n",
            "\u001b[K    100% |████████████████████████████████| 61kB 5.2MB/s \n",
            "\u001b[?25hCollecting pandas>=1.0.1 (from bert-score==0.3.3)\n",
            "  Downloading https://files.pythonhosted.org/packages/c3/e2/00cacecafbab071c787019f00ad84ca3185952f6bb9bca9550ed83870d4d/pandas-1.1.5-cp36-cp36m-manylinux1_x86_64.whl (9.5MB)\n",
            "\u001b[K    100% |████████████████████████████████| 9.5MB 147kB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/site-packages (from bert-score==0.3.3)\n",
            "Requirement already satisfied: tqdm>=4.31.1 in /usr/local/lib/python3.6/site-packages (from bert-score==0.3.3)\n",
            "Requirement already satisfied: transformers>=2.2.0 in /usr/local/lib/python3.6/site-packages (from bert-score==0.3.3)\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.6/site-packages (from bert-score==0.3.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/site-packages (from bert-score==0.3.3)\n",
            "Collecting matplotlib (from bert-score==0.3.3)\n",
            "  Downloading https://files.pythonhosted.org/packages/09/03/b7b30fa81cb687d1178e085d0f01111ceaea3bf81f9330c937fb6f6c8ca0/matplotlib-3.3.4-cp36-cp36m-manylinux1_x86_64.whl (11.5MB)\n",
            "\u001b[K    100% |████████████████████████████████| 11.5MB 117kB/s \n",
            "\u001b[?25hCollecting pytz>=2017.2 (from pandas>=1.0.1->bert-score==0.3.3)\n",
            "  Downloading https://files.pythonhosted.org/packages/9c/3d/a121f284241f08268b21359bd425f7d4825cffc5ac5cd0e1b3d82ffd2b10/pytz-2024.1-py2.py3-none-any.whl (505kB)\n",
            "\u001b[K    100% |████████████████████████████████| 512kB 3.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/site-packages (from pandas>=1.0.1->bert-score==0.3.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/site-packages (from requests->bert-score==0.3.3)\n",
            "Requirement already satisfied: idna<4,>=2.5; python_version >= \"3\" in /usr/local/lib/python3.6/site-packages (from requests->bert-score==0.3.3)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0; python_version >= \"3\" in /usr/local/lib/python3.6/site-packages (from requests->bert-score==0.3.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.6/site-packages (from requests->bert-score==0.3.3)\n",
            "Requirement already satisfied: importlib-resources; python_version < \"3.7\" in /usr/local/lib/python3.6/site-packages (from tqdm>=4.31.1->bert-score==0.3.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/site-packages (from transformers>=2.2.0->bert-score==0.3.3)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/site-packages (from transformers>=2.2.0->bert-score==0.3.3)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/site-packages (from transformers>=2.2.0->bert-score==0.3.3)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/site-packages (from transformers>=2.2.0->bert-score==0.3.3)\n",
            "Requirement already satisfied: tokenizers==0.7.0 in /usr/local/lib/python3.6/site-packages (from transformers>=2.2.0->bert-score==0.3.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/site-packages (from transformers>=2.2.0->bert-score==0.3.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/site-packages (from torch>=1.0.0->bert-score==0.3.3)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /usr/local/lib/python3.6/site-packages (from matplotlib->bert-score==0.3.3)\n",
            "Collecting kiwisolver>=1.0.1 (from matplotlib->bert-score==0.3.3)\n",
            "  Downloading https://files.pythonhosted.org/packages/a7/1b/cbd8ae738719b5f41592a12057ef5442e2ed5f5cb5451f8fc7e9f8875a1a/kiwisolver-1.3.1-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K    100% |████████████████████████████████| 1.1MB 1.2MB/s \n",
            "\u001b[?25hCollecting pillow>=6.2.0 (from matplotlib->bert-score==0.3.3)\n",
            "  Downloading https://files.pythonhosted.org/packages/7d/2a/2fc11b54e2742db06297f7fa7f420a0e3069fdcf0e4b57dfec33f0b08622/Pillow-8.4.0.tar.gz (49.4MB)\n",
            "\u001b[K    100% |████████████████████████████████| 49.4MB 27kB/s \n",
            "\u001b[?25hCollecting cycler>=0.10 (from matplotlib->bert-score==0.3.3)\n",
            "  Downloading https://files.pythonhosted.org/packages/5c/f9/695d6bedebd747e5eb0fe8fad57b72fdf25411273a39791cde838d5a8f51/cycler-0.11.0-py3-none-any.whl\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/site-packages (from python-dateutil>=2.7.3->pandas>=1.0.1->bert-score==0.3.3)\n",
            "Requirement already satisfied: zipp>=3.1.0; python_version < \"3.10\" in /usr/local/lib/python3.6/site-packages (from importlib-resources; python_version < \"3.7\"->tqdm>=4.31.1->bert-score==0.3.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/site-packages (from sacremoses->transformers>=2.2.0->bert-score==0.3.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/site-packages (from sacremoses->transformers>=2.2.0->bert-score==0.3.3)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/site-packages (from click->sacremoses->transformers>=2.2.0->bert-score==0.3.3)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/site-packages (from importlib-metadata; python_version < \"3.8\"->click->sacremoses->transformers>=2.2.0->bert-score==0.3.3)\n",
            "Building wheels for collected packages: pillow\n",
            "  Running setup.py bdist_wheel for pillow ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/a7/69/9a/bba9fca6782340f88dbc378893095722a663cbc618e58fe401\n",
            "Successfully built pillow\n",
            "Installing collected packages: pytz, pandas, kiwisolver, pillow, cycler, matplotlib, bert-score\n",
            "Successfully installed bert-score-0.3.3 cycler-0.11.0 kiwisolver-1.3.1 matplotlib-3.3.4 pandas-1.1.5 pillow-8.4.0 pytz-2024.1\n",
            "Collecting sacrebleu==1.4.12\n",
            "  Downloading https://files.pythonhosted.org/packages/66/5b/cf661da8e9b0229f5d98c2961b072a5728fd11a0758957f8c0fd36081c06/sacrebleu-1.4.12-py3-none-any.whl (54kB)\n",
            "\u001b[K    100% |████████████████████████████████| 61kB 5.4MB/s \n",
            "\u001b[?25hCollecting portalocker (from sacrebleu==1.4.12)\n",
            "  Downloading https://files.pythonhosted.org/packages/8c/df/d4f711d168524f5aebd7fb30969eaa31e3048cf8979688cde3b08f6e5eb8/portalocker-2.7.0-py2.py3-none-any.whl\n",
            "Collecting mecab-python3==0.996.5 (from sacrebleu==1.4.12)\n",
            "  Downloading https://files.pythonhosted.org/packages/3e/65/98b9101d9b37d01ec205a4eb2f7bc085e1828302a9500bc8326b9daf7163/mecab_python3-0.996.5-cp36-cp36m-manylinux1_x86_64.whl (15.9MB)\n",
            "\u001b[K    100% |████████████████████████████████| 15.9MB 82kB/s \n",
            "\u001b[?25hInstalling collected packages: portalocker, mecab-python3, sacrebleu\n",
            "Successfully installed mecab-python3-0.996.5 portalocker-2.7.0 sacrebleu-1.4.12\n",
            "Collecting moverscore==1.0.3\n",
            "  Downloading https://files.pythonhosted.org/packages/d7/66/5ac942903a4d2d316f6c1df716e047e34fb0f9bfc7e70d44dab995708e9d/moverscore-1.0.3.tar.gz\n",
            "Collecting typing (from moverscore==1.0.3)\n",
            "  Downloading https://files.pythonhosted.org/packages/05/d9/6eebe19d46bd05360c9a9aae822e67a80f9242aabbfc58b641b957546607/typing-3.7.4.3.tar.gz (78kB)\n",
            "\u001b[K    100% |████████████████████████████████| 81kB 5.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: portalocker in /usr/local/lib/python3.6/site-packages (from moverscore==1.0.3)\n",
            "Building wheels for collected packages: moverscore, typing\n",
            "  Running setup.py bdist_wheel for moverscore ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/79/1b/3b/146bc744eef28d6c091b864fac8b76a7ffec5a694776c105f1\n",
            "  Running setup.py bdist_wheel for typing ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/2d/04/41/8e1836e79581989c22eebac3f4e70aaac9af07b0908da173be\n",
            "Successfully built moverscore typing\n",
            "Installing collected packages: typing, moverscore\n",
            "Successfully installed moverscore-1.0.3 typing-3.7.4.3\n",
            "Collecting pyemd\n",
            "  Downloading https://files.pythonhosted.org/packages/c0/c5/7fea8e7a71cd026b30ed3c40e4c5ea13a173e28f8855da17e25271e8f545/pyemd-0.5.1.tar.gz (91kB)\n",
            "\u001b[K    100% |████████████████████████████████| 92kB 4.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy<2.0.0,>=1.9.0 in /usr/local/lib/python3.6/site-packages (from pyemd)\n",
            "Building wheels for collected packages: pyemd\n",
            "  Running setup.py bdist_wheel for pyemd ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/e4/ba/b0/1f4178a35c916b22fc51dc56f278125d4b8cfb0592e5f0cc24\n",
            "Successfully built pyemd\n",
            "Installing collected packages: pyemd\n",
            "Successfully installed pyemd-0.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! git clone https://github.com/google-research/bleurt.git\n",
        "%cd bleurt\n",
        "! pip install .\n",
        "%cd .."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sB04VgmO5WNd",
        "outputId": "06c22d7f-dced-401f-c2e2-d6038b1af7df"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'bleurt'...\n",
            "remote: Enumerating objects: 134, done.\u001b[K\n",
            "remote: Counting objects: 100% (18/18), done.\u001b[K\n",
            "remote: Compressing objects: 100% (18/18), done.\u001b[K\n",
            "remote: Total 134 (delta 0), reused 17 (delta 0), pack-reused 116\u001b[K\n",
            "Receiving objects: 100% (134/134), 31.28 MiB | 23.85 MiB/s, done.\n",
            "Resolving deltas: 100% (49/49), done.\n",
            "/content/SciGen/baselines/bleurt\n",
            "Processing /content/SciGen/baselines/bleurt\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/site-packages (from BLEURT==0.0.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/site-packages (from BLEURT==0.0.2)\n",
            "Collecting scipy (from BLEURT==0.0.2)\n",
            "  Downloading https://files.pythonhosted.org/packages/c8/89/63171228d5ced148f5ced50305c89e8576ffc695a90b58fe5bb602b910c2/scipy-1.5.4-cp36-cp36m-manylinux1_x86_64.whl (25.9MB)\n",
            "\u001b[K    100% |████████████████████████████████| 25.9MB 52kB/s \n",
            "\u001b[?25hCollecting tensorflow (from BLEURT==0.0.2)\n",
            "  Downloading https://files.pythonhosted.org/packages/de/f0/96fb2e0412ae9692dbf400e5b04432885f677ad6241c088ccc5fe7724d69/tensorflow-1.14.0-cp36-cp36m-manylinux1_x86_64.whl (109.2MB)\n",
            "\u001b[K    100% |████████████████████████████████| 109.2MB 12kB/s \n",
            "\u001b[?25hCollecting tf-slim>=1.1 (from BLEURT==0.0.2)\n",
            "  Downloading https://files.pythonhosted.org/packages/02/97/b0f4a64df018ca018cc035d44f2ef08f91e2e8aa67271f6f19633a015ff7/tf_slim-1.1.0-py2.py3-none-any.whl (352kB)\n",
            "\u001b[K    100% |████████████████████████████████| 358kB 3.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: sentencepiece in /usr/local/lib/python3.6/site-packages (from BLEURT==0.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/site-packages (from pandas->BLEURT==0.0.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/site-packages (from pandas->BLEURT==0.0.2)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/site-packages (from tensorflow->BLEURT==0.0.2)\n",
            "Collecting termcolor>=1.1.0 (from tensorflow->BLEURT==0.0.2)\n",
            "  Downloading https://files.pythonhosted.org/packages/8a/48/a76be51647d0eb9f10e2a4511bf3ffb8cc1e6b14e9e4fab46173aa79f981/termcolor-1.1.0.tar.gz\n",
            "Collecting keras-preprocessing>=1.0.5 (from tensorflow->BLEURT==0.0.2)\n",
            "  Downloading https://files.pythonhosted.org/packages/79/4c/7c3275a01e12ef9368a892926ab932b33bb13d55794881e3573482b378a7/Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42kB)\n",
            "\u001b[K    100% |████████████████████████████████| 51kB 11.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/site-packages (from tensorflow->BLEURT==0.0.2)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/site-packages (from tensorflow->BLEURT==0.0.2)\n",
            "Collecting tensorflow-estimator<1.15.0rc0,>=1.14.0rc0 (from tensorflow->BLEURT==0.0.2)\n",
            "  Downloading https://files.pythonhosted.org/packages/3c/d5/21860a5b11caf0678fbc8319341b0ae21a07156911132e0e71bffed0510d/tensorflow_estimator-1.14.0-py2.py3-none-any.whl (488kB)\n",
            "\u001b[K    100% |████████████████████████████████| 491kB 2.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/site-packages (from tensorflow->BLEURT==0.0.2)\n",
            "Collecting google-pasta>=0.1.6 (from tensorflow->BLEURT==0.0.2)\n",
            "  Downloading https://files.pythonhosted.org/packages/a3/de/c648ef6835192e6e2cc03f40b19eeda4382c49b5bafb43d88b931c4c74ac/google_pasta-0.2.0-py3-none-any.whl (57kB)\n",
            "\u001b[K    100% |████████████████████████████████| 61kB 10.7MB/s \n",
            "\u001b[?25hCollecting astor>=0.6.0 (from tensorflow->BLEURT==0.0.2)\n",
            "  Downloading https://files.pythonhosted.org/packages/c3/88/97eef84f48fa04fbd6750e62dcceafba6c63c81b7ac1420856c8dcc0a3f9/astor-0.8.1-py2.py3-none-any.whl\n",
            "Collecting tensorboard<1.15.0,>=1.14.0 (from tensorflow->BLEURT==0.0.2)\n",
            "  Downloading https://files.pythonhosted.org/packages/91/2d/2ed263449a078cd9c8a9ba50ebd50123adf1f8cfbea1492f9084169b89d9/tensorboard-1.14.0-py3-none-any.whl (3.1MB)\n",
            "\u001b[K    100% |████████████████████████████████| 3.2MB 442kB/s \n",
            "\u001b[?25hCollecting keras-applications>=1.0.6 (from tensorflow->BLEURT==0.0.2)\n",
            "  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n",
            "\u001b[K    100% |████████████████████████████████| 51kB 10.4MB/s \n",
            "\u001b[?25hCollecting wrapt>=1.11.1 (from tensorflow->BLEURT==0.0.2)\n",
            "  Downloading https://files.pythonhosted.org/packages/66/50/1c5ccb23dd63f8f3d312dc2d5e0e64c681faf7c2388fa1ab2553713cef11/wrapt-1.16.0-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (77kB)\n",
            "\u001b[K    100% |████████████████████████████████| 81kB 11.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/site-packages (from tensorflow->BLEURT==0.0.2)\n",
            "Collecting gast>=0.2.0 (from tensorflow->BLEURT==0.0.2)\n",
            "  Downloading https://files.pythonhosted.org/packages/fa/39/5aae571e5a5f4de9c3445dae08a530498e5c53b0e74410eeeb0991c79047/gast-0.5.4-py3-none-any.whl\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/site-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow->BLEURT==0.0.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/site-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow->BLEURT==0.0.2)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/site-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow->BLEURT==0.0.2)\n",
            "Collecting h5py (from keras-applications>=1.0.6->tensorflow->BLEURT==0.0.2)\n",
            "  Downloading https://files.pythonhosted.org/packages/70/7a/e53e500335afb6b1aade11227cdf107fca54106a1dca5c9d13242a043f3b/h5py-3.1.0-cp36-cp36m-manylinux1_x86_64.whl (4.0MB)\n",
            "\u001b[K    100% |████████████████████████████████| 4.0MB 342kB/s \n",
            "\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/site-packages (from werkzeug>=0.11.15->tensorboard<1.15.0,>=1.14.0->tensorflow->BLEURT==0.0.2)\n",
            "Requirement already satisfied: importlib-metadata>=4.4; python_version < \"3.10\" in /usr/local/lib/python3.6/site-packages (from markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow->BLEURT==0.0.2)\n",
            "Collecting cached-property; python_version < \"3.8\" (from h5py->keras-applications>=1.0.6->tensorflow->BLEURT==0.0.2)\n",
            "  Downloading https://files.pythonhosted.org/packages/48/19/f2090f7dad41e225c7f2326e4cfe6fff49e57dedb5b53636c9551f86b069/cached_property-1.5.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/site-packages (from importlib-metadata>=4.4; python_version < \"3.10\"->markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow->BLEURT==0.0.2)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/site-packages (from importlib-metadata>=4.4; python_version < \"3.10\"->markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow->BLEURT==0.0.2)\n",
            "Building wheels for collected packages: termcolor\n",
            "  Running setup.py bdist_wheel for termcolor ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/7c/06/54/bc84598ba1daf8f970247f550b175aaaee85f68b4b0c5ab2c6\n",
            "Successfully built termcolor\n",
            "Installing collected packages: scipy, termcolor, keras-preprocessing, tensorflow-estimator, google-pasta, astor, tensorboard, cached-property, h5py, keras-applications, wrapt, gast, tensorflow, tf-slim, BLEURT\n",
            "  Found existing installation: tensorboard 2.10.1\n",
            "    Uninstalling tensorboard-2.10.1:\n",
            "      Successfully uninstalled tensorboard-2.10.1\n",
            "  Running setup.py install for BLEURT ... \u001b[?25ldone\n",
            "\u001b[?25hSuccessfully installed BLEURT-0.0.2 astor-0.8.1 cached-property-1.5.2 gast-0.5.4 google-pasta-0.2.0 h5py-3.1.0 keras-applications-1.0.8 keras-preprocessing-1.1.2 scipy-1.5.4 tensorboard-1.14.0 tensorflow-1.14.0 tensorflow-estimator-1.14.0 termcolor-1.1.0 tf-slim-1.1.0 wrapt-1.16.0\n",
            "/content/SciGen/baselines\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clone the GitHub repo for SciGen"
      ],
      "metadata": {
        "id": "niBtXU5EzjB3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/UKPLab/SciGen.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WaqP0od_yNSO",
        "outputId": "b167f523-b203-4be3-c869-04eaa4e1de70"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'SciGen'...\n",
            "remote: Enumerating objects: 479, done.\u001b[K\n",
            "remote: Counting objects: 100% (479/479), done.\u001b[K\n",
            "remote: Compressing objects: 100% (460/460), done.\u001b[K\n",
            "remote: Total 479 (delta 19), reused 454 (delta 11), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (479/479), 39.76 MiB | 20.07 MiB/s, done.\n",
            "Resolving deltas: 100% (19/19), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/mHarvey52/com4513_assignment.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pacXTPtRucSi",
        "outputId": "256791c5-1cbd-419d-cba9-1ad919a26995"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'com4513_assignment'...\n",
            "remote: Enumerating objects: 6, done.\u001b[K\n",
            "remote: Counting objects: 100% (6/6), done.\u001b[K\n",
            "remote: Compressing objects: 100% (4/4), done.\u001b[K\n",
            "remote: Total 6 (delta 0), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (6/6), 12.58 KiB | 6.29 MiB/s, done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Running baselines"
      ],
      "metadata": {
        "id": "Tp333EQn5um6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd SciGen/baselines"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CoNPBn4I5tpy",
        "outputId": "a3af03ec-441a-4da3-ba70-ea788f61de57"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/SciGen/baselines\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python convert_json_files.py -f ../dataset/train/few-shot/train.json -s train\n",
        "!python convert_json_files.py -f ../dataset/train/few-shot/train.json -s dev\n",
        "!python convert_json_files.py -f ../dataset/train/few-shot/train.json -s test"
      ],
      "metadata": {
        "id": "OzbHX1HMzokC"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir output_train_few_shot\n",
        "!mkdir data_few_shot"
      ],
      "metadata": {
        "id": "eEp5DoA555Mk"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mv train.* data_few_shot/\n",
        "!mv dev.* data_few_shot/\n",
        "!mv test.* data_few_shot/"
      ],
      "metadata": {
        "id": "Gr220jjz6G4a"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python train_table2text_t5.py \\\n",
        "--data_dir=data_few_shot \\\n",
        "--model_name_or_path=facebook/bart-base \\\n",
        "--learning_rate=3e-5 \\\n",
        "--num_train_epochs 30 \\\n",
        "--train_batch_size=8 \\\n",
        "--eval_batch_size=4 \\\n",
        "--test_batch_size=4 \\\n",
        "--output_dir=output_train_large \\\n",
        "--n_gpu 1 \\\n",
        "--do_train \\\n",
        "--do_predict \\\n",
        "--early_stopping_patience 10 \\\n",
        "--max_source_length 384 \\\n",
        "--max_target_length 384"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OlQ3TXtw6PV5",
        "outputId": "7b8e207f-917b-4b37-c8fd-59b578397408"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
            "/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
            "2024-04-21 22:10:43 connectionpool - _new_conn: Starting new HTTPS connection (1): s3.amazonaws.com:443\n",
            "2024-04-21 22:10:44 connectionpool - _make_request: https://s3.amazonaws.com:443 \"HEAD /models.huggingface.co/bert/distilbert-base-uncased-config.json HTTP/1.1\" 200 0\n",
            "2024-04-21 22:10:44 _api - acquire: Attempting to acquire lock 138870464560656 on /root/.cache/torch/transformers/a41e817d5c0743e29e86ff85edc8c257e61bc8d88e4271bb1b243b6e7614c633.8949e27aafafa845a18d98a0e3a88bc2d248bbc32a1b75947366664658f23b1c.lock\n",
            "2024-04-21 22:10:44 _api - acquire: Lock 138870464560656 acquired on /root/.cache/torch/transformers/a41e817d5c0743e29e86ff85edc8c257e61bc8d88e4271bb1b243b6e7614c633.8949e27aafafa845a18d98a0e3a88bc2d248bbc32a1b75947366664658f23b1c.lock\n",
            "2024-04-21 22:10:44 file_utils - get_from_cache: https://s3.amazonaws.com/models.huggingface.co/bert/distilbert-base-uncased-config.json not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmpnxkp14vh\n",
            "2024-04-21 22:10:44 connectionpool - _new_conn: Starting new HTTPS connection (1): s3.amazonaws.com:443\n",
            "2024-04-21 22:10:44 connectionpool - _make_request: https://s3.amazonaws.com:443 \"GET /models.huggingface.co/bert/distilbert-base-uncased-config.json HTTP/1.1\" 200 442\n",
            "Downloading: 100% 442/442 [00:00<00:00, 325kB/s]\n",
            "2024-04-21 22:10:44 file_utils - get_from_cache: storing https://s3.amazonaws.com/models.huggingface.co/bert/distilbert-base-uncased-config.json in cache at /root/.cache/torch/transformers/a41e817d5c0743e29e86ff85edc8c257e61bc8d88e4271bb1b243b6e7614c633.8949e27aafafa845a18d98a0e3a88bc2d248bbc32a1b75947366664658f23b1c\n",
            "2024-04-21 22:10:44 file_utils - get_from_cache: creating metadata file for /root/.cache/torch/transformers/a41e817d5c0743e29e86ff85edc8c257e61bc8d88e4271bb1b243b6e7614c633.8949e27aafafa845a18d98a0e3a88bc2d248bbc32a1b75947366664658f23b1c\n",
            "2024-04-21 22:10:44 _api - release: Attempting to release lock 138870464560656 on /root/.cache/torch/transformers/a41e817d5c0743e29e86ff85edc8c257e61bc8d88e4271bb1b243b6e7614c633.8949e27aafafa845a18d98a0e3a88bc2d248bbc32a1b75947366664658f23b1c.lock\n",
            "2024-04-21 22:10:44 _api - release: Lock 138870464560656 released on /root/.cache/torch/transformers/a41e817d5c0743e29e86ff85edc8c257e61bc8d88e4271bb1b243b6e7614c633.8949e27aafafa845a18d98a0e3a88bc2d248bbc32a1b75947366664658f23b1c.lock\n",
            "2024-04-21 22:10:44 configuration_utils - get_config_dict: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/distilbert-base-uncased-config.json from cache at /root/.cache/torch/transformers/a41e817d5c0743e29e86ff85edc8c257e61bc8d88e4271bb1b243b6e7614c633.8949e27aafafa845a18d98a0e3a88bc2d248bbc32a1b75947366664658f23b1c\n",
            "2024-04-21 22:10:44 configuration_utils - from_dict: Model config DistilBertConfig {\n",
            "  \"activation\": \"gelu\",\n",
            "  \"architectures\": [\n",
            "    \"DistilBertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"dim\": 768,\n",
            "  \"dropout\": 0.1,\n",
            "  \"hidden_dim\": 3072,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"distilbert\",\n",
            "  \"n_heads\": 12,\n",
            "  \"n_layers\": 6,\n",
            "  \"output_attentions\": true,\n",
            "  \"output_hidden_states\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"qa_dropout\": 0.1,\n",
            "  \"seq_classif_dropout\": 0.2,\n",
            "  \"sinusoidal_pos_embds\": false,\n",
            "  \"tie_weights_\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "2024-04-21 22:10:44 connectionpool - _new_conn: Starting new HTTPS connection (1): s3.amazonaws.com:443\n",
            "2024-04-21 22:10:44 connectionpool - _make_request: https://s3.amazonaws.com:443 \"HEAD /models.huggingface.co/bert/bert-base-uncased-vocab.txt HTTP/1.1\" 200 0\n",
            "2024-04-21 22:10:44 _api - acquire: Attempting to acquire lock 138870464560936 on /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084.lock\n",
            "2024-04-21 22:10:44 _api - acquire: Lock 138870464560936 acquired on /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084.lock\n",
            "2024-04-21 22:10:44 file_utils - get_from_cache: https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmpn9soykm6\n",
            "2024-04-21 22:10:44 connectionpool - _new_conn: Starting new HTTPS connection (1): s3.amazonaws.com:443\n",
            "2024-04-21 22:10:44 connectionpool - _make_request: https://s3.amazonaws.com:443 \"GET /models.huggingface.co/bert/bert-base-uncased-vocab.txt HTTP/1.1\" 200 231508\n",
            "Downloading: 100% 232k/232k [00:00<00:00, 1.22MB/s]\n",
            "2024-04-21 22:10:45 file_utils - get_from_cache: storing https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt in cache at /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
            "2024-04-21 22:10:45 file_utils - get_from_cache: creating metadata file for /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
            "2024-04-21 22:10:45 _api - release: Attempting to release lock 138870464560936 on /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084.lock\n",
            "2024-04-21 22:10:45 _api - release: Lock 138870464560936 released on /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084.lock\n",
            "2024-04-21 22:10:45 tokenization_utils - _from_pretrained: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
            "2024-04-21 22:10:45 connectionpool - _new_conn: Starting new HTTPS connection (1): cdn.huggingface.co:443\n",
            "2024-04-21 22:10:45 connectionpool - _make_request: https://cdn.huggingface.co:443 \"HEAD /distilbert-base-uncased-pytorch_model.bin HTTP/1.1\" 200 0\n",
            "2024-04-21 22:10:45 _api - acquire: Attempting to acquire lock 138870464601336 on /root/.cache/torch/transformers/ae9df7a8d658c4f3e1917a471a8a21cf678fa1d4cb91e7702dfe0598dbdcf354.c2015533705b9dff680ae707e205a35e2860e8d148b45d35085419d74fe57ac5.lock\n",
            "2024-04-21 22:10:45 _api - acquire: Lock 138870464601336 acquired on /root/.cache/torch/transformers/ae9df7a8d658c4f3e1917a471a8a21cf678fa1d4cb91e7702dfe0598dbdcf354.c2015533705b9dff680ae707e205a35e2860e8d148b45d35085419d74fe57ac5.lock\n",
            "2024-04-21 22:10:45 file_utils - get_from_cache: https://cdn.huggingface.co/distilbert-base-uncased-pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmpythut69i\n",
            "2024-04-21 22:10:45 connectionpool - _new_conn: Starting new HTTPS connection (1): cdn.huggingface.co:443\n",
            "2024-04-21 22:10:45 connectionpool - _make_request: https://cdn.huggingface.co:443 \"GET /distilbert-base-uncased-pytorch_model.bin HTTP/1.1\" 200 267967963\n",
            "Downloading: 100% 268M/268M [00:03<00:00, 71.3MB/s]\n",
            "2024-04-21 22:10:49 file_utils - get_from_cache: storing https://cdn.huggingface.co/distilbert-base-uncased-pytorch_model.bin in cache at /root/.cache/torch/transformers/ae9df7a8d658c4f3e1917a471a8a21cf678fa1d4cb91e7702dfe0598dbdcf354.c2015533705b9dff680ae707e205a35e2860e8d148b45d35085419d74fe57ac5\n",
            "2024-04-21 22:10:49 file_utils - get_from_cache: creating metadata file for /root/.cache/torch/transformers/ae9df7a8d658c4f3e1917a471a8a21cf678fa1d4cb91e7702dfe0598dbdcf354.c2015533705b9dff680ae707e205a35e2860e8d148b45d35085419d74fe57ac5\n",
            "2024-04-21 22:10:49 _api - release: Attempting to release lock 138870464601336 on /root/.cache/torch/transformers/ae9df7a8d658c4f3e1917a471a8a21cf678fa1d4cb91e7702dfe0598dbdcf354.c2015533705b9dff680ae707e205a35e2860e8d148b45d35085419d74fe57ac5.lock\n",
            "2024-04-21 22:10:49 _api - release: Lock 138870464601336 released on /root/.cache/torch/transformers/ae9df7a8d658c4f3e1917a471a8a21cf678fa1d4cb91e7702dfe0598dbdcf354.c2015533705b9dff680ae707e205a35e2860e8d148b45d35085419d74fe57ac5.lock\n",
            "2024-04-21 22:10:49 modeling_utils - from_pretrained: loading weights file https://cdn.huggingface.co/distilbert-base-uncased-pytorch_model.bin from cache at /root/.cache/torch/transformers/ae9df7a8d658c4f3e1917a471a8a21cf678fa1d4cb91e7702dfe0598dbdcf354.c2015533705b9dff680ae707e205a35e2860e8d148b45d35085419d74fe57ac5\n",
            "2024-04-21 22:10:55 __init__ - wrapper: (private) matplotlib data path: /usr/local/lib/python3.6/site-packages/matplotlib/mpl-data\n",
            "2024-04-21 22:10:55 __init__ - wrapper: matplotlib data path: /usr/local/lib/python3.6/site-packages/matplotlib/mpl-data\n",
            "2024-04-21 22:10:55 __init__ - wrapper: CONFIGDIR=/root/.config/matplotlib\n",
            "2024-04-21 22:10:55 __init__ - <module>: matplotlib version 3.3.4\n",
            "2024-04-21 22:10:55 __init__ - <module>: interactive is False\n",
            "2024-04-21 22:10:55 __init__ - <module>: platform is linux\n",
            "2024-04-21 22:10:55 __init__ - <module>: loaded modules: ['builtins', 'sys', '_frozen_importlib', '_imp', '_warnings', '_thread', '_weakref', '_frozen_importlib_external', '_io', 'marshal', 'posix', 'zipimport', 'encodings', 'codecs', '_codecs', 'encodings.aliases', 'encodings.utf_8', '_signal', '__main__', 'encodings.latin_1', 'io', 'abc', '_weakrefset', 'warnings', 're', 'enum', 'types', 'functools', '_functools', 'collections', '_collections_abc', 'operator', '_operator', 'keyword', 'heapq', '_heapq', 'itertools', 'reprlib', '_collections', 'weakref', 'collections.abc', 'sre_compile', '_sre', 'sre_parse', 'sre_constants', '_locale', 'copyreg', 'site', 'os', 'errno', 'stat', '_stat', 'posixpath', 'genericpath', 'os.path', '_sitebuiltins', 'sysconfig', '_sysconfigdata_m_linux_x86_64-linux-gnu', '_bootlocale', 'importlib', 'importlib._bootstrap', 'importlib._bootstrap_external', 'importlib.util', 'importlib.abc', 'importlib.machinery', 'contextlib', 'mpl_toolkits', 'google', 'argparse', 'copy', 'textwrap', 'gettext', 'locale', 'struct', '_struct', 'glob', 'fnmatch', 'logging', 'time', 'traceback', 'linecache', 'tokenize', 'token', 'string', '_string', 'threading', 'atexit', 'typing', 'typing.io', 'typing.re', 'pathlib', 'ntpath', 'urllib', 'urllib.parse', 'torch', 'platform', 'subprocess', 'signal', '_posixsubprocess', 'select', 'selectors', 'math', 'ctypes', '_ctypes', 'ctypes._endian', 'torch._utils', 'torch._utils_internal', '__future__', 'inspect', 'ast', '_ast', 'dis', 'opcode', '_opcode', 'torch.version', 'torch._six', 'queue', 'torch._C._onnx', 'torch._C._jit_tree_views', 'torch._C.cpp', 'torch._C.cpp.nn', 'torch._C._cudart', 'torch._C._nvtx', 'torch._C._cudnn', 'numpy', 'numpy._globals', 'numpy.__config__', 'numpy.version', 'numpy._distributor_init', 'numpy.core', 'numpy.core.multiarray', 'numpy.core.overrides', 'datetime', '_datetime', 'numpy.core._multiarray_umath', 'numpy.compat', 'numpy.compat._inspect', 'numpy.compat.py3k', 'pickle', '_compat_pickle', '_pickle', 'numpy.core.umath', 'numpy.core.numerictypes', 'numbers', 'numpy.core._string_helpers', 'numpy.core._type_aliases', 'numpy.core._dtype', 'numpy.core.numeric', 'numpy.core.shape_base', 'numpy.core._asarray', 'numpy.core.fromnumeric', 'numpy.core._methods', 'numpy.core._exceptions', 'numpy.core._ufunc_config', 'numpy.core.arrayprint', 'numpy.core.defchararray', 'numpy.core.records', 'numpy.core.memmap', 'numpy.core.function_base', 'numpy.core.machar', 'numpy.core.getlimits', 'numpy.core.einsumfunc', 'numpy.core._add_newdocs', 'numpy.core._multiarray_tests', 'numpy.core._dtype_ctypes', 'numpy.core._internal', 'numpy._pytesttester', 'numpy.lib', 'numpy.lib.mixins', 'numpy.lib.scimath', 'numpy.lib.type_check', 'numpy.lib.ufunclike', 'numpy.lib.index_tricks', 'numpy.matrixlib', 'numpy.matrixlib.defmatrix', 'numpy.linalg', 'numpy.linalg.linalg', 'numpy.lib.twodim_base', 'numpy.linalg.lapack_lite', 'numpy.linalg._umath_linalg', 'numpy.lib.function_base', 'numpy.lib.histograms', 'numpy.lib.stride_tricks', 'numpy.lib.nanfunctions', 'numpy.lib.shape_base', 'numpy.lib.polynomial', 'numpy.lib.utils', 'numpy.lib.arraysetops', 'numpy.lib.npyio', 'numpy.lib.format', 'numpy.lib._datasource', 'shutil', 'zlib', 'bz2', '_compression', '_bz2', 'lzma', '_lzma', 'pwd', 'grp', 'numpy.lib._iotools', 'numpy.lib.financial', 'decimal', '_decimal', 'numpy.lib.arrayterator', 'numpy.lib.arraypad', 'numpy.lib._version', 'numpy.fft', 'numpy.fft._pocketfft', 'numpy.fft._pocketfft_internal', 'numpy.fft.helper', 'numpy.polynomial', 'numpy.polynomial.polynomial', 'numpy.polynomial.polyutils', 'numpy.polynomial._polybase', 'numpy.polynomial.chebyshev', 'numpy.polynomial.legendre', 'numpy.polynomial.hermite', 'numpy.polynomial.hermite_e', 'numpy.polynomial.laguerre', 'numpy.random', 'numpy.random._pickle', 'numpy.random.mtrand', 'cython_runtime', 'numpy.random.bit_generator', '_cython_0_29_21', 'numpy.random._common', 'secrets', 'base64', 'binascii', 'hmac', 'hashlib', '_hashlib', '_blake2', '_sha3', 'random', 'bisect', '_bisect', '_random', 'numpy.random._bounded_integers', 'numpy.random._mt19937', 'numpy.random._philox', 'numpy.random._pcg64', 'numpy.random._sfc64', 'numpy.random._generator', 'numpy.ctypeslib', 'numpy.ma', 'numpy.ma.core', 'numpy.ma.extras', 'numpy.testing', 'unittest', 'unittest.result', 'unittest.util', 'unittest.case', 'difflib', 'pprint', 'unittest.suite', 'unittest.loader', 'unittest.main', 'unittest.runner', 'unittest.signals', 'numpy.testing._private', 'numpy.testing._private.utils', 'gc', 'tempfile', 'numpy.testing._private.decorators', 'numpy.testing._private.nosetester', 'torch._C', 'torch.random', 'torch.serialization', 'tarfile', 'torch._tensor_str', 'torch.tensor', 'torch._namedtensor_internals', 'torch.utils', 'torch.utils.throughput_benchmark', 'torch.utils.hooks', 'torch.storage', 'torch.cuda', 'torch.cuda._utils', 'torch.cuda.memory', 'torch.cuda.random', 'torch.cuda.sparse', 'torch.cuda.profiler', 'torch.cuda.nvtx', 'torch.cuda.streams', 'torch.cuda.amp', 'torch.cuda.amp.grad_scaler', 'torch.sparse', 'torch.functional', 'torch.nn', 'torch.nn.modules', 'torch.nn.modules.module', 'torch.nn.parameter', 'torch.nn.modules.linear', 'torch.nn.functional', 'torch.nn._reduction', 'torch.nn.modules.utils', 'torch.nn.grad', 'torch._VF', 'torch._jit_internal', 'typing_extensions', 'torch._overrides', 'torch.nn.init', 'torch.nn.modules.conv', 'torch.nn.modules.activation', 'torch.nn.modules.loss', 'torch.nn.modules.container', 'torch.nn.modules.pooling', 'torch.nn.modules.batchnorm', 'torch.nn.modules._functions', 'torch.autograd', 'torch.autograd.variable', 'torch.autograd.function', 'torch.autograd.gradcheck', 'torch.testing', 'torch.autograd.grad_mode', 'torch.autograd.anomaly_mode', 'torch.autograd.profiler', 'torch.autograd.functional', 'torch.nn.modules.instancenorm', 'torch.nn.modules.normalization', 'torch.nn.modules.dropout', 'torch.nn.modules.padding', 'torch.nn.modules.sparse', 'torch.nn.modules.rnn', 'torch.nn.utils', 'torch.nn.utils.rnn', 'torch.nn.utils.clip_grad', 'torch.nn.utils.weight_norm', 'torch.nn.utils.convert_parameters', 'torch.nn.utils.spectral_norm', 'torch.nn.utils.fusion', 'torch.nn.utils.memory_format', 'torch.nn.modules.pixelshuffle', 'torch.nn.modules.upsampling', 'torch.nn.modules.distance', 'torch.nn.modules.fold', 'torch.nn.modules.adaptive', 'torch.nn.modules.transformer', 'torch.nn.modules.flatten', 'torch.nn.parallel', 'torch.nn.parallel.parallel_apply', 'torch.nn.parallel.replicate', 'torch.cuda.comm', 'torch.cuda.nccl', 'torch.nn.parallel.data_parallel', 'torch.nn.parallel.scatter_gather', 'torch.nn.parallel._functions', 'torch.nn.parallel.distributed', 'torch.distributed', 'torch.distributed.distributed_c10d', 'torch.distributed.constants', 'torch.distributed.rendezvous', 'torch._lowrank', 'torch._linalg_utils', 'torch.nn.intrinsic', 'torch.nn.intrinsic.modules', 'torch.nn.intrinsic.modules.fused', 'torch.nn.quantized', 'torch.nn.quantized.modules', 'torch.nn.quantized.modules.activation', 'torch.nn.quantized.functional', 'torch.nn.quantized.modules.batchnorm', 'torch.nn.quantized.modules.conv', 'torch.nn.intrinsic.qat', 'torch.nn.intrinsic.qat.modules', 'torch.nn.intrinsic.qat.modules.linear_relu', 'torch.nn.qat', 'torch.nn.qat.modules', 'torch.nn.qat.modules.linear', 'torch.nn.qat.modules.conv', 'torch.nn.intrinsic.qat.modules.conv_fused', 'torch._ops', 'torch.jit', 'torch.jit.annotations', 'torch.jit._recursive', 'torch.jit.frontend', 'torch.jit._builtins', 'torch.backends', 'torch.backends.cudnn', 'torch.distributed.autograd', 'torch.nn.quantized.modules.utils', 'torch.nn.quantized.modules.linear', 'torch.nn.quantized.modules.functional_modules', 'torch.optim', 'torch.optim.adadelta', 'torch.optim.optimizer', 'torch.optim.adagrad', 'torch.optim.adam', 'torch.optim.adamw', 'torch.optim.sparse_adam', 'torch.optim.adamax', 'torch.optim.asgd', 'torch.optim.sgd', 'torch.optim.rprop', 'torch.optim.rmsprop', 'torch.optim.lbfgs', 'torch.optim.lr_scheduler', 'torch.multiprocessing', 'torch.multiprocessing.reductions', 'multiprocessing', 'multiprocessing.context', 'multiprocessing.process', 'multiprocessing.reduction', 'socket', '_socket', 'array', '__mp_main__', 'multiprocessing.util', 'multiprocessing.resource_sharer', 'torch.multiprocessing.spawn', 'multiprocessing.connection', '_multiprocessing', 'torch.utils.backcompat', 'torch.onnx', 'torch.hub', 'zipfile', 'urllib.request', 'email', 'http', 'http.client', 'email.parser', 'email.feedparser', 'email.errors', 'email._policybase', 'email.header', 'email.quoprimime', 'email.base64mime', 'email.charset', 'email.encoders', 'quopri', 'email.utils', 'email._parseaddr', 'calendar', 'email.message', 'uu', 'email._encoded_words', 'email.iterators', 'ssl', 'ipaddress', '_ssl', 'urllib.error', 'urllib.response', 'tqdm', 'tqdm._monitor', 'tqdm._tqdm_pandas', 'tqdm.cli', 'tqdm.std', 'tqdm.utils', 'unicodedata', 'tqdm.version', 'tqdm._dist_ver', 'tqdm.gui', 'tqdm.auto', 'tqdm.autonotebook', 'tqdm.asyncio', 'asyncio', 'asyncio.base_events', 'concurrent', 'concurrent.futures', 'concurrent.futures._base', 'concurrent.futures.process', 'concurrent.futures.thread', 'asyncio.compat', 'asyncio.coroutines', 'asyncio.constants', 'asyncio.events', 'asyncio.base_futures', 'asyncio.log', 'asyncio.futures', 'asyncio.base_tasks', '_asyncio', 'asyncio.tasks', 'asyncio.locks', 'asyncio.protocols', 'asyncio.queues', 'asyncio.streams', 'asyncio.subprocess', 'asyncio.transports', 'asyncio.unix_events', 'asyncio.base_subprocess', 'asyncio.selector_events', 'asyncio.sslproto', 'torch.distributions', 'torch.distributions.bernoulli', 'torch.distributions.constraints', 'torch.distributions.exp_family', 'torch.distributions.distribution', 'torch.distributions.utils', 'torch.distributions.beta', 'torch.distributions.dirichlet', 'torch.distributions.binomial', 'torch.distributions.categorical', 'torch.distributions.cauchy', 'torch.distributions.chi2', 'torch.distributions.gamma', 'torch.distributions.constraint_registry', 'torch.distributions.transforms', 'torch.distributions.continuous_bernoulli', 'torch.distributions.exponential', 'torch.distributions.fishersnedecor', 'torch.distributions.geometric', 'torch.distributions.gumbel', 'torch.distributions.uniform', 'torch.distributions.transformed_distribution', 'torch.distributions.half_cauchy', 'torch.distributions.half_normal', 'torch.distributions.normal', 'torch.distributions.independent', 'torch.distributions.kl', 'torch.distributions.laplace', 'torch.distributions.lowrank_multivariate_normal', 'torch.distributions.multivariate_normal', 'torch.distributions.one_hot_categorical', 'torch.distributions.pareto', 'torch.distributions.poisson', 'torch.distributions.log_normal', 'torch.distributions.logistic_normal', 'torch.distributions.mixture_same_family', 'torch.distributions.multinomial', 'torch.distributions.negative_binomial', 'torch.distributions.relaxed_bernoulli', 'torch.distributions.relaxed_categorical', 'torch.distributions.studentT', 'torch.distributions.von_mises', 'torch.distributed.rpc', 'torch.distributed.rpc.api', 'torch.distributed.rpc.backend_registry', 'torch.distributed.rpc.constants', 'torch.distributed.rpc.internal', 'torch.distributions.weibull', 'torch.backends.cuda', 'torch.backends.mkl', 'torch.backends.mkldnn', 'torch.backends.openmp', 'torch.backends.quantized', 'torch.quantization', 'torch.quantization.quantize', 'torch.quantization.default_mappings', 'torch.nn.intrinsic.quantized', 'torch.nn.intrinsic.quantized.modules', 'torch.nn.intrinsic.quantized.modules.linear_relu', 'torch.nn.intrinsic.quantized.modules.conv_relu', 'torch.nn.quantized.dynamic', 'torch.nn.quantized.dynamic.modules', 'torch.nn.quantized.dynamic.modules.linear', 'torch.nn.quantized.dynamic.modules.rnn', 'torch.quantization.stubs', 'torch.quantization.qconfig', 'torch.quantization.observer', 'torch.quantization.fake_quantize', 'torch.quantization.fuse_modules', 'torch.utils.data', 'torch.utils.data.sampler', 'torch.utils.data.distributed', 'torch.utils.data.dataset', 'torch.utils.data.dataloader', 'torch.utils.data._utils', 'torch.utils.data._utils.worker', 'torch.utils.data._utils.signal_handling', 'torch.utils.data._utils.pin_memory', 'torch.utils.data._utils.collate', 'torch.utils.data._utils.fetch', 'torch.__config__', 'torch.__future__', 'torch._torch_docs', 'torch._tensor_docs', 'torch._storage_docs', 'torch._classes', 'torch.quasirandom', 'torch.multiprocessing._atfork', 'torch._lobpcg', 'lightning_base', 'pytorch_lightning', 'pytorch_lightning.core', 'pytorch_lightning.core.decorators', 'pytorch_lightning.core.lightning', 'pytorch_lightning.core.grads', 'pytorch_lightning.core.hooks', 'pytorch_lightning.utilities', 'pytorch_lightning.utilities.distributed', 'pytorch_lightning.utilities.apply_func', 'pytorch_lightning.utilities.parsing', 'pytorch_lightning.core.memory', 'pytorch_lightning.core.saving', 'csv', '_csv', 'yaml', 'yaml.error', 'yaml.tokens', 'yaml.events', 'yaml.nodes', 'yaml.loader', 'yaml.reader', 'yaml.scanner', 'yaml.parser', 'yaml.composer', 'yaml.constructor', 'yaml.resolver', 'yaml.dumper', 'yaml.emitter', 'yaml.serializer', 'yaml.representer', 'pytorch_lightning.utilities.cloud_io', 'pytorch_lightning.utilities.device_dtype_mixin', 'pytorch_lightning.overrides', 'pytorch_lightning.overrides.data_parallel', 'pytorch_lightning.utilities.exceptions', 'pytorch_lightning.callbacks', 'pytorch_lightning.callbacks.base', 'pytorch_lightning.callbacks.early_stopping', 'pytorch_lightning.callbacks.gradient_accumulation_scheduler', 'pytorch_lightning.callbacks.model_checkpoint', 'pytorch_lightning.callbacks.lr_logger', 'pytorch_lightning.callbacks.progress', 'pytorch_lightning.trainer', 'pytorch_lightning.trainer.trainer', 'pytorch_lightning.loggers', 'pytorch_lightning.loggers.base', 'pytorch_lightning.loggers.tensorboard', 'pkg_resources', 'pkgutil', 'plistlib', 'xml', 'xml.parsers', 'xml.parsers.expat', 'pyexpat.errors', 'pyexpat.model', 'pyexpat', 'xml.parsers.expat.model', 'xml.parsers.expat.errors', 'pkg_resources.extern', 'pkg_resources._vendor', 'pkg_resources._vendor.appdirs', 'pkg_resources.extern.appdirs', 'pkg_resources._vendor.packaging', 'pkg_resources._vendor.packaging.__about__', 'pkg_resources.extern.packaging', 'pkg_resources.extern.packaging.version', 'pkg_resources.extern.packaging._structures', 'pkg_resources.extern.packaging._typing', 'pkg_resources.extern.packaging.specifiers', 'pkg_resources.extern.packaging._compat', 'pkg_resources.extern.packaging.utils', 'pkg_resources.extern.packaging.requirements', 'pkg_resources._vendor.pyparsing', 'pkg_resources.extern.pyparsing', 'pkg_resources.extern.packaging.markers', 'torch.utils.tensorboard', 'tensorboard', 'tensorboard.lazy', 'tensorboard.version', 'tensorboard.summary', 'tensorboard.summary.v1', 'tensorboard.plugins', 'tensorboard.plugins.audio', 'tensorboard.plugins.audio.summary', 'tensorboard.util', 'tensorboard.util.encoder', 'tensorboard.util.op_evaluator', 'tensorboard.plugins.audio.metadata', 'tensorboard.compat', 'tensorboard.compat.proto', 'tensorboard.compat.proto.summary_pb2', 'google.protobuf', 'google.protobuf.descriptor', 'google.protobuf.internal', 'google.protobuf.internal.api_implementation', 'google.protobuf.message', 'google.protobuf.reflection', 'google.protobuf.message_factory', 'google.protobuf.descriptor_pool', 'google.protobuf.descriptor_database', 'google.protobuf.text_encoding', 'google.protobuf.internal.python_message', 'google.protobuf.internal.containers', 'google.protobuf.internal.decoder', 'google.protobuf.internal.encoder', 'google.protobuf.internal.wire_format', 'google.protobuf.internal.enum_type_wrapper', 'google.protobuf.internal.extension_dict', 'google.protobuf.internal.type_checkers', 'google.protobuf.internal.message_listener', 'google.protobuf.internal.well_known_types', 'google.protobuf.text_format', 'encodings.raw_unicode_escape', 'encodings.unicode_escape', 'google.protobuf.symbol_database', 'tensorboard.compat.proto.tensor_pb2', 'tensorboard.compat.proto.resource_handle_pb2', 'tensorboard.compat.proto.tensor_shape_pb2', 'tensorboard.compat.proto.types_pb2', 'google.protobuf.descriptor_pb2', 'tensorboard.plugins.audio.plugin_data_pb2', 'tensorboard.util.tb_logging', 'tensorboard.plugins.audio.summary_v2', 'tensorboard.plugins.custom_scalar', 'tensorboard.plugins.custom_scalar.summary', 'tensorboard.plugins.custom_scalar.layout_pb2', 'tensorboard.plugins.custom_scalar.metadata', 'tensorboard.plugins.histogram', 'tensorboard.plugins.histogram.summary', 'tensorboard.plugins.histogram.metadata', 'tensorboard.plugins.histogram.plugin_data_pb2', 'tensorboard.plugins.histogram.summary_v2', 'tensorboard.util.tensor_util', 'six', 'tensorboard.compat.tensorflow_stub', 'tensorboard.compat.proto.config_pb2', 'tensorboard.compat.proto.cost_graph_pb2', 'tensorboard.compat.proto.graph_pb2', 'tensorboard.compat.proto.node_def_pb2', 'tensorboard.compat.proto.attr_value_pb2', 'tensorboard.compat.proto.function_pb2', 'tensorboard.compat.proto.op_def_pb2', 'tensorboard.compat.proto.versions_pb2', 'tensorboard.compat.proto.step_stats_pb2', 'tensorboard.compat.proto.allocation_description_pb2', 'tensorboard.compat.proto.tensor_description_pb2', 'tensorboard.compat.proto.cluster_pb2', 'tensorboard.compat.proto.debug_pb2', 'tensorboard.compat.proto.rewriter_config_pb2', 'tensorboard.compat.proto.verifier_config_pb2', 'tensorboard.compat.proto.event_pb2', 'tensorboard.compat.proto.meta_graph_pb2', 'google.protobuf.any_pb2', 'tensorboard.compat.proto.saved_object_graph_pb2', 'tensorboard.compat.proto.trackable_object_graph_pb2', 'tensorboard.compat.proto.struct_pb2', 'tensorboard.compat.proto.variable_pb2', 'tensorboard.compat.proto.saver_pb2', 'tensorboard.compat.tensorflow_stub.dtypes', 'tensorboard.compat.tensorflow_stub.pywrap_tensorflow', 'tensorboard.compat.tensorflow_stub.errors', 'tensorboard.compat.tensorflow_stub.error_codes', 'tensorboard.compat.tensorflow_stub.io', 'tensorboard.compat.tensorflow_stub.io.gfile', 'uuid', 'ctypes.util', 'tensorboard.compat.tensorflow_stub.compat', 'tensorboard.compat.tensorflow_stub.compat.v1', 'tensorboard.compat.tensorflow_stub.app', 'tensorboard.compat.tensorflow_stub.flags', 'absl', 'absl.flags', 'getopt', 'absl.flags._argument_parser', 'absl.flags._helpers', 'fcntl', 'termios', 'absl.flags._defines', 'absl.flags._exceptions', 'absl.flags._flag', 'absl.flags._flagvalues', 'xml.dom', 'xml.dom.domreg', 'xml.dom.minidom', 'xml.dom.minicompat', 'xml.dom.xmlbuilder', 'xml.dom.NodeFilter', 'absl.flags._validators_classes', 'absl.flags._validators', 'tensorboard.compat.tensorflow_stub.tensor_shape', 'tensorboard.plugins.image', 'tensorboard.plugins.image.summary', 'tensorboard.plugins.image.metadata', 'tensorboard.plugins.image.plugin_data_pb2', 'tensorboard.plugins.image.summary_v2', 'tensorboard.plugins.pr_curve', 'tensorboard.plugins.pr_curve.summary', 'tensorboard.plugins.pr_curve.metadata', 'tensorboard.plugins.pr_curve.plugin_data_pb2', 'tensorboard.plugins.scalar', 'tensorboard.plugins.scalar.summary', 'tensorboard.plugins.scalar.metadata', 'tensorboard.plugins.scalar.plugin_data_pb2', 'tensorboard.plugins.scalar.summary_v2', 'tensorboard.plugins.text', 'tensorboard.plugins.text.summary', 'tensorboard.plugins.text.metadata', 'tensorboard.plugins.text.plugin_data_pb2', 'tensorboard.plugins.text.summary_v2', 'tensorboard.summary.v2', 'tensorboard.summary.writer', 'tensorboard.summary.writer.record_writer', 'torch.utils.tensorboard.writer', 'tensorboard.plugins.projector', 'tensorboard.plugins.projector.projector_plugin', 'imghdr', 'werkzeug', 'werkzeug.serving', 'socketserver', 'http.server', 'html', 'html.entities', 'mimetypes', 'werkzeug._internal', 'werkzeug.exceptions', 'werkzeug.urls', 'werkzeug.test', 'http.cookiejar', 'werkzeug.datastructures', 'werkzeug.filesystem', 'werkzeug.http', 'werkzeug.sansio', 'werkzeug.sansio.multipart', 'dataclasses', 'werkzeug.utils', 'werkzeug.security', 'werkzeug.wsgi', 'werkzeug.sansio.utils', 'werkzeug.wrappers', 'werkzeug.wrappers.accept', 'werkzeug.wrappers.auth', 'werkzeug.wrappers.base_request', 'werkzeug.wrappers.request', 'json', 'json.decoder', 'json.scanner', '_json', 'json.encoder', 'werkzeug.formparser', 'werkzeug.sansio.request', 'werkzeug.user_agent', 'werkzeug.useragents', 'werkzeug.wrappers.base_response', 'werkzeug.wrappers.response', 'werkzeug.sansio.response', 'werkzeug.wrappers.common_descriptors', 'werkzeug.wrappers.etag', 'werkzeug.wrappers.user_agent', 'google.protobuf.json_format', 'tensorboard.backend', 'tensorboard.backend.http_util', 'gzip', 'wsgiref', 'wsgiref.handlers', 'wsgiref.util', 'wsgiref.headers', 'tensorboard.backend.json_util', 'tensorboard.plugins.base_plugin', 'tensorboard.plugins.projector.projector_config_pb2', 'tensorboard.summary.writer.event_file_writer', 'torch.utils.tensorboard._convert_np', 'torch.utils.tensorboard._embedding', 'torch.utils.tensorboard._utils', 'torch.utils.tensorboard._onnx_graph', 'torch.utils.tensorboard._pytorch_graph', 'torch.utils.tensorboard._proto_graph', 'torch.utils.tensorboard.summary', 'six.moves', 'pytorch_lightning.loggers.comet', 'pytorch_lightning.loggers.mlflow', 'pytorch_lightning.loggers.neptune', 'PIL', 'PIL._version', 'PIL.Image', 'PIL.ImageMode', 'PIL.TiffTags', 'PIL._binary', 'PIL._util', 'PIL._imaging', 'cffi', 'cffi.api', 'cffi.lock', 'cffi.error', 'cffi.model', 'pytorch_lightning.loggers.test_tube', 'pytorch_lightning.loggers.wandb', 'pytorch_lightning.profiler', 'pytorch_lightning.profiler.profilers', 'cProfile', '_lsprof', 'profile', 'optparse', 'pstats', 'pytorch_lightning.trainer.auto_mix_precision', 'pytorch_lightning.trainer.callback_config', 'pytorch_lightning.trainer.callback_hook', 'pytorch_lightning.trainer.data_loading', 'pytorch_lightning.trainer.deprecated_api', 'pytorch_lightning.trainer.distrib_data_parallel', 'pytorch_lightning.trainer.distrib_parts', 'pytorch_lightning.trainer.evaluation_loop', 'pytorch_lightning.trainer.logging', 'pytorch_lightning.utilities.memory', 'pytorch_lightning.trainer.model_hooks', 'pytorch_lightning.trainer.optimizers', 'pytorch_lightning.trainer.supporters', 'pytorch_lightning.trainer.training_io', 'pytorch_lightning.trainer.training_loop', 'pytorch_lightning.trainer.training_tricks', 'pytorch_lightning.trainer.lr_finder', 'pytorch_lightning.utilities.seed', 'transformers', 'absl.logging', 'getpass', 'timeit', 'absl.logging.converter', 'transformers.benchmark_utils', 'transformers.file_utils', 'requests', 'urllib3', 'urllib3.exceptions', 'urllib3.packages', 'urllib3.packages.six', 'urllib3.packages.six.moves', 'urllib3.packages.six.moves.http_client', 'urllib3._version', 'urllib3.connectionpool', 'urllib3.connection', 'urllib3.util', 'urllib3.util.connection', 'urllib3.contrib', 'urllib3.contrib._appengine_environ', 'urllib3.util.wait', 'urllib3.util.request', 'brotli', 'brotli.brotli', '_cffi_backend', '_brotli.lib', '_brotli', 'brotli._brotli', 'urllib3.util.response', 'urllib3.util.retry', 'urllib3.util.ssl_', 'urllib3.util.url', 'urllib3.util.ssltransport', 'urllib3.util.timeout', 'urllib3.util.proxy', 'urllib3._collections', 'urllib3.util.ssl_match_hostname', 'urllib3.request', 'urllib3.filepost', 'urllib3.fields', 'urllib3.packages.six.moves.urllib', 'urllib3.packages.six.moves.urllib.parse', 'urllib3.response', 'urllib3.util.queue', 'urllib3.poolmanager', 'requests.exceptions', 'requests.compat', 'charset_normalizer', 'charset_normalizer.api', 'charset_normalizer.constant', 'charset_normalizer.md', 'charset_normalizer.utils', '_multibytecodec', 'charset_normalizer.models', 'charset_normalizer.cd', 'charset_normalizer.assets', 'charset_normalizer.legacy', 'charset_normalizer.version', 'http.cookies', 'requests.__version__', 'requests.utils', 'requests.certs', 'certifi', 'certifi.core', 'requests._internal_utils', 'requests.cookies', 'requests.structures', 'requests.packages', 'requests.packages.urllib3', 'requests.packages.urllib3.exceptions', 'requests.packages.urllib3.packages', 'requests.packages.urllib3.packages.six', 'requests.packages.urllib3.packages.six.moves', 'requests.packages.urllib3.packages.six.moves.http_client', 'requests.packages.urllib3._version', 'requests.packages.urllib3.connectionpool', 'requests.packages.urllib3.connection', 'requests.packages.urllib3.util', 'requests.packages.urllib3.util.connection', 'requests.packages.urllib3.contrib', 'requests.packages.urllib3.contrib._appengine_environ', 'requests.packages.urllib3.util.wait', 'requests.packages.urllib3.util.request', 'requests.packages.urllib3.util.response', 'requests.packages.urllib3.util.retry', 'requests.packages.urllib3.util.ssl_', 'requests.packages.urllib3.util.url', 'requests.packages.urllib3.util.ssltransport', 'requests.packages.urllib3.util.timeout', 'requests.packages.urllib3.util.proxy', 'requests.packages.urllib3._collections', 'requests.packages.urllib3.util.ssl_match_hostname', 'requests.packages.urllib3.request', 'requests.packages.urllib3.filepost', 'requests.packages.urllib3.fields', 'requests.packages.urllib3.packages.six.moves.urllib', 'requests.packages.urllib3.packages.six.moves.urllib.parse', 'requests.packages.urllib3.response', 'requests.packages.urllib3.util.queue', 'requests.packages.urllib3.poolmanager', 'idna', 'idna.core', 'idna.idnadata', 'idna.intranges', 'requests.packages.idna', 'requests.packages.idna.core', 'requests.packages.idna.idnadata', 'requests.packages.idna.intranges', 'requests.packages.chardet', 'requests.models', 'encodings.idna', 'stringprep', 'requests.hooks', 'requests.auth', 'requests.status_codes', 'requests.api', 'requests.sessions', 'requests.adapters', 'urllib3.contrib.socks', 'socks', 'filelock', 'filelock._api', 'filelock._error', 'filelock._soft', 'filelock._util', 'filelock._unix', 'filelock._windows', 'filelock.version', 'tensorflow', 'distutils', 'tensorflow.python', 'tensorflow.python.pywrap_tensorflow', 'tensorflow.python.platform', 'tensorflow.python.platform.self_check', 'tensorflow.python.platform.build_info', 'tensorflow.python.pywrap_tensorflow_internal', 'imp', 'swig_runtime_data4', '_pywrap_tensorflow_internal', 'tensorflow.python.util', 'tensorflow.python.util.deprecation', 'tensorflow.python.platform.tf_logging', 'tensorflow.python.util.tf_export', 'tensorflow.python.util.tf_decorator', 'tensorflow.python.util.tf_stack', 'tensorflow.python.util.tf_inspect', 'tensorflow.python.util.decorator_utils', 'tensorflow.python.util.is_in_graph_mode', 'tensorflow.python.util.tf_contextlib', 'tensorflow.core', 'tensorflow.core.framework', 'tensorflow.core.framework.graph_pb2', 'tensorflow.core.framework.node_def_pb2', 'tensorflow.core.framework.attr_value_pb2', 'tensorflow.core.framework.tensor_pb2', 'tensorflow.core.framework.resource_handle_pb2', 'tensorflow.core.framework.tensor_shape_pb2', 'tensorflow.core.framework.types_pb2', 'tensorflow.core.framework.function_pb2', 'tensorflow.core.framework.op_def_pb2', 'tensorflow.core.framework.versions_pb2', 'tensorflow.core.framework.summary_pb2', 'tensorflow.core.protobuf', 'tensorflow.core.protobuf.meta_graph_pb2', 'tensorflow.core.protobuf.saved_object_graph_pb2', 'tensorflow.core.protobuf.trackable_object_graph_pb2', 'tensorflow.core.protobuf.struct_pb2', 'tensorflow.core.framework.variable_pb2', 'tensorflow.core.protobuf.saver_pb2', 'tensorflow.core.protobuf.config_pb2', 'tensorflow.core.framework.cost_graph_pb2', 'tensorflow.core.framework.step_stats_pb2', 'tensorflow.core.framework.allocation_description_pb2', 'tensorflow.core.framework.tensor_description_pb2', 'tensorflow.core.protobuf.cluster_pb2', 'tensorflow.core.protobuf.debug_pb2', 'tensorflow.core.protobuf.rewriter_config_pb2', 'tensorflow.core.protobuf.verifier_config_pb2', 'tensorflow.core.protobuf.tensorflow_server_pb2', 'tensorflow.core.util', 'tensorflow.core.util.event_pb2', 'tensorflow.python.framework', 'tensorflow.python.framework.framework_lib', 'tensorflow.python.framework.device', 'tensorflow.python.tf2', 'tensorflow.python.framework.device_spec', 'tensorflow.python.framework.ops', 'tensorflow.python.eager', 'tensorflow.python.eager.context', 'tensorflow.python.framework.c_api_util', 'tensorflow.core.framework.api_def_pb2', 'tensorflow.python.util.compat', 'tensorflow.python.eager.core', 'tensorflow.python.framework.errors', 'tensorflow.python.framework.errors_impl', 'tensorflow.core.lib', 'tensorflow.core.lib.core', 'tensorflow.core.lib.core.error_codes_pb2', 'tensorflow.python.framework.error_interpolation', 'tensorflow.python.eager.tape', 'tensorflow.python.util.lazy_loader', 'tensorflow.python.framework.composite_tensor', 'tensorflow.python.util.nest', 'tensorflow.python.framework.dtypes', 'tensorflow.python.framework.op_def_registry', 'tensorflow.python.framework.registry', 'tensorflow.python.framework.tensor_shape', 'tensorflow.python.framework.traceable_stack', 'tensorflow.python.framework.versions', 'tensorflow.python.ops', 'tensorflow.python.ops.control_flow_util', 'tensorflow.python.platform.app', 'absl.app', 'pdb', 'cmd', 'bdb', 'code', 'codeop', 'absl.command_name', 'faulthandler', 'tensorflow.python.platform.flags', 'tensorflow.python.util.function_utils', 'tensorflow.python.util.lock_util', 'tensorflow.python.util.memory', 'tensorflow.python.framework.sparse_tensor', 'tensorflow.python.framework.tensor_spec', 'tensorflow.python.framework.common_shapes', 'tensorflow.python.framework.cpp_shape_inference_pb2', 'tensorflow.python.framework.tensor_util', 'tensorflow.python.framework.fast_tensor_util', 'tensorflow.python.framework.random_seed', 'tensorflow.python.framework.importer', 'tensorflow.python.framework.function', 'tensorflow.python.framework.graph_to_function_def', 'tensorflow.python.ops.array_ops', 'tensorflow.python.compat', 'tensorflow.python.compat.compat', 'tensorflow.python.framework.constant_op', 'tensorflow.python.eager.execute', 'tensorflow.python.ops.gen_array_ops', 'tensorflow.python.framework.op_def_library', 'tensorflow.python.util.dispatch', 'tensorflow.tools', 'tensorflow.tools.docs', 'tensorflow.tools.docs.doc_controls', 'tensorflow.python.ops.gen_math_ops', 'tensorflow.python.ops.resource_variable_ops', 'tensorflow.python.ops.gen_resource_variable_ops', 'tensorflow.python.ops.gen_state_ops', 'tensorflow.python.ops.math_ops', 'tensorflow.python.framework.graph_util', 'tensorflow.python.framework.graph_util_impl', 'tensorflow.python.ops.gen_data_flow_ops', 'tensorflow.python.ops.gen_nn_ops', 'tensorflow.python.ops.gen_sparse_ops', 'tensorflow.python.ops.state_ops', 'tensorflow.python.ops.variables', 'tensorflow.python.ops.control_flow_ops', 'tensorflow.core.protobuf.control_flow_pb2', 'tensorflow.python.ops.gen_control_flow_ops', 'tensorflow.python.ops.gen_logging_ops', 'tensorflow.python.ops.tensor_array_ops', 'tensorflow.python.ops.list_ops', 'tensorflow.python.ops.gen_list_ops', 'tensorflow.python.util.tf_should_use', 'tensorflow.python.training', 'tensorflow.python.training.tracking', 'tensorflow.python.training.tracking.base', 'tensorflow.python.ops.gen_io_ops', 'tensorflow.python.training.saving', 'tensorflow.python.training.saving.saveable_object', 'tensorflow.python.ops.variable_scope', 'tensorflow.python.ops.init_ops', 'tensorflow.python.ops.gen_linalg_ops', 'tensorflow.python.ops.linalg_ops_impl', 'tensorflow.python.ops.random_ops', 'tensorflow.python.ops.gen_random_ops', 'tensorflow.python.framework.load_library', 'tensorflow.python.lib', 'tensorflow.python.lib.io', 'tensorflow.python.lib.io.file_io', 'tensorflow.python.framework.config', 'tensorflow.python.client', 'tensorflow.python.client.client_lib', 'tensorflow.python.client.session', 'tensorflow.python.ops.session_ops', 'tensorflow.python.training.experimental', 'tensorflow.python.training.experimental.mixed_precision_global_state', 'tensorflow.python.ops.standard_ops', 'tensorflow.python.autograph', 'tensorflow.python.autograph.operators', 'tensorflow.python.autograph.operators.control_flow', 'tensorflow.python.autograph.operators.py_builtins', 'tensorflow.python.autograph.utils', 'tensorflow.python.autograph.utils.context_managers', 'tensorflow.python.autograph.utils.misc', 'tensorflow.python.autograph.utils.py_func', 'tensorflow.python.ops.script_ops', 'tensorflow.python.eager.backprop', 'tensorflow.python.eager.imperative_grad', 'tensorflow.python.ops.unconnected_gradients', 'tensorflow.python.ops.check_ops', 'tensorflow.python.ops.gen_script_ops', 'tensorflow.python.autograph.utils.tensor_list', 'tensorflow.python.autograph.utils.testing', 'tensorflow.python.autograph.utils.type_check', 'tensorflow.python.autograph.utils.tensors', 'tensorflow.python.ops.gen_parsing_ops', 'tensorflow.python.ops.gen_string_ops', 'tensorflow.python.autograph.operators.special_values', 'tensorflow.python.autograph.utils.ag_logging', 'tensorflow.python.data', 'tensorflow.python.data.experimental', 'tensorflow.python.data.experimental.ops', 'tensorflow.python.data.experimental.ops.batching', 'tensorflow.python.data.ops', 'tensorflow.python.data.ops.dataset_ops', 'tensorflow.python.data.experimental.ops.distribute_options', 'tensorflow.python.data.util', 'tensorflow.python.data.util.options', 'tensorflow.python.data.experimental.ops.optimization_options', 'tensorflow.python.data.experimental.ops.stats_options', 'tensorflow.python.data.experimental.ops.stats_aggregator', 'tensorflow.python.ops.gen_experimental_dataset_ops', 'tensorflow.python.ops.summary_ops_v2', 'tensorflow.python.eager.profiler', 'tensorflow.python.platform.gfile', 'tensorflow.python.framework.smart_cond', 'tensorflow.python.ops.gen_summary_ops', 'tensorflow.python.ops.summary_op_util', 'tensorflow.python.training.training_util', 'tensorflow.python.framework.graph_io', 'tensorflow.python.data.experimental.ops.threading_options', 'tensorflow.python.data.ops.iterator_ops', 'tensorflow.python.data.ops.optional_ops', 'tensorflow.python.data.util.structure', 'tensorflow.python.data.util.nest', 'tensorflow.python.ops.sparse_ops', 'tensorflow.python.ops.ragged', 'tensorflow.python.ops.ragged.ragged_array_ops', 'tensorflow.python.ops.ragged.ragged_functional_ops', 'tensorflow.python.ops.ragged.ragged_config', 'tensorflow.python.ops.ragged.ragged_tensor', 'tensorflow.python.ops.gen_ragged_conversion_ops', 'tensorflow.python.ops.ragged.ragged_tensor_value', 'tensorflow.python.ops.ragged.ragged_util', 'tensorflow.python.ops.gen_ragged_math_ops', 'tensorflow.python.ops.ragged.segment_id_ops', 'tensorflow.python.ops.ragged.ragged_math_ops', 'tensorflow.python.ops.ragged.ragged_batch_gather_ops', 'tensorflow.python.ops.ragged.ragged_gather_ops', 'tensorflow.python.ops.gen_ragged_array_ops', 'tensorflow.python.ops.ragged.ragged_batch_gather_with_default_op', 'tensorflow.python.ops.ragged.ragged_dispatch', 'tensorflow.python.ops.clip_ops', 'tensorflow.python.ops.gen_bitwise_ops', 'tensorflow.python.ops.parsing_ops', 'tensorflow.python.ops.string_ops', 'tensorflow.python.ops.ragged.ragged_concat_ops', 'tensorflow.python.ops.ragged.ragged_squeeze_op', 'tensorflow.python.ops.ragged.ragged_tensor_shape', 'tensorflow.python.ops.ragged.ragged_where_op', 'tensorflow.python.ops.ragged.ragged_operators', 'tensorflow.python.ops.ragged.ragged_getitem', 'tensorflow.python.ops.ragged.ragged_conversion_ops', 'tensorflow.python.ops.ragged.ragged_factory_ops', 'tensorflow.python.ops.ragged.ragged_map_ops', 'tensorflow.python.ops.ragged.ragged_string_ops', 'tensorflow.python.ops.gen_dataset_ops', 'tensorflow.python.training.saver', 'tensorflow.python.framework.meta_graph', 'distutils.version', 'tensorflow.core.protobuf.graph_debug_info_pb2', 'tensorflow.python.ops.io_ops', 'tensorflow.python.lib.io.python_io', 'tensorflow.python.lib.io.tf_record', 'tensorflow.python.training.checkpoint_management', 'tensorflow.python.training.checkpoint_state_pb2', 'tensorflow.python.training.saving.saveable_object_util', 'tensorflow.python.data.util.random_seed', 'tensorflow.python.data.util.sparse', 'tensorflow.python.data.util.traverse', 'tensorflow.python.eager.function', 'tensorflow.python.eager.graph_only_ops', 'tensorflow.python.framework.func_graph', 'tensorflow.python.framework.auto_control_deps', 'tensorflow.python.ops.custom_gradient', 'tensorflow.python.ops.functional_ops', 'tensorflow.python.ops.gen_functional_ops', 'tensorflow.python.ops.gradients_util', 'tensorflow.python.training.tracking.tracking', 'tensorflow.python.eager.def_function', 'tensorflow.python.eager.lift_to_graph', 'tensorflow.python.training.tracking.data_structures', 'wrapt', 'wrapt.__wrapt__', 'wrapt.wrappers', 'wrapt._wrappers', 'wrapt.patches', 'wrapt.weakrefs', 'wrapt.decorators', 'wrapt.arguments', 'wrapt.importer', 'tensorflow.python.saved_model', 'tensorflow.python.saved_model.revived_types', 'tensorflow.python.training.tracking.layer_utils', 'tensorflow.python.training.tracking.object_identity', 'tensorflow.python.data.util.convert', 'tensorflow.python.data.experimental.ops.cardinality', 'tensorflow.python.data.experimental.ops.counter', 'tensorflow.python.data.experimental.ops.scan_ops', 'tensorflow.python.data.experimental.ops.enumerate_ops', 'tensorflow.python.data.experimental.ops.error_ops', 'tensorflow.python.data.experimental.ops.get_single_element', 'tensorflow.python.data.experimental.ops.grouping', 'tensorflow.python.data.experimental.ops.interleave_ops', 'tensorflow.python.data.experimental.ops.random_ops', 'tensorflow.python.data.ops.readers', 'tensorflow.python.ops.gen_stateless_random_ops', 'tensorflow.python.data.experimental.ops.iterator_ops', 'tensorflow.python.training.basic_session_run_hooks', 'tensorflow.python.client.timeline', 'tensorflow.python.training.session_run_hook', 'tensorflow.python.training.summary_io', 'tensorflow.python.summary', 'tensorflow.python.summary.summary_iterator', 'tensorflow.python.summary.writer', 'tensorflow.python.summary.writer.writer', 'tensorflow.python.summary.plugin_asset', 'tensorflow.python.summary.writer.event_file_writer', 'tensorflow.python.summary.writer.event_file_writer_v2', 'tensorflow.python.summary.writer.writer_cache', 'tensorflow.python.data.experimental.ops.optimization', 'tensorflow.python.data.experimental.ops.parsing_ops', 'tensorflow.python.data.experimental.ops.prefetching_ops', 'tensorflow.python.data.experimental.ops.readers', 'tensorflow.python.data.experimental.ops.shuffle_ops', 'tensorflow.python.data.experimental.ops.resampling', 'tensorflow.python.ops.logging_ops', 'tensorflow.python.data.experimental.ops.stats_ops', 'tensorflow.python.data.experimental.ops.take_while_ops', 'tensorflow.python.data.experimental.ops.unique', 'tensorflow.python.data.experimental.ops.writers', 'tensorflow.python.util.all_util', 'tensorflow.python.autograph.operators.data_structures', 'tensorflow.python.autograph.operators.exceptions', 'tensorflow.python.autograph.operators.logical', 'tensorflow.python.autograph.operators.slices', 'tensorflow.python.autograph.core', 'tensorflow.python.autograph.core.converter', 'tensorflow.python.autograph.pyct', 'tensorflow.python.autograph.pyct.anno', 'gast', 'gast.gast', 'gast.ast3', 'gast.astn', 'gast.version', 'tensorflow.python.autograph.pyct.ast_util', 'tensorflow.python.autograph.pyct.parser', 'tensorflow.python.autograph.pyct.inspect_utils', 'tensorflow.python.autograph.pyct.cfg', 'tensorflow.python.autograph.pyct.compiler', 'astor', 'astor.code_gen', 'astor.op_util', 'astor.node_util', 'astor.string_repr', 'astor.source_repr', 'astor.file_util', 'astor.tree_walk', 'tensorflow.python.autograph.pyct.origin_info', 'tensorflow.python.autograph.pyct.pretty_printer', 'termcolor', 'tensorflow.python.autograph.pyct.qual_names', 'tensorflow.python.autograph.pyct.templates', 'tensorflow.python.autograph.pyct.transformer', 'tensorflow.python.autograph.pyct.static_analysis', 'tensorflow.python.autograph.pyct.static_analysis.activity', 'tensorflow.python.autograph.pyct.static_analysis.annos', 'tensorflow.python.autograph.pyct.static_analysis.liveness', 'tensorflow.python.autograph.pyct.static_analysis.reaching_definitions', 'tensorflow.python.autograph.impl', 'tensorflow.python.autograph.impl.api', 'tensorflow.python.autograph.impl.conversion', 'tensorflow.python.autograph.converters', 'tensorflow.python.autograph.converters.arg_defaults', 'tensorflow.python.autograph.converters.asserts', 'tensorflow.python.autograph.converters.break_statements', 'tensorflow.python.autograph.converters.call_trees', 'tensorflow.python.autograph.converters.conditional_expressions', 'tensorflow.python.autograph.converters.continue_statements', 'tensorflow.python.autograph.converters.control_flow', 'tensorflow.python.autograph.converters.directives', 'tensorflow.python.autograph.lang', 'tensorflow.python.autograph.lang.directives', 'tensorflow.python.autograph.converters.function_scopes', 'tensorflow.python.autograph.converters.lists', 'tensorflow.python.autograph.converters.logical_expressions', 'tensorflow.python.autograph.converters.return_statements', 'tensorflow.python.autograph.converters.side_effect_guards', 'tensorflow.python.autograph.converters.slices', 'tensorflow.python.autograph.core.config', 'tensorflow.python.autograph.core.function_wrapping', 'tensorflow.python.autograph.core.naming', 'tensorflow.python.autograph.core.unsupported_features_checker', 'tensorflow.python.autograph.lang.special_functions', 'tensorflow.python.autograph.pyct.errors', 'tensorflow.python.ops.array_grad', 'tensorflow.python.ops.cudnn_rnn_grad', 'tensorflow.python.ops.gen_cudnn_rnn_ops', 'tensorflow.python.ops.data_flow_grad', 'tensorflow.python.ops.data_flow_ops', 'tensorflow.python.ops.manip_grad', 'tensorflow.python.ops.manip_ops', 'tensorflow.python.ops.gen_manip_ops', 'tensorflow.python.ops.math_grad', 'tensorflow.python.ops.random_grad', 'tensorflow.python.ops.sparse_grad', 'tensorflow.python.ops.state_grad', 'tensorflow.python.ops.tensor_array_grad', 'tensorflow.python.ops.special_math_ops', 'tensorflow.compiler', 'tensorflow.compiler.tf2xla', 'tensorflow.compiler.tf2xla.ops', 'tensorflow.compiler.tf2xla.ops.gen_xla_ops', 'tensorflow.python.ops.confusion_matrix', 'tensorflow.python.eager.wrap_function', 'tensorflow.python.ops.batch_ops', 'tensorflow.python.ops.gen_batch_ops', 'tensorflow.python.ops.critical_section_ops', 'tensorflow.python.ops.gradients', 'tensorflow.python.ops.gradients_impl', 'tensorflow.python.ops.control_flow_grad', 'tensorflow.python.ops.image_grad', 'tensorflow.python.ops.gen_image_ops', 'tensorflow.python.ops.linalg_grad', 'tensorflow.python.ops.linalg_ops', 'tensorflow.python.ops.map_fn', 'tensorflow.python.ops.linalg', 'tensorflow.python.ops.linalg.linalg_impl', 'tensorflow.python.ops.optional_grad', 'tensorflow.python.ops.histogram_ops', 'tensorflow.python.ops.lookup_ops', 'tensorflow.python.ops.gen_lookup_ops', 'tensorflow.python.ops.numerics', 'tensorflow.python.ops.partitioned_variables', 'tensorflow.python.ops.proto_ops', 'tensorflow.python.ops.gen_decode_proto_ops', 'tensorflow.python.ops.gen_encode_proto_ops', 'tensorflow.python.ops.sort_ops', 'tensorflow.python.ops.nn_ops', 'tensorflow.python.ops.stateless_random_ops', 'tensorflow.python.ops.template', 'tensorflow.python.training.tracking.util', 'tensorflow.python.training.saving.functional_saver', 'tensorflow.python.training.tracking.graph_view', 'tensorflow.python.training.optimizer', 'tensorflow.python.distribute', 'tensorflow.python.distribute.cluster_resolver', 'tensorflow.python.distribute.cluster_resolver.cluster_resolver', 'tensorflow.python.training.server_lib', 'tensorflow.python.distribute.cluster_resolver.gce_cluster_resolver', 'tensorflow.python.distribute.cluster_resolver.kubernetes_cluster_resolver', 'tensorflow.python.distribute.cluster_resolver.slurm_cluster_resolver', 'tensorflow.python.distribute.cluster_resolver.tfconfig_cluster_resolver', 'tensorflow.python.distribute.cluster_resolver.tpu_cluster_resolver', 'six.moves.urllib', 'six.moves.urllib.error', 'six.moves.urllib.request', 'tensorflow.python.distribute.cross_device_ops', 'tensorflow.python.client.device_lib', 'tensorflow.core.framework.device_attributes_pb2', 'tensorflow.python.distribute.cross_device_utils', 'tensorflow.python.distribute.all_reduce', 'tensorflow.python.ops.nccl_ops', 'tensorflow.python.ops.gen_nccl_ops', 'tensorflow.python.distribute.values', 'tensorflow.python.distribute.device_util', 'tensorflow.python.distribute.distribute_lib', 'tensorflow.python.distribute.distribution_strategy_context', 'tensorflow.python.distribute.numpy_dataset', 'tensorflow.python.distribute.reduce_util', 'tensorflow.python.ops.losses', 'tensorflow.python.ops.losses.loss_reduction', 'tensorflow.python.ops.losses.losses_impl', 'tensorflow.python.ops.nn', 'tensorflow.python.ops.ctc_ops', 'tensorflow.python.ops.gen_ctc_ops', 'tensorflow.python.ops.inplace_ops', 'tensorflow.python.ops.nn_grad', 'tensorflow.python.ops.embedding_ops', 'tensorflow.python.ops.nn_impl', 'tensorflow.python.ops.candidate_sampling_ops', 'tensorflow.python.ops.gen_candidate_sampling_ops', 'tensorflow.python.ops.weights_broadcast_ops', 'tensorflow.python.ops.sets', 'tensorflow.python.ops.sets_impl', 'tensorflow.python.ops.gen_set_ops', 'tensorflow.python.ops.losses.util', 'tensorflow.python.ops.collective_ops', 'tensorflow.python.ops.gen_collective_ops', 'tensorflow.python.distribute.mirrored_strategy', 'tensorflow.python.distribute.input_lib', 'tensorflow.python.data.experimental.ops.distribute', 'tensorflow.python.data.ops.multi_device_iterator_ops', 'tensorflow.python.distribute.input_ops', 'tensorflow.python.distribute.multi_worker_util', 'tensorflow.python.distribute.shared_variable_creator', 'tensorflow.python.training.coordinator', 'tensorflow.python.distribute.one_device_strategy', 'tensorflow.python.distribute.experimental', 'tensorflow.python.distribute.central_storage_strategy', 'tensorflow.python.distribute.parameter_server_strategy', 'tensorflow.python.training.device_setter', 'tensorflow.python.distribute.collective_all_reduce_strategy', 'tensorflow.python.distribute.tpu_strategy', 'tensorflow.python.tpu', 'tensorflow.python.tpu.device_assignment', 'tensorflow.python.tpu.topology', 'tensorflow.core.protobuf.tpu', 'tensorflow.core.protobuf.tpu.topology_pb2', 'tensorflow.python.tpu.tpu', 'tensorflow.core.protobuf.tpu.dynamic_padding_pb2', 'tensorflow.python.compiler', 'tensorflow.python.compiler.xla', 'tensorflow.python.compiler.xla.jit', 'tensorflow.python.compiler.xla.xla', 'tensorflow.compiler.jit', 'tensorflow.compiler.jit.ops', 'tensorflow.compiler.jit.ops.xla_ops', 'tensorflow.compiler.jit.ops.xla_ops_grad', 'tensorflow.python.distribute.summary_op_util', 'tensorflow.python.tpu.tpu_function', 'tensorflow.python.tpu.ops', 'tensorflow.python.tpu.ops.tpu_ops', 'tensorflow.python.ops.gen_tpu_ops', 'tensorflow.python.tpu.tpu_strategy_util', 'tensorflow.python.tpu.tpu_system_metadata', 'tensorflow.python.tpu.training_loop', 'tensorflow.python.tpu.tensor_tracer', 'tensorflow.python.tpu.tensor_tracer_flags', 'tensorflow.python.training.slot_creator', 'tensorflow.python.ops.parallel_for', 'tensorflow.python.ops.parallel_for.control_flow_ops', 'tensorflow.python.ops.parallel_for.pfor', 'tensorflow.python.ops.bitwise_ops', 'tensorflow.python.ops.parallel_for.gradients', 'tensorflow.python.ops.initializers_ns', 'tensorflow.python.keras', 'tensorflow.python.keras.activations', 'tensorflow.python.keras.backend', 'tensorflow.python.distribute.distribute_coordinator', 'tensorflow.python.distribute.distribute_coordinator_context', 'tensorflow.python.training.monitored_session', 'tensorflow.python.ops.resources', 'tensorflow.python.summary.summary', 'tensorflow.python.training.queue_runner', 'tensorflow.python.training.queue_runner_impl', 'tensorflow.core.protobuf.queue_runner_pb2', 'tensorflow.python.training.session_manager', 'tensorflow.python.keras.backend_config', 'tensorflow.python.ops.image_ops', 'tensorflow.python.ops.image_ops_impl', 'tensorflow.python.keras.utils', 'tensorflow.python.keras.utils.data_utils', 'multiprocessing.pool', 'tensorflow.python.keras.utils.generic_utils', 'tensorflow.python.keras.utils.io_utils', 'h5py', 'h5py._errors', 'h5py.version', 'h5py.h5', 'h5py.defs', 'h5py._objects', 'h5py._conv', 'h5py.h5r', 'h5py.h5t', 'h5py.h5p', 'h5py.h5s', 'h5py.utils', 'h5py.h5ac', 'h5py.h5z', 'h5py.h5a', 'h5py._proxy', 'h5py.h5d', 'h5py.h5ds', 'h5py.h5f', 'h5py.h5g', 'h5py.h5i', 'h5py.h5fd', 'h5py.h5pl', 'h5py._hl', 'h5py._hl.filters', 'h5py._hl.compat', 'h5py._hl.base', 'h5py._hl.files', 'h5py._hl.group', 'h5py.h5o', 'h5py.h5l', 'h5py._hl.dataset', 'cached_property', 'h5py._selector', 'h5py._hl.selections', 'h5py._hl.selections2', 'h5py._hl.datatype', 'h5py._hl.vds', 'h5py._hl.attrs', 'tensorflow.python.keras.utils.layer_utils', 'tensorflow.python.keras.utils.conv_utils', 'tensorflow.python.keras.utils.losses_utils', 'tensorflow.python.keras.utils.multi_gpu_utils', 'tensorflow.python.keras.engine', 'tensorflow.python.keras.engine.base_layer', 'tensorflow.python.keras.constraints', 'tensorflow.python.keras.initializers', 'tensorflow.python.ops.init_ops_v2', 'tensorflow.python.keras.regularizers', 'tensorflow.python.keras.engine.base_layer_utils', 'tensorflow.python.ops.control_flow_util_v2', 'tensorflow.python.keras.engine.input_spec', 'tensorflow.python.keras.mixed_precision', 'tensorflow.python.keras.mixed_precision.experimental', 'tensorflow.python.keras.mixed_precision.experimental.loss_scale_optimizer', 'tensorflow.python.keras.optimizer_v2', 'tensorflow.python.keras.optimizer_v2.optimizer_v2', 'tensorflow.python.keras.optimizer_v2.learning_rate_schedule', 'tensorflow.python.keras.utils.tf_utils', 'tensorflow.python.training.experimental.loss_scale', 'tensorflow.python.keras.mixed_precision.experimental.policy', 'tensorflow.python.keras.mixed_precision.experimental.autocast_variable', 'tensorflow.python.module', 'tensorflow.python.module.module', 'tensorflow.python.keras.engine.input_layer', 'tensorflow.python.keras.distribute', 'tensorflow.python.keras.distribute.distributed_training_utils', 'tensorflow.python.keras.callbacks', 'tensorflow.python.keras.utils.mode_keys', 'tensorflow.python.saved_model.model_utils', 'tensorflow.python.saved_model.model_utils.export_output', 'tensorflow.python.saved_model.signature_def_utils', 'tensorflow.python.saved_model.signature_def_utils_impl', 'tensorflow.python.saved_model.signature_constants', 'tensorflow.python.saved_model.utils_impl', 'tensorflow.python.saved_model.constants', 'tensorflow.python.saved_model.model_utils.export_utils', 'tensorflow.python.saved_model.tag_constants', 'tensorflow.python.saved_model.model_utils.mode_keys', 'tensorflow.python.keras.metrics', 'tensorflow.python.keras.losses', 'tensorflow.python.keras.utils.metrics_utils', 'tensorflow.python.keras.optimizers', 'tensorflow.python.keras.optimizer_v2.adadelta', 'tensorflow.python.training.training_ops', 'tensorflow.python.training.gen_training_ops', 'tensorflow.python.keras.optimizer_v2.adagrad', 'tensorflow.python.keras.optimizer_v2.adam', 'tensorflow.python.keras.optimizer_v2.adamax', 'tensorflow.python.keras.optimizer_v2.ftrl', 'tensorflow.python.keras.optimizer_v2.gradient_descent', 'tensorflow.python.keras.optimizer_v2.nadam', 'tensorflow.python.keras.optimizer_v2.rmsprop', 'tensorflow.python.keras.engine.training_utils', 'tensorflow.python.framework.composite_tensor_utils', 'tensorflow.python.keras.engine.training', 'tensorflow.python.eager.monitoring', 'tensorflow.python.keras.engine.network', 'tensorflow.python.keras.saving', 'tensorflow.python.keras.saving.hdf5_format', 'tensorflow.python.keras.saving.model_config', 'tensorflow.python.util.serialization', 'tensorflow.python.keras.saving.save', 'tensorflow.python.keras.saving.saved_model', 'tensorflow.python.keras.saving.saving_utils', 'tensorflow.python.saved_model.builder', 'tensorflow.python.saved_model.builder_impl', 'tensorflow.core.protobuf.saved_model_pb2', 'tensorflow.python.saved_model.save', 'tensorflow.python.saved_model.function_serialization', 'tensorflow.python.saved_model.nested_structure_coder', 'tensorflow.python.saved_model.signature_serialization', 'tensorflow.python.saved_model.loader_impl', 'tensorflow.python.keras.engine.training_arrays', 'scipy', 'scipy._lib', 'scipy._lib._testutils', 'scipy._lib.deprecation', 'scipy.__config__', 'scipy.version', 'scipy._distributor_init', 'scipy._lib._pep440', 'scipy._lib._ccallback', 'scipy._lib._ccallback_c', 'scipy.fft', 'scipy.fft._basic', 'scipy._lib.uarray', 'scipy._lib._uarray', 'scipy._lib._uarray._backend', 'scipy._lib._uarray._uarray', 'scipy.fft._realtransforms', 'scipy.fft._helper', 'scipy.fft._pocketfft', 'scipy.fft._pocketfft.basic', 'scipy.fft._pocketfft.pypocketfft', 'scipy.fft._pocketfft.helper', 'scipy.fft._pocketfft.realtransforms', 'scipy.fft._backend', 'scipy.sparse', 'scipy.sparse.base', 'scipy.sparse.sputils', 'scipy._lib._util', 'scipy.sparse.csr', 'scipy.sparse._sparsetools', 'scipy.sparse.compressed', 'scipy.sparse.data', 'scipy.sparse.dia', 'scipy.sparse._index', 'scipy.sparse.csc', 'scipy.sparse.lil', 'scipy.sparse._csparsetools', 'scipy.sparse.dok', 'scipy.sparse.coo', 'scipy.sparse.bsr', 'scipy.sparse.construct', 'scipy.sparse.extract', 'scipy.sparse._matrix_io', 'scipy.sparse.csgraph', 'scipy.sparse.csgraph._laplacian', 'scipy.sparse.csgraph._shortest_path', '_cython_0_29_18', 'scipy.sparse.csgraph._validation', 'scipy.sparse.csgraph._tools', 'scipy.sparse.csgraph._traversal', 'scipy.sparse.csgraph._min_spanning_tree', 'scipy.sparse.csgraph._flow', 'scipy.sparse.csgraph._matching', 'scipy.sparse.csgraph._reordering', 'tensorflow.python.keras.engine.training_distributed', 'tensorflow.python.keras.engine.partial_batch_padding_handler', 'tensorflow.python.keras.engine.training_eager', 'tensorflow.python.keras.engine.training_generator', 'tensorflow.python.keras.utils.np_utils', 'tensorflow.python.keras.utils.vis_utils', 'tensorflow.python.keras.applications', 'keras_applications', 'keras_applications.vgg16', 'keras_applications.imagenet_utils', 'keras_applications.vgg19', 'keras_applications.resnet50', 'keras_applications.inception_v3', 'keras_applications.inception_resnet_v2', 'keras_applications.xception', 'keras_applications.mobilenet', 'keras_applications.mobilenet_v2', 'keras_applications.densenet', 'keras_applications.nasnet', 'keras_applications.resnet', 'keras_applications.resnet_common', 'keras_applications.resnet_v2', 'keras_applications.resnext', 'tensorflow.python.keras.layers', 'tensorflow.python.keras.layers.advanced_activations', 'tensorflow.python.keras.layers.convolutional', 'tensorflow.python.keras.layers.pooling', 'tensorflow.python.keras.layers.core', 'tensorflow.python.keras.layers.dense_attention', 'tensorflow.python.keras.layers.embeddings', 'tensorflow.python.keras.layers.local', 'tensorflow.python.keras.layers.merge', 'tensorflow.python.keras.layers.noise', 'tensorflow.python.keras.layers.normalization', 'tensorflow.python.keras.layers.normalization_v2', 'tensorflow.python.keras.layers.kernelized', 'tensorflow.python.keras.layers.recurrent', 'tensorflow.python.keras.layers.recurrent_v2', 'tensorflow.python.keras.layers.convolutional_recurrent', 'tensorflow.python.keras.layers.cudnn_recurrent', 'tensorflow.python.keras.layers.wrappers', 'tensorflow.python.keras.layers.serialization', 'tensorflow.python.keras.models', 'tensorflow.python.keras.engine.sequential', 'tensorflow.python.keras.applications.densenet', 'tensorflow.python.keras.applications.inception_resnet_v2', 'tensorflow.python.keras.applications.inception_v3', 'tensorflow.python.keras.applications.mobilenet', 'tensorflow.python.keras.applications.mobilenet_v2', 'tensorflow.python.keras.applications.nasnet', 'tensorflow.python.keras.applications.resnet50', 'tensorflow.python.keras.applications.vgg16', 'tensorflow.python.keras.applications.vgg19', 'tensorflow.python.keras.applications.xception', 'tensorflow.python.keras.callbacks_v1', 'tensorflow.python.keras.datasets', 'tensorflow.python.keras.datasets.boston_housing', 'tensorflow.python.keras.datasets.cifar10', 'tensorflow.python.keras.datasets.cifar', 'tensorflow.python.keras.datasets.cifar100', 'tensorflow.python.keras.datasets.fashion_mnist', 'tensorflow.python.keras.datasets.imdb', 'tensorflow.python.keras.preprocessing', 'keras_preprocessing', 'tensorflow.python.keras.preprocessing.image', 'keras_preprocessing.image', 'keras_preprocessing.image.affine_transformations', 'keras_preprocessing.image.utils', 'PIL.ImageEnhance', 'PIL.ImageFilter', 'PIL.ImageStat', 'scipy.ndimage', 'scipy.ndimage.filters', 'scipy.ndimage._ni_support', 'scipy.ndimage._nd_image', 'scipy.ndimage._ni_docstrings', 'scipy._lib.doccer', 'scipy.ndimage.fourier', 'scipy.ndimage.interpolation', 'scipy.special', 'scipy.special.sf_error', 'scipy.special._ufuncs', 'scipy.special._ufuncs_cxx', 'scipy.special._basic', 'scipy.special.specfun', 'scipy.special.orthogonal', 'scipy.linalg', 'scipy.linalg.misc', 'scipy.linalg.blas', 'scipy.linalg._fblas', 'scipy.linalg.lapack', 'scipy.linalg._flapack', 'scipy.linalg.basic', 'scipy.linalg.flinalg', 'scipy.linalg._flinalg', 'scipy.linalg.decomp', 'scipy.linalg.decomp_svd', 'scipy.linalg._solve_toeplitz', 'scipy.linalg.decomp_lu', 'scipy.linalg._decomp_ldl', 'scipy.linalg.decomp_cholesky', 'scipy.linalg.decomp_qr', 'scipy.linalg._decomp_qz', 'scipy.linalg.decomp_schur', 'scipy.linalg._decomp_polar', 'scipy.linalg.matfuncs', 'scipy.linalg.special_matrices', 'scipy.linalg._expm_frechet', 'scipy.linalg._matfuncs_sqrtm', 'scipy.linalg._solvers', 'scipy.linalg._procrustes', 'scipy.linalg._decomp_update', 'scipy.linalg.cython_blas', 'scipy.linalg.cython_lapack', 'scipy.linalg._sketches', 'scipy.linalg._decomp_cossin', 'scipy.special._comb', 'scipy.special._logsumexp', 'scipy.special.spfun_stats', 'scipy.special._ellip_harm', 'scipy.special._ellip_harm_2', 'scipy.special._lambertw', 'scipy.special._spherical_bessel', 'scipy.ndimage.measurements', 'scipy.ndimage._ni_label', '_ni_label', 'scipy.ndimage.morphology', 'keras_preprocessing.image.dataframe_iterator', 'keras_preprocessing.image.iterator', 'keras_preprocessing.image.directory_iterator', 'keras_preprocessing.image.image_data_generator', 'keras_preprocessing.image.numpy_array_iterator', 'tensorflow.python.keras.preprocessing.sequence', 'keras_preprocessing.sequence', 'tensorflow.python.keras.preprocessing.text', 'keras_preprocessing.text', 'tensorflow.python.keras.datasets.mnist', 'tensorflow.python.keras.datasets.reuters', 'tensorflow.python.keras.estimator', 'tensorflow.python.keras.ops', 'tensorflow.python.keras.wrappers', 'tensorflow.python.keras.wrappers.scikit_learn', 'tensorflow.python.feature_column', 'tensorflow.python.feature_column.feature_column_lib', 'tensorflow.python.feature_column.feature_column', 'tensorflow.python.feature_column.utils', 'tensorflow.python.layers', 'tensorflow.python.layers.base', 'tensorflow.python.training.checkpoint_utils', 'tensorflow.python.feature_column.feature_column_v2', 'tensorflow.python.feature_column.sequence_feature_column', 'tensorflow.python.layers.layers', 'tensorflow.python.layers.core', 'tensorflow.python.layers.convolutional', 'tensorflow.python.layers.pooling', 'tensorflow.python.layers.normalization', 'tensorflow.python.ops.gradient_checker_v2', 'tensorflow.python.ops.metrics', 'tensorflow.python.ops.metrics_impl', 'tensorflow.python.ops.stateful_random_ops', 'tensorflow.python.ops.gen_stateful_random_ops', 'tensorflow.python.ops.distributions', 'tensorflow.python.ops.distributions.distributions', 'tensorflow.python.ops.distributions.bernoulli', 'tensorflow.python.ops.distributions.distribution', 'tensorflow.python.ops.distributions.kullback_leibler', 'tensorflow.python.ops.distributions.util', 'tensorflow.python.ops.distributions.beta', 'tensorflow.python.ops.distributions.categorical', 'tensorflow.python.ops.distributions.dirichlet', 'tensorflow.python.ops.distributions.dirichlet_multinomial', 'tensorflow.python.ops.distributions.exponential', 'tensorflow.python.ops.distributions.gamma', 'tensorflow.python.ops.distributions.laplace', 'tensorflow.python.ops.distributions.special_math', 'tensorflow.python.ops.distributions.multinomial', 'tensorflow.python.ops.distributions.normal', 'tensorflow.python.ops.distributions.student_t', 'tensorflow.python.ops.distributions.uniform', 'tensorflow.python.ops.linalg.linalg', 'tensorflow.python.ops.linalg.adjoint_registrations', 'tensorflow.python.ops.linalg.linear_operator', 'tensorflow.python.ops.linalg.linear_operator_algebra', 'tensorflow.python.ops.linalg.linear_operator_util', 'tensorflow.python.ops.linalg.linear_operator_adjoint', 'tensorflow.python.ops.linalg.linear_operator_block_diag', 'tensorflow.python.ops.linalg.linear_operator_circulant', 'tensorflow.python.ops.signal', 'tensorflow.python.ops.signal.fft_ops', 'tensorflow.python.ops.gen_spectral_ops', 'tensorflow.python.ops.linalg.linear_operator_diag', 'tensorflow.python.ops.linalg.linear_operator_householder', 'tensorflow.python.ops.linalg.linear_operator_identity', 'tensorflow.python.ops.linalg.linear_operator_kronecker', 'tensorflow.python.ops.linalg.cholesky_registrations', 'tensorflow.python.ops.linalg.linear_operator_lower_triangular', 'tensorflow.python.ops.linalg.inverse_registrations', 'tensorflow.python.ops.linalg.linear_operator_inversion', 'tensorflow.python.ops.linalg.matmul_registrations', 'tensorflow.python.ops.linalg.linear_operator_composition', 'tensorflow.python.ops.linalg.linear_operator_zeros', 'tensorflow.python.ops.linalg.registrations_util', 'tensorflow.python.ops.linalg.solve_registrations', 'tensorflow.python.ops.linalg.linear_operator_full_matrix', 'tensorflow.python.ops.linalg.linear_operator_low_rank_update', 'tensorflow.python.ops.linalg.linear_operator_toeplitz', 'tensorflow.python.ops.losses.losses', 'tensorflow.python.ops.signal.signal', 'tensorflow.python.ops.signal.dct_ops', 'tensorflow.python.ops.signal.mel_ops', 'tensorflow.python.ops.signal.shape_ops', 'tensorflow.python.ops.signal.util_ops', 'fractions', 'tensorflow.python.ops.signal.mfcc_ops', 'tensorflow.python.ops.signal.reconstruction_ops', 'tensorflow.python.ops.signal.spectral_ops', 'tensorflow.python.ops.signal.window_ops', 'tensorflow.python.profiler', 'tensorflow.python.profiler.profiler', 'tensorflow.core.profiler', 'tensorflow.core.profiler.tfprof_log_pb2', 'tensorflow.core.profiler.tfprof_output_pb2', 'tensorflow.python.profiler.model_analyzer', 'tensorflow.core.profiler.tfprof_options_pb2', 'tensorflow.python.profiler.option_builder', 'tensorflow.python.profiler.tfprof_logger', 'tensorflow.python.profiler.internal', 'tensorflow.python.profiler.internal.flops_registry', 'tensorflow.python.saved_model.saved_model', 'tensorflow.python.saved_model.loader', 'tensorflow.python.saved_model.main_op', 'tensorflow.python.saved_model.main_op_impl', 'tensorflow.python.saved_model.utils', 'tensorflow.python.saved_model.load', 'tensorflow.python.saved_model.function_deserialization', 'tensorflow.python.framework.function_def_to_graph', 'tensorflow.python.saved_model.load_v1_in_v2', 'tensorflow.python.saved_model.simple_save', 'tensorflow.python.tpu.bfloat16', 'tensorflow.python.tpu.tpu_optimizer', 'tensorflow.python.user_ops', 'tensorflow.python.user_ops.user_ops', 'tensorflow.python.ops.gen_user_ops', 'tensorflow.python.ops.gen_audio_ops', 'tensorflow.python.ops.gen_boosted_trees_ops', 'tensorflow.python.training.training', 'tensorflow.python.ops.sdca_ops', 'tensorflow.python.ops.gen_sdca_ops', 'tensorflow.python.training.adadelta', 'tensorflow.python.training.adagrad', 'tensorflow.python.training.adagrad_da', 'tensorflow.python.training.proximal_adagrad', 'tensorflow.python.training.adam', 'tensorflow.python.training.ftrl', 'tensorflow.python.training.experimental.loss_scale_optimizer', 'tensorflow.python.training.experimental.mixed_precision', 'tensorflow.python.training.momentum', 'tensorflow.python.training.moving_averages', 'tensorflow.python.training.rmsprop', 'tensorflow.python.training.gradient_descent', 'tensorflow.python.training.proximal_gradient_descent', 'tensorflow.python.training.sync_replicas_optimizer', 'tensorflow.python.training.input', 'tensorflow.python.layers.utils', 'tensorflow.python.training.basic_loops', 'tensorflow.python.training.tracking.python_state', 'tensorflow.python.training.supervisor', 'tensorflow.python.training.warm_starting_util', 'tensorflow.python.training.checkpoint_ops', 'tensorflow.python.ops.gen_checkpoint_ops', 'tensorflow.core.example', 'tensorflow.core.example.example_pb2', 'tensorflow.core.example.feature_pb2', 'tensorflow.python.training.learning_rate_decay', 'tensorflow.python.platform.resource_loader', 'tensorflow.python.platform.sysconfig', 'tensorflow.python.platform.test', 'tensorflow.python.framework.test_util', 'absl.testing', 'absl.testing.parameterized', 'absl.testing.absltest', 'shlex', 'unittest.mock', 'absl.testing._pretty_print_reporter', 'absl.testing.xml_reporter', 'xml.sax', 'xml.sax.xmlreader', 'xml.sax.handler', 'xml.sax._exceptions', 'xml.sax.saxutils', 'tensorflow.python.platform.googletest', 'tensorflow.python.platform.benchmark', 'tensorflow.core.util.test_log_pb2', 'google.protobuf.wrappers_pb2', 'tensorflow.python.util.protobuf', 'tensorflow.python.util.protobuf.compare', 'tensorflow.python.ops.gradient_checker', 'tensorflow.python.compat.v2_compat', 'tensorflow.python.eager.remote', 'tensorflow.python.ops.rnn', 'tensorflow.python.ops.rnn_cell_impl', 'tensorflow.python.ops.rnn_cell', 'tensorflow.python.tools', 'tensorflow.python.tools.module_util', 'tensorflow._api', 'tensorflow._api.v1', 'tensorflow._api.v1.app', 'tensorflow.python.util.deprecation_wrapper', 'tensorflow.tools.compatibility', 'tensorflow.tools.compatibility.all_renames_v2', 'tensorflow.tools.compatibility.renames_v2', 'tensorflow._api.v1.audio', 'tensorflow._api.v1.autograph', 'tensorflow._api.v1.autograph.experimental', 'tensorflow._api.v1.bitwise', 'tensorflow._api.v1.compat', 'tensorflow._api.v1.compat.v1', 'tensorflow._api.v1.compat.v1.app', 'tensorflow._api.v1.compat.v1.audio', 'tensorflow._api.v1.compat.v1.autograph', 'tensorflow._api.v1.compat.v1.autograph.experimental', 'tensorflow._api.v1.compat.v1.bitwise', 'tensorflow._api.v1.compat.v1.compat', 'tensorflow._api.v1.compat.v1.config', 'tensorflow._api.v1.compat.v1.config.experimental', 'tensorflow._api.v1.compat.v1.config.optimizer', 'tensorflow._api.v1.compat.v1.config.threading', 'tensorflow._api.v1.compat.v1.data', 'tensorflow._api.v1.compat.v1.data.experimental', 'tensorflow._api.v1.compat.v1.debugging', 'tensorflow._api.v1.compat.v1.distribute', 'tensorflow._api.v1.compat.v1.distribute.cluster_resolver', 'tensorflow._api.v1.compat.v1.distribute.experimental', 'tensorflow._api.v1.compat.v1.distributions', 'tensorflow._api.v1.compat.v1.dtypes', 'tensorflow._api.v1.compat.v1.errors', 'tensorflow.lite', 'tensorflow.lite.python', 'tensorflow.lite.python.lite', 'tensorflow.lite.experimental', 'tensorflow.lite.experimental.examples', 'tensorflow.lite.experimental.examples.lstm', 'tensorflow.lite.experimental.examples.lstm.rnn', 'tensorflow.lite.python.op_hint', 'tensorflow.lite.experimental.examples.lstm.rnn_cell', 'tensorflow.lite.experimental.tensorboard', 'tensorflow.lite.experimental.tensorboard.ops_util', 'tensorflow.lite.python.wrap_toco', 'tensorflow.lite.python.lite_constants', 'tensorflow.lite.toco', 'tensorflow.lite.toco.toco_flags_pb2', 'tensorflow.lite.toco.types_pb2', 'tensorflow.lite.python.convert', 'tensorflow.lite.python.util', 'tensorflow.python.grappler', 'tensorflow.python.grappler.tf_optimizer', 'tensorflow.python.grappler.cluster', 'tensorflow.core.grappler', 'tensorflow.core.grappler.costs', 'tensorflow.core.grappler.costs.op_performance_data_pb2', 'tensorflow.core.protobuf.device_properties_pb2', 'tensorflow.lite.toco.model_flags_pb2', 'tensorflow.lite.python.convert_saved_model', 'tensorflow.lite.python.interpreter', 'tensorflow.lite.python.optimize', 'tensorflow.lite.python.optimize.calibrator', 'tensorflow.python.framework.convert_to_constants', 'tensorflow._api.v1.compat.v1.experimental', 'tensorflow._api.v1.compat.v1.feature_column', 'tensorflow._api.v1.compat.v1.gfile', 'tensorflow._api.v1.compat.v1.graph_util', 'tensorflow._api.v1.compat.v1.image', 'tensorflow._api.v1.compat.v1.initializers', 'tensorflow._api.v1.compat.v1.io', 'tensorflow._api.v1.compat.v1.io.gfile', 'tensorflow._api.v1.compat.v1.layers', 'tensorflow._api.v1.compat.v1.layers.experimental', 'tensorflow._api.v1.compat.v1.linalg', 'tensorflow._api.v1.compat.v1.lite', 'tensorflow._api.v1.compat.v1.lite.constants', 'tensorflow._api.v1.compat.v1.lite.experimental', 'tensorflow._api.v1.compat.v1.lite.experimental.nn', 'tensorflow._api.v1.compat.v1.logging', 'tensorflow._api.v1.compat.v1.lookup', 'tensorflow._api.v1.compat.v1.lookup.experimental', 'tensorflow._api.v1.compat.v1.losses', 'tensorflow._api.v1.compat.v1.manip', 'tensorflow._api.v1.compat.v1.math', 'tensorflow._api.v1.compat.v1.metrics', 'tensorflow._api.v1.compat.v1.nest', 'tensorflow._api.v1.compat.v1.nn', 'tensorflow._api.v1.compat.v1.nn.rnn_cell', 'tensorflow._api.v1.compat.v1.profiler', 'tensorflow._api.v1.compat.v1.python_io', 'tensorflow._api.v1.compat.v1.quantization', 'tensorflow._api.v1.compat.v1.queue', 'tensorflow._api.v1.compat.v1.ragged', 'tensorflow._api.v1.compat.v1.random', 'tensorflow._api.v1.compat.v1.random.experimental', 'tensorflow._api.v1.compat.v1.raw_ops', 'tensorflow._api.v1.compat.v1.resource_loader', 'tensorflow._api.v1.compat.v1.saved_model', 'tensorflow._api.v1.compat.v1.saved_model.builder', 'tensorflow._api.v1.compat.v1.saved_model.constants', 'tensorflow._api.v1.compat.v1.saved_model.experimental', 'tensorflow._api.v1.compat.v1.saved_model.loader', 'tensorflow._api.v1.compat.v1.saved_model.main_op', 'tensorflow._api.v1.compat.v1.saved_model.signature_constants', 'tensorflow._api.v1.compat.v1.saved_model.signature_def_utils', 'tensorflow._api.v1.compat.v1.saved_model.tag_constants', 'tensorflow._api.v1.compat.v1.saved_model.utils', 'tensorflow._api.v1.compat.v1.sets', 'tensorflow._api.v1.compat.v1.signal', 'tensorflow._api.v1.compat.v1.sparse', 'tensorflow._api.v1.compat.v1.spectral', 'tensorflow._api.v1.compat.v1.strings', 'tensorflow._api.v1.compat.v1.summary', 'tensorflow._api.v1.compat.v1.sysconfig', 'tensorflow._api.v1.compat.v1.test', 'tensorflow._api.v1.compat.v1.tpu', 'tensorflow._api.v1.compat.v1.tpu.experimental', 'tensorflow._api.v1.compat.v1.train', 'tensorflow._api.v1.compat.v1.train.experimental', 'tensorflow._api.v1.compat.v1.train.queue_runner', 'tensorflow._api.v1.compat.v1.user_ops', 'tensorflow._api.v1.compat.v1.version', 'tensorflow._api.v1.compat.v1.xla', 'tensorflow._api.v1.compat.v1.xla.experimental', 'tensorflow_estimator', 'tensorflow_estimator._api', 'tensorflow_estimator._api.v1', 'tensorflow_estimator._api.v1.estimator', 'tensorflow_estimator._api.v1.estimator.experimental', 'tensorflow_estimator.python', 'tensorflow_estimator.python.estimator', 'tensorflow_estimator.python.estimator.estimator_lib', 'tensorflow_estimator.python.estimator.canned', 'tensorflow_estimator.python.estimator.canned.baseline', 'tensorflow_estimator.python.estimator.estimator', 'tensorflow.python.distribute.estimator_training', 'tensorflow.python.training.evaluation', 'tensorflow.python.util.compat_internal', 'tensorflow_estimator.python.estimator.model_fn', 'tensorflow_estimator.python.estimator.mode_keys', 'tensorflow_estimator.python.estimator.run_config', 'tensorflow_estimator.python.estimator.util', 'tensorflow_estimator.python.estimator.export', 'tensorflow_estimator.python.estimator.export.export_lib', 'tensorflow_estimator.python.estimator.export.export', 'tensorflow_estimator.python.estimator.canned.head', 'tensorflow_estimator.python.estimator.canned.metric_keys', 'tensorflow_estimator.python.estimator.canned.prediction_keys', 'tensorflow_estimator.python.estimator.export.export_output', 'tensorflow_estimator.python.estimator.canned.optimizers', 'tensorflow_estimator.python.estimator.head', 'tensorflow_estimator.python.estimator.head.head_utils', 'tensorflow_estimator.python.estimator.head.binary_class_head', 'tensorflow_estimator.python.estimator.head.base_head', 'tensorflow_estimator.python.estimator.head.multi_class_head', 'tensorflow_estimator.python.estimator.head.regression_head', 'tensorflow_estimator.python.estimator.canned.boosted_trees', 'tensorflow.core.kernels', 'tensorflow.core.kernels.boosted_trees', 'tensorflow.core.kernels.boosted_trees.boosted_trees_pb2', 'tensorflow.python.ops.boosted_trees_ops', 'tensorflow_estimator.python.estimator.canned.boosted_trees_utils', 'tensorflow_estimator.python.estimator.canned.dnn', 'tensorflow_estimator.python.estimator.canned.dnn_linear_combined', 'tensorflow_estimator.python.estimator.canned.linear', 'tensorflow_estimator.python.estimator.canned.linear_optimizer', 'tensorflow_estimator.python.estimator.canned.linear_optimizer.python', 'tensorflow_estimator.python.estimator.canned.linear_optimizer.python.utils', 'tensorflow_estimator.python.estimator.canned.linear_optimizer.python.utils.sdca_ops', 'tensorflow_estimator.python.estimator.canned.linear_optimizer.python.utils.sharded_mutable_dense_hashtable', 'tensorflow_estimator.python.estimator.canned.kmeans', 'tensorflow.python.ops.clustering_ops', 'tensorflow.python.ops.gen_clustering_ops', 'tensorflow_estimator.python.estimator.canned.parsing_utils', 'tensorflow_estimator.python.estimator.early_stopping', 'tensorflow_estimator.python.estimator.exporter', 'tensorflow_estimator.python.estimator.gc', 'tensorflow_estimator.python.estimator.extenders', 'tensorflow_estimator.python.estimator.hooks', 'tensorflow_estimator.python.estimator.hooks.basic_session_run_hooks', 'tensorflow_estimator.python.estimator.hooks.hooks', 'tensorflow_estimator.python.estimator.hooks.session_run_hook', 'tensorflow_estimator.python.estimator.inputs', 'tensorflow_estimator.python.estimator.inputs.inputs', 'tensorflow_estimator.python.estimator.inputs.numpy_io', 'tensorflow_estimator.python.estimator.inputs.queues', 'tensorflow_estimator.python.estimator.inputs.queues.feeding_functions', 'tensorflow_estimator.python.estimator.inputs.queues.feeding_queue_runner', 'pandas', 'pytz', 'pytz.exceptions', 'pytz.lazy', 'pytz.tzinfo', 'pytz.tzfile', 'dateutil', 'dateutil._version', 'pandas.compat', 'pandas._typing', 'pandas.compat.numpy', 'pandas._libs', 'pandas._libs.interval', 'pandas._libs.hashtable', 'pandas._libs.missing', 'pandas._libs.tslibs', 'pandas._libs.tslibs.dtypes', 'pandas._libs.tslibs.conversion', 'pandas._libs.tslibs.base', 'pandas._libs.tslibs.nattype', 'pandas._libs.tslibs.np_datetime', 'pandas._libs.tslibs.timezones', 'dateutil.tz', 'dateutil.tz.tz', 'dateutil.tz._common', 'dateutil.tz._factories', 'pandas._libs.tslibs.tzconversion', 'pandas._libs.tslibs.ccalendar', 'pandas._libs.tslibs.parsing', 'pandas._libs.tslibs.offsets', 'pandas._libs.tslibs.timedeltas', 'pandas._libs.tslibs.timestamps', 'pandas._libs.tslibs.fields', 'pandas._config', 'pandas._config.config', 'pandas._config.dates', 'pandas._config.display', 'pandas._config.localization', 'pandas._libs.tslibs.strptime', 'dateutil.easter', 'dateutil.relativedelta', 'dateutil._common', 'pandas._libs.properties', 'dateutil.parser', 'dateutil.parser._parser', 'dateutil.parser.isoparser', 'pandas._libs.tslibs.period', 'pandas._libs.tslibs.vectorized', 'pandas._libs.ops_dispatch', 'pandas._libs.algos', 'pandas._libs.lib', 'pandas._libs.tslib', 'pandas.core', 'pandas.core.config_init', 'pandas.core.api', 'pandas.core.dtypes', 'pandas.core.dtypes.dtypes', 'pandas.core.dtypes.base', 'pandas.errors', 'pandas.core.dtypes.generic', 'pandas.core.dtypes.inference', 'pandas.core.dtypes.missing', 'pandas.core.dtypes.common', 'pandas.core.algorithms', 'pandas.util', 'pandas.util._decorators', 'pandas.core.util', 'pandas.core.util.hashing', 'pandas._libs.hashing', 'pandas.core.dtypes.cast', 'pandas.util._validators', 'pandas.core.construction', 'pandas.core.common', 'pandas.core.indexers', 'pandas.core.arrays', 'pandas.core.arrays.base', 'pandas.compat.numpy.function', 'pandas.core.ops', 'pandas.core.ops.array_ops', 'pandas._libs.ops', 'pandas.core.ops.missing', 'pandas.core.ops.roperator', 'pandas.core.ops.dispatch', 'pandas.core.ops.invalid', 'pandas.core.ops.common', 'pandas.core.ops.docstrings', 'pandas.core.ops.mask_ops', 'pandas.core.ops.methods', 'pandas.core.missing', 'pandas.compat._optional', 'pandas.core.sorting', 'pandas.core.arrays.boolean', 'pandas.core.arrays.masked', 'pandas.core.nanops', 'pandas.core.array_algos', 'pandas.core.array_algos.masked_reductions', 'pandas.core.arrays.categorical', 'pandas.core.accessor', 'pandas.core.array_algos.transforms', 'pandas.core.arrays._mixins', 'pandas.core.base', 'pandas.io', 'pandas.io.formats', 'pandas.io.formats.console', 'pandas.core.arrays.datetimes', 'pandas.core.arrays.datetimelike', 'pandas.tseries', 'pandas.tseries.frequencies', 'pandas.core.arrays._ranges', 'pandas.tseries.offsets', 'pandas.core.arrays.integer', 'pandas.core.tools', 'pandas.core.tools.numeric', 'pandas.core.arrays.interval', 'pandas.core.indexes', 'pandas.core.indexes.base', 'pandas._libs.index', 'pandas._libs.join', 'pandas.core.dtypes.concat', 'pandas.core.arrays.sparse', 'pandas.core.arrays.sparse.accessor', 'pandas.core.arrays.sparse.array', 'pandas._libs.sparse', 'pandas.core.arrays.sparse.dtype', 'pandas.io.formats.printing', 'pandas.core.indexes.frozen', 'pandas.core.strings', 'pandas.core.arrays.numpy_', 'pandas.core.arrays.period', 'pandas.core.arrays.string_', 'pandas.core.arrays.timedeltas', 'pandas.core.groupby', 'pandas.core.groupby.generic', 'pandas.core.aggregation', 'pandas.core.indexes.api', 'pandas.core.indexes.category', 'pandas.core.indexes.extension', 'pandas.core.indexes.datetimes', 'pandas.core.indexes.datetimelike', 'pandas.core.indexes.numeric', 'pandas.core.tools.timedeltas', 'pandas.core.tools.times', 'pandas.core.indexes.interval', 'pandas.util._exceptions', 'pandas.core.indexes.multi', 'pandas.core.indexes.timedeltas', 'pandas.core.indexes.period', 'pandas.core.indexes.range', 'pandas.core.series', 'pandas._libs.reshape', 'pandas.core.generic', 'pandas.core.indexing', 'pandas._libs.indexing', 'pandas.core.internals', 'pandas.core.internals.blocks', 'pandas._libs.writers', 'pandas._libs.internals', 'pandas.core.internals.concat', 'pandas.core.internals.managers', 'pandas.core.internals.ops', 'pandas.core.shared_docs', 'pandas.io.formats.format', 'pandas.io.common', 'mmap', 'pandas.core.indexes.accessors', 'pandas.core.tools.datetimes', 'pandas.arrays', 'pandas.plotting', 'pandas.plotting._core', 'pandas.plotting._misc', 'pandas.core.window', 'pandas.core.window.ewm', 'pandas._libs.window', 'pandas._libs.window.aggregations', 'pandas.core.window.common', 'pandas.core.groupby.base', 'pandas.core.window.rolling', 'pandas.core.util.numba_', 'pandas.core.window.indexers', 'pandas._libs.window.indexers', 'pandas.core.window.numba_', 'pandas.core.window.expanding', 'pandas.core.frame', 'pandas.core.internals.construction', 'pandas.core.reshape', 'pandas.core.reshape.melt', 'pandas.core.reshape.concat', 'pandas.core.reshape.util', 'pandas.io.formats.info', 'pandas.core.groupby.groupby', 'pandas._libs.groupby', 'pandas.core.groupby.ops', 'pandas._libs.reduction', 'pandas.core.groupby.grouper', 'pandas.core.groupby.categorical', 'pandas.tseries.api', 'pandas.core.computation', 'pandas.core.computation.api', 'pandas.core.computation.eval', 'pandas.core.computation.engines', 'pandas.core.computation.align', 'pandas.core.computation.common', 'pandas.core.computation.ops', 'pandas.core.computation.scope', 'pandas.compat.chainmap', 'pandas.core.computation.expr', 'pandas.core.computation.parsing', 'pandas.core.reshape.api', 'pandas.core.reshape.merge', 'pandas.core.reshape.pivot', 'pandas.core.reshape.reshape', 'pandas.core.reshape.tile', 'pandas.api', 'pandas.api.extensions', 'pandas.api.indexers', 'pandas.api.types', 'pandas.core.dtypes.api', 'pandas.util._print_versions', 'pandas.io.api', 'pandas.io.clipboards', 'pandas.io.excel', 'pandas.io.excel._base', 'pandas._libs.parsers', 'pandas.io.excel._util', 'pandas.io.parsers', 'pandas.io.date_converters', 'pandas.io.excel._odfreader', 'pandas.io.excel._openpyxl', 'pandas.io.excel._pyxlsb', 'pandas.io.excel._xlrd', 'pandas.io.excel._odswriter', 'pandas._libs.json', 'pandas.io.formats.excel', 'pandas.io.formats.css', 'pandas.io.excel._xlsxwriter', 'pandas.io.excel._xlwt', 'pandas.io.feather_format', 'pandas.io.gbq', 'pandas.io.html', 'pandas.io.json', 'pandas.io.json._json', 'pandas.io.json._normalize', 'pandas.io.json._table_schema', 'pandas.io.orc', 'pandas.io.parquet', 'pandas.io.pickle', 'pandas.compat.pickle_compat', 'pandas.io.pytables', 'pandas.core.computation.pytables', 'pandas.io.sas', 'pandas.io.sas.sasreader', 'pandas.io.spss', 'pandas.io.sql', 'pandas.io.stata', 'pandas.util._tester', 'pandas.testing', 'pandas._testing', 'pandas._libs.testing', 'pandas._version', 'tensorflow_estimator.python.estimator.inputs.pandas_io', 'tensorflow_estimator.python.estimator.keras', 'tensorflow_estimator.python.estimator.tpu', 'tensorflow_estimator.python.estimator.tpu.tpu_estimator', 'tensorflow.core.protobuf.tpu.compilation_result_pb2', 'tensorflow.compiler.xla', 'tensorflow.compiler.xla.service', 'tensorflow.compiler.xla.service.hlo_pb2', 'tensorflow.compiler.xla.xla_data_pb2', 'tensorflow.python.tpu.functional', 'tensorflow.python.tpu.preempted_hook', 'tensorflow.python.tpu.session_support', 'tensorflow.python.tpu.tpu_embedding_gradient', 'tensorflow.python.tpu.tpu_feed', 'tensorflow.compiler.xla.experimental', 'tensorflow.compiler.xla.experimental.xla_sharding', 'tensorflow.compiler.xla.experimental.xla_sharding.xla_sharding', 'tensorflow.python.tpu.tpu_sharding', 'tensorflow_estimator.python.estimator.tpu._tpu_estimator_embedding', 'tensorflow.python.tpu.feature_column', 'tensorflow.python.tpu.tpu_embedding', 'tensorflow.core.protobuf.tpu.optimization_parameters_pb2', 'tensorflow.core.protobuf.tpu.tpu_embedding_configuration_pb2', 'tensorflow.core.protobuf.tpu.tpu_embedding_output_layout_pb2', 'tensorflow_estimator.python.estimator.tpu.error_handling', 'tensorflow_estimator.python.estimator.tpu.iteration_count_estimator', 'tensorflow_estimator.python.estimator.tpu.tpu_config', 'tensorflow_estimator.python.estimator.tpu.util', 'tensorflow_estimator.python.estimator.tpu.tpu_context', 'tensorflow_estimator.python.estimator.training', 'tensorflow_estimator._api.v1.estimator.export', 'tensorflow_estimator._api.v1.estimator.inputs', 'tensorflow_estimator._api.v1.estimator.tpu', 'tensorflow_estimator.python.estimator.api', 'tensorflow_estimator.python.estimator.api._v1', 'tensorflow_estimator.python.estimator.api._v1.estimator', 'tensorflow_estimator.python.estimator.api._v1.estimator.experimental', 'tensorflow_estimator.python.estimator.api._v1.estimator.export', 'tensorflow_estimator.python.estimator.api._v1.estimator.inputs', 'tensorflow_estimator.python.estimator.api._v1.estimator.tpu', 'tensorflow.python.keras.api', 'tensorflow.python.keras.api.keras', 'tensorflow.python.keras.api.keras.activations', 'tensorflow.python.keras.api.keras.applications', 'tensorflow.python.keras.api.keras.applications.densenet', 'tensorflow.python.keras.api.keras.applications.inception_resnet_v2', 'tensorflow.python.keras.api.keras.applications.inception_v3', 'tensorflow.python.keras.api.keras.applications.mobilenet', 'tensorflow.python.keras.api.keras.applications.mobilenet_v2', 'tensorflow.python.keras.api.keras.applications.nasnet', 'tensorflow.python.keras.api.keras.applications.resnet50', 'tensorflow.python.keras.api.keras.applications.vgg16', 'tensorflow.python.keras.api.keras.applications.vgg19', 'tensorflow.python.keras.api.keras.applications.xception', 'tensorflow.python.keras.api.keras.backend', 'tensorflow.python.keras.api.keras.callbacks', 'tensorflow.python.keras.api.keras.constraints', 'tensorflow.python.keras.api.keras.datasets', 'tensorflow.python.keras.api.keras.datasets.boston_housing', 'tensorflow.python.keras.api.keras.datasets.cifar10', 'tensorflow.python.keras.api.keras.datasets.cifar100', 'tensorflow.python.keras.api.keras.datasets.fashion_mnist', 'tensorflow.python.keras.api.keras.datasets.imdb', 'tensorflow.python.keras.api.keras.datasets.mnist', 'tensorflow.python.keras.api.keras.datasets.reuters', 'tensorflow.python.keras.api.keras.estimator', 'tensorflow.python.keras.api.keras.experimental', 'tensorflow.python.keras.api.keras.initializers', 'tensorflow.python.keras.api.keras.layers', 'tensorflow.python.keras.api.keras.losses', 'tensorflow.python.keras.api.keras.metrics', 'tensorflow.python.keras.api.keras.mixed_precision', 'tensorflow.python.keras.api.keras.mixed_precision.experimental', 'tensorflow.python.keras.api.keras.models', 'tensorflow.python.keras.api.keras.optimizers', 'tensorflow.python.keras.api.keras.optimizers.schedules', 'tensorflow.python.keras.api.keras.preprocessing', 'tensorflow.python.keras.api.keras.preprocessing.image', 'tensorflow.python.keras.api.keras.preprocessing.sequence', 'tensorflow.python.keras.api.keras.preprocessing.text', 'tensorflow.python.keras.api.keras.regularizers', 'tensorflow.python.keras.api.keras.utils', 'tensorflow.python.keras.api.keras.wrappers', 'tensorflow.python.keras.api.keras.wrappers.scikit_learn', 'tensorflow.python.keras.api._v1', 'tensorflow.python.keras.api._v1.keras', 'tensorflow.python.keras.api._v1.keras.activations', 'tensorflow.python.keras.api._v1.keras.applications', 'tensorflow.python.keras.api._v1.keras.applications.densenet', 'tensorflow.python.keras.api._v1.keras.applications.inception_resnet_v2', 'tensorflow.python.keras.api._v1.keras.applications.inception_v3', 'tensorflow.python.keras.api._v1.keras.applications.mobilenet', 'tensorflow.python.keras.api._v1.keras.applications.mobilenet_v2', 'tensorflow.python.keras.api._v1.keras.applications.nasnet', 'tensorflow.python.keras.api._v1.keras.applications.resnet50', 'tensorflow.python.keras.api._v1.keras.applications.vgg16', 'tensorflow.python.keras.api._v1.keras.applications.vgg19', 'tensorflow.python.keras.api._v1.keras.applications.xception', 'tensorflow.python.keras.api._v1.keras.backend', 'tensorflow.python.keras.api._v1.keras.callbacks', 'tensorflow.python.keras.api._v1.keras.constraints', 'tensorflow.python.keras.api._v1.keras.datasets', 'tensorflow.python.keras.api._v1.keras.datasets.boston_housing', 'tensorflow.python.keras.api._v1.keras.datasets.cifar10', 'tensorflow.python.keras.api._v1.keras.datasets.cifar100', 'tensorflow.python.keras.api._v1.keras.datasets.fashion_mnist', 'tensorflow.python.keras.api._v1.keras.datasets.imdb', 'tensorflow.python.keras.api._v1.keras.datasets.mnist', 'tensorflow.python.keras.api._v1.keras.datasets.reuters', 'tensorflow.python.keras.api._v1.keras.estimator', 'tensorflow.python.keras.api._v1.keras.experimental', 'tensorflow.python.keras.api._v1.keras.initializers', 'tensorflow.python.keras.api._v1.keras.layers', 'tensorflow.python.keras.api._v1.keras.losses', 'tensorflow.python.keras.api._v1.keras.metrics', 'tensorflow.python.keras.api._v1.keras.mixed_precision', 'tensorflow.python.keras.api._v1.keras.mixed_precision.experimental', 'tensorflow.python.keras.api._v1.keras.models', 'tensorflow.python.keras.api._v1.keras.optimizers', 'tensorflow.python.keras.api._v1.keras.optimizers.schedules', 'tensorflow.python.keras.api._v1.keras.preprocessing', 'tensorflow.python.keras.api._v1.keras.preprocessing.image', 'tensorflow.python.keras.api._v1.keras.preprocessing.sequence', 'tensorflow.python.keras.api._v1.keras.preprocessing.text', 'tensorflow.python.keras.api._v1.keras.regularizers', 'tensorflow.python.keras.api._v1.keras.utils', 'tensorflow.python.keras.api._v1.keras.wrappers', 'tensorflow.python.keras.api._v1.keras.wrappers.scikit_learn', 'tensorflow._api.v1.compat.v2', 'tensorflow._api.v1.compat.v2.audio', 'tensorflow._api.v1.compat.v2.autograph', 'tensorflow._api.v1.compat.v2.autograph.experimental', 'tensorflow._api.v1.compat.v2.bitwise', 'tensorflow._api.v1.compat.v2.compat', 'tensorflow._api.v1.compat.v2.config', 'tensorflow._api.v1.compat.v2.config.experimental', 'tensorflow._api.v1.compat.v2.config.optimizer', 'tensorflow._api.v1.compat.v2.config.threading', 'tensorflow._api.v1.compat.v2.data', 'tensorflow._api.v1.compat.v2.data.experimental', 'tensorflow._api.v1.compat.v2.debugging', 'tensorflow._api.v1.compat.v2.distribute', 'tensorflow._api.v1.compat.v2.distribute.cluster_resolver', 'tensorflow._api.v1.compat.v2.distribute.experimental', 'tensorflow._api.v1.compat.v2.dtypes', 'tensorflow._api.v1.compat.v2.errors', 'tensorflow._api.v1.compat.v2.experimental', 'tensorflow._api.v1.compat.v2.feature_column', 'tensorflow._api.v1.compat.v2.graph_util', 'tensorflow._api.v1.compat.v2.image', 'tensorflow._api.v1.compat.v2.io', 'tensorflow._api.v1.compat.v2.io.gfile', 'tensorflow._api.v1.compat.v2.linalg', 'tensorflow._api.v1.compat.v2.lite', 'tensorflow._api.v1.compat.v2.lookup', 'tensorflow._api.v1.compat.v2.lookup.experimental', 'tensorflow._api.v1.compat.v2.math', 'tensorflow._api.v1.compat.v2.nest', 'tensorflow._api.v1.compat.v2.nn', 'tensorflow._api.v1.compat.v2.quantization', 'tensorflow._api.v1.compat.v2.queue', 'tensorflow._api.v1.compat.v2.ragged', 'tensorflow._api.v1.compat.v2.random', 'tensorflow._api.v1.compat.v2.random.experimental', 'tensorflow._api.v1.compat.v2.raw_ops', 'tensorflow._api.v1.compat.v2.saved_model', 'tensorflow._api.v1.compat.v2.sets', 'tensorflow._api.v1.compat.v2.signal', 'tensorflow._api.v1.compat.v2.sparse', 'tensorflow._api.v1.compat.v2.strings', 'tensorflow._api.v1.compat.v2.summary', 'tensorflow._api.v1.compat.v2.summary.experimental', 'tensorflow._api.v1.compat.v2.sysconfig', 'tensorflow._api.v1.compat.v2.test', 'tensorflow._api.v1.compat.v2.tpu', 'tensorflow._api.v1.compat.v2.tpu.experimental', 'tensorflow._api.v1.compat.v2.train', 'tensorflow._api.v1.compat.v2.train.experimental', 'tensorflow._api.v1.compat.v2.version', 'tensorflow._api.v1.compat.v2.xla', 'tensorflow._api.v1.compat.v2.xla.experimental', 'tensorboard.summary._tf', 'tensorboard.summary._tf.summary', 'tensorflow_estimator.python.estimator.api._v2', 'tensorflow_estimator.python.estimator.api._v2.estimator', 'tensorflow_estimator.python.estimator.api._v2.estimator.experimental', 'tensorflow_estimator.python.estimator.api._v2.estimator.export', 'tensorflow.python.keras.api._v2', 'tensorflow.python.keras.api._v2.keras', 'tensorflow.python.keras.api._v2.keras.activations', 'tensorflow.python.keras.api._v2.keras.applications', 'tensorflow.python.keras.api._v2.keras.applications.densenet', 'tensorflow.python.keras.api._v2.keras.applications.inception_resnet_v2', 'tensorflow.python.keras.api._v2.keras.applications.inception_v3', 'tensorflow.python.keras.api._v2.keras.applications.mobilenet', 'tensorflow.python.keras.api._v2.keras.applications.mobilenet_v2', 'tensorflow.python.keras.api._v2.keras.applications.nasnet', 'tensorflow.python.keras.api._v2.keras.applications.resnet50', 'tensorflow.python.keras.api._v2.keras.applications.vgg16', 'tensorflow.python.keras.api._v2.keras.applications.vgg19', 'tensorflow.python.keras.api._v2.keras.applications.xception', 'tensorflow.python.keras.api._v2.keras.backend', 'tensorflow.python.keras.api._v2.keras.callbacks', 'tensorflow.python.keras.api._v2.keras.constraints', 'tensorflow.python.keras.api._v2.keras.datasets', 'tensorflow.python.keras.api._v2.keras.datasets.boston_housing', 'tensorflow.python.keras.api._v2.keras.datasets.cifar10', 'tensorflow.python.keras.api._v2.keras.datasets.cifar100', 'tensorflow.python.keras.api._v2.keras.datasets.fashion_mnist', 'tensorflow.python.keras.api._v2.keras.datasets.imdb', 'tensorflow.python.keras.api._v2.keras.datasets.mnist', 'tensorflow.python.keras.api._v2.keras.datasets.reuters', 'tensorflow.python.keras.api._v2.keras.estimator', 'tensorflow.python.keras.api._v2.keras.experimental', 'tensorflow.python.keras.api._v2.keras.initializers', 'tensorflow.python.keras.api._v2.keras.layers', 'tensorflow.python.keras.api._v2.keras.losses', 'tensorflow.python.keras.api._v2.keras.metrics', 'tensorflow.python.keras.api._v2.keras.mixed_precision', 'tensorflow.python.keras.api._v2.keras.mixed_precision.experimental', 'tensorflow.python.keras.api._v2.keras.models', 'tensorflow.python.keras.api._v2.keras.optimizers', 'tensorflow.python.keras.api._v2.keras.optimizers.schedules', 'tensorflow.python.keras.api._v2.keras.preprocessing', 'tensorflow.python.keras.api._v2.keras.preprocessing.image', 'tensorflow.python.keras.api._v2.keras.preprocessing.sequence', 'tensorflow.python.keras.api._v2.keras.preprocessing.text', 'tensorflow.python.keras.api._v2.keras.regularizers', 'tensorflow.python.keras.api._v2.keras.utils', 'tensorflow.python.keras.api._v2.keras.wrappers', 'tensorflow.python.keras.api._v2.keras.wrappers.scikit_learn', 'tensorflow._api.v1.config', 'tensorflow._api.v1.config.experimental', 'tensorflow._api.v1.config.optimizer', 'tensorflow._api.v1.config.threading', 'tensorflow._api.v1.data', 'tensorflow._api.v1.data.experimental', 'tensorflow._api.v1.debugging', 'tensorflow._api.v1.distribute', 'tensorflow._api.v1.distribute.cluster_resolver', 'tensorflow._api.v1.distribute.experimental', 'tensorflow._api.v1.distributions', 'tensorflow._api.v1.dtypes', 'tensorflow._api.v1.errors', 'tensorflow._api.v1.experimental', 'tensorflow._api.v1.feature_column', 'tensorflow._api.v1.gfile', 'tensorflow._api.v1.graph_util', 'tensorflow._api.v1.image', 'tensorflow._api.v1.initializers', 'tensorflow._api.v1.io', 'tensorflow._api.v1.io.gfile', 'tensorflow._api.v1.layers', 'tensorflow._api.v1.layers.experimental', 'tensorflow._api.v1.linalg', 'tensorflow._api.v1.lite', 'tensorflow._api.v1.lite.constants', 'tensorflow._api.v1.lite.experimental', 'tensorflow._api.v1.lite.experimental.nn', 'tensorflow._api.v1.logging', 'tensorflow._api.v1.lookup', 'tensorflow._api.v1.lookup.experimental', 'tensorflow._api.v1.losses', 'tensorflow._api.v1.manip', 'tensorflow._api.v1.math', 'tensorflow._api.v1.metrics', 'tensorflow._api.v1.nest', 'tensorflow._api.v1.nn', 'tensorflow._api.v1.nn.rnn_cell', 'tensorflow._api.v1.profiler', 'tensorflow._api.v1.python_io', 'tensorflow._api.v1.quantization', 'tensorflow._api.v1.queue', 'tensorflow._api.v1.ragged', 'tensorflow._api.v1.random', 'tensorflow._api.v1.random.experimental', 'tensorflow._api.v1.raw_ops', 'tensorflow._api.v1.resource_loader', 'tensorflow._api.v1.saved_model', 'tensorflow._api.v1.saved_model.builder', 'tensorflow._api.v1.saved_model.constants', 'tensorflow._api.v1.saved_model.experimental', 'tensorflow._api.v1.saved_model.loader', 'tensorflow._api.v1.saved_model.main_op', 'tensorflow._api.v1.saved_model.signature_constants', 'tensorflow._api.v1.saved_model.signature_def_utils', 'tensorflow._api.v1.saved_model.tag_constants', 'tensorflow._api.v1.saved_model.utils', 'tensorflow._api.v1.sets', 'tensorflow._api.v1.signal', 'tensorflow._api.v1.sparse', 'tensorflow._api.v1.spectral', 'tensorflow._api.v1.strings', 'tensorflow._api.v1.summary', 'tensorflow._api.v1.sysconfig', 'tensorflow._api.v1.test', 'tensorflow._api.v1.tpu', 'tensorflow._api.v1.tpu.experimental', 'tensorflow._api.v1.train', 'tensorflow._api.v1.train.experimental', 'tensorflow._api.v1.train.queue_runner', 'tensorflow._api.v1.user_ops', 'tensorflow._api.v1.version', 'tensorflow._api.v1.xla', 'tensorflow._api.v1.xla.experimental', 'transformers.configuration_albert', 'transformers.configuration_utils', 'transformers.configuration_auto', 'transformers.configuration_bart', 'transformers.configuration_bert', 'transformers.configuration_camembert', 'transformers.configuration_roberta', 'transformers.configuration_ctrl', 'transformers.configuration_distilbert', 'transformers.configuration_electra', 'transformers.configuration_encoder_decoder', 'transformers.configuration_flaubert', 'transformers.configuration_xlm', 'transformers.configuration_gpt2', 'transformers.configuration_longformer', 'transformers.configuration_marian', 'transformers.configuration_openai', 'transformers.configuration_reformer', 'transformers.configuration_t5', 'transformers.configuration_transfo_xl', 'transformers.configuration_xlm_roberta', 'transformers.configuration_xlnet', 'transformers.configuration_mmbt', 'transformers.data', 'transformers.data.metrics', 'scipy.stats', 'scipy.stats.stats', 'scipy.spatial', 'scipy.spatial.kdtree', 'scipy.spatial.ckdtree', 'scipy.spatial.qhull', 'scipy._lib.messagestream', 'scipy.spatial._spherical_voronoi', 'scipy.spatial._voronoi', 'scipy.spatial._plotutils', 'scipy._lib.decorator', 'scipy.spatial._procrustes', 'scipy.spatial._geometric_slerp', 'scipy.spatial.distance', 'scipy.spatial._distance_wrap', 'scipy.spatial._hausdorff', 'scipy.spatial.transform', 'scipy.spatial.transform.rotation', 'scipy.spatial.transform._rotation_groups', 'scipy.constants', 'scipy.constants.codata', 'scipy.constants.constants', 'scipy.spatial.transform._rotation_spline', 'scipy.stats.distributions', 'scipy.stats._distn_infrastructure', 'scipy.stats._distr_params', 'scipy.optimize', 'scipy.optimize.optimize', 'scipy.optimize.linesearch', 'scipy.optimize.minpack2', 'scipy.optimize._numdiff', 'scipy.sparse.linalg', 'scipy.sparse.linalg.isolve', 'scipy.sparse.linalg.isolve.iterative', 'scipy.sparse.linalg.isolve._iterative', 'scipy.sparse.linalg.interface', 'scipy.sparse.linalg.isolve.utils', 'scipy._lib._threadsafety', 'scipy.sparse.linalg.isolve.minres', 'scipy.sparse.linalg.isolve.lgmres', 'scipy.sparse.linalg.isolve._gcrotmk', 'scipy.sparse.linalg.isolve.lsqr', 'scipy.sparse.linalg.isolve.lsmr', 'scipy.sparse.linalg.dsolve', 'scipy.sparse.linalg.dsolve.linsolve', 'scipy.sparse.linalg.dsolve._superlu', 'scipy.sparse.linalg.dsolve._add_newdocs', 'scipy.sparse.linalg.eigen', 'scipy.sparse.linalg.eigen.arpack', 'scipy.sparse.linalg.eigen.arpack.arpack', 'scipy.sparse.linalg.eigen.arpack._arpack', 'scipy.sparse.linalg.eigen.lobpcg', 'scipy.sparse.linalg.eigen.lobpcg.lobpcg', 'scipy.sparse.linalg.matfuncs', 'scipy.sparse.linalg._expm_multiply', 'scipy.sparse.linalg._onenormest', 'scipy.sparse.linalg._norm', 'scipy.optimize._group_columns', 'scipy.optimize._differentiable_functions', 'scipy.optimize._hessian_update_strategy', 'scipy.optimize._minimize', 'scipy.optimize._trustregion_dogleg', 'scipy.optimize._trustregion', 'scipy.optimize._trustregion_ncg', 'scipy.optimize._trustregion_krylov', 'scipy.optimize._trlib', 'scipy.optimize._trlib._trlib', 'scipy.optimize._trustregion_exact', 'scipy.optimize._trustregion_constr', 'scipy.optimize._trustregion_constr.minimize_trustregion_constr', 'scipy.optimize._constraints', 'scipy.optimize._trustregion_constr.equality_constrained_sqp', 'scipy.optimize._trustregion_constr.projections', 'scipy.optimize._trustregion_constr.qp_subproblem', 'scipy.optimize._trustregion_constr.canonical_constraint', 'scipy.optimize._trustregion_constr.tr_interior_point', 'scipy.optimize._trustregion_constr.report', 'scipy.optimize.lbfgsb', 'scipy.optimize._lbfgsb', 'scipy.optimize.tnc', 'scipy.optimize.moduleTNC', 'scipy.optimize.cobyla', 'scipy.optimize._cobyla', 'scipy.optimize.slsqp', 'scipy.optimize._slsqp', 'scipy.optimize._root', 'scipy.optimize.minpack', 'scipy.optimize._minpack', 'scipy.optimize._lsq', 'scipy.optimize._lsq.least_squares', 'scipy.optimize._lsq.trf', 'scipy.optimize._lsq.common', 'scipy.optimize._lsq.dogbox', 'scipy.optimize._lsq.lsq_linear', 'scipy.optimize._lsq.trf_linear', 'scipy.optimize._lsq.givens_elimination', 'scipy.optimize._lsq.bvls', 'scipy.optimize._spectral', 'scipy.optimize.nonlin', 'scipy.optimize._root_scalar', 'scipy.optimize.zeros', 'scipy.optimize._zeros', 'scipy.optimize._nnls', 'scipy.optimize.__nnls', 'scipy.optimize._basinhopping', 'scipy.optimize._linprog', 'scipy.optimize._linprog_ip', 'scipy.optimize._linprog_util', 'scipy.optimize._remove_redundancy', 'scipy.optimize._linprog_simplex', 'scipy.optimize._linprog_rs', 'scipy.optimize._bglu_dense', 'scipy.optimize._lsap', 'scipy.optimize._lsap_module', 'scipy.optimize._differentialevolution', 'scipy.optimize._shgo', 'scipy.optimize._shgo_lib', 'scipy.optimize._shgo_lib.sobol_seq', 'scipy.optimize._shgo_lib.triangulation', 'scipy.optimize._dual_annealing', 'scipy.integrate', 'scipy.integrate._quadrature', 'scipy.integrate.odepack', 'scipy.integrate._odepack', 'scipy.integrate.quadpack', 'scipy.integrate._quadpack', 'scipy.integrate._ode', 'scipy.integrate.vode', 'scipy.integrate._dop', 'scipy.integrate.lsoda', 'scipy.integrate._bvp', 'scipy.integrate._ivp', 'scipy.integrate._ivp.ivp', 'scipy.integrate._ivp.bdf', 'scipy.integrate._ivp.common', 'scipy.integrate._ivp.base', 'scipy.integrate._ivp.radau', 'scipy.integrate._ivp.rk', 'scipy.integrate._ivp.dop853_coefficients', 'scipy.integrate._ivp.lsoda', 'scipy.integrate._quad_vec', 'scipy.misc', 'scipy.misc.doccer', 'scipy.misc.common', 'scipy.stats._constants', 'scipy.stats._continuous_distns', 'scipy.interpolate', 'scipy.interpolate.interpolate', 'scipy.interpolate.fitpack', 'scipy.interpolate._fitpack_impl', 'scipy.interpolate._fitpack', 'scipy.interpolate.dfitpack', 'scipy.interpolate._bsplines', 'scipy.interpolate._bspl', 'scipy.interpolate.polyint', 'scipy.interpolate._ppoly', 'scipy.interpolate.fitpack2', 'scipy.interpolate.interpnd', 'scipy.interpolate.rbf', 'scipy.interpolate._cubic', 'scipy.interpolate.ndgriddata', 'scipy.interpolate._pade', 'scipy.stats._stats', 'scipy.special.cython_special', 'scipy.stats._rvs_sampling', 'scipy.stats._tukeylambda_stats', 'scipy.stats._ksstats', 'scipy.stats._discrete_distns', 'scipy.stats.mstats_basic', 'scipy.stats._stats_mstats_common', 'scipy.stats._hypotests', 'scipy.stats._wilcoxon_data', 'scipy.stats.morestats', 'scipy.stats.statlib', 'scipy.stats.contingency', 'scipy.stats._binned_statistic', 'scipy.stats.kde', 'scipy.stats.mvn', 'scipy.stats.mstats', 'scipy.stats.mstats_extras', 'scipy.stats._multivariate', 'transformers.data.processors', 'transformers.data.processors.glue', 'transformers.tokenization_utils', 'tokenizers', 'tokenizers.tokenizers', 'tokenizers.implementations', 'tokenizers.implementations.base_tokenizer', 'tokenizers.models', 'tokenizers.implementations.byte_level_bpe', 'tokenizers.normalizers', 'tokenizers.implementations.char_level_bpe', 'tokenizers.implementations.sentencepiece_bpe', 'tokenizers.implementations.bert_wordpiece', 'tokenizers.pre_tokenizers', 'tokenizers.processors', 'tokenizers.decoders', 'transformers.data.processors.utils', 'transformers.data.processors.squad', 'transformers.tokenization_bert', 'transformers.data.processors.xnli', 'transformers.hf_argparser', 'transformers.modelcard', 'transformers.modeling_tf_pytorch_utils', 'transformers.pipelines', 'transformers.tokenization_auto', 'transformers.tokenization_albert', 'transformers.tokenization_bart', 'transformers.tokenization_roberta', 'transformers.tokenization_gpt2', 'regex', 'regex.regex', 'regex._regex_core', 'regex._regex', 'transformers.tokenization_xlm_roberta', 'transformers.tokenization_xlnet', 'transformers.tokenization_bert_japanese', 'transformers.tokenization_camembert', 'sentencepiece', 'sentencepiece._sentencepiece', 'sentencepiece._version', 'transformers.tokenization_ctrl', 'transformers.tokenization_distilbert', 'transformers.tokenization_electra', 'transformers.tokenization_flaubert', 'transformers.tokenization_xlm', 'sacremoses', 'sacremoses.corpus', 'sacremoses.tokenize', 'sacremoses.util', 'joblib', 'joblib.memory', 'pydoc', 'joblib.hashing', 'joblib.func_inspect', 'joblib.logger', 'joblib.disk', 'joblib._store_backends', 'joblib.backports', 'joblib.numpy_pickle', 'joblib.compressor', 'joblib.numpy_pickle_utils', 'joblib.numpy_pickle_compat', 'joblib.parallel', 'joblib._multiprocessing_helpers', 'joblib._parallel_backends', 'joblib.my_exceptions', 'joblib._deprecated_my_exceptions', 'joblib.pool', 'joblib._memmapping_reducer', 'joblib.externals', 'joblib.externals.loky', 'joblib.externals.loky._base', 'joblib.externals.loky.backend', 'joblib.externals.loky.backend.context', 'joblib.externals.loky.backend.process', 'joblib.externals.loky.backend.compat', 'joblib.externals.loky.backend.compat_posix', 'multiprocessing.synchronize', 'joblib.externals.loky.backend.reduction', 'joblib.externals.loky.backend._posix_reduction', 'joblib.externals.cloudpickle', 'joblib.externals.cloudpickle.cloudpickle', 'joblib.externals.cloudpickle.compat', 'joblib.externals.cloudpickle.cloudpickle_fast', 'joblib.externals.loky.reusable_executor', 'joblib.externals.loky.process_executor', 'joblib.externals.loky.backend.queues', 'multiprocessing.queues', 'joblib.externals.loky.backend.utils', 'joblib.externals.loky.initializers', 'joblib.externals.loky.cloudpickle_wrapper', 'joblib.externals.loky.backend.resource_tracker', 'joblib.externals.loky.backend.spawn', 'runpy', 'joblib.executor', 'joblib._utils', 'sacremoses.indic', 'sacremoses.truecase', 'sacremoses.normalize', 'transformers.tokenization_longformer', 'transformers.tokenization_marian', 'transformers.tokenization_openai', 'transformers.tokenization_reformer', 'transformers.tokenization_t5', 'transformers.tokenization_transfo_xl', 'transformers.modeling_auto', 'transformers.modeling_albert', 'transformers.modeling_bert', 'transformers.activations', 'transformers.modeling_utils', 'transformers.modeling_bart', 'transformers.modeling_camembert', 'transformers.modeling_roberta', 'transformers.modeling_ctrl', 'transformers.modeling_distilbert', 'transformers.modeling_electra', 'transformers.modeling_encoder_decoder', 'transformers.modeling_flaubert', 'transformers.modeling_xlm', 'transformers.modeling_gpt2', 'transformers.modeling_longformer', 'transformers.modeling_marian', 'transformers.modeling_openai', 'transformers.modeling_reformer', 'transformers.modeling_t5', 'transformers.modeling_transfo_xl', 'transformers.modeling_transfo_xl_utilities', 'transformers.modeling_xlm_roberta', 'transformers.modeling_xlnet', 'transformers.trainer_utils', 'transformers.training_args', 'transformers.training_args_tf', 'transformers.modeling_mmbt', 'transformers.optimization', 'transformers.trainer', 'transformers.data.data_collator', 'transformers.data.datasets', 'transformers.data.datasets.glue', 'transformers.data.datasets.language_modeling', 'callbacks', 'utils', 'sacrebleu', 'sacrebleu.utils', 'portalocker', 'portalocker.__about__', 'portalocker.constants', 'portalocker.exceptions', 'portalocker.portalocker', 'portalocker.utils', 'sacrebleu.dataset', 'sacrebleu.tokenizers', 'sacrebleu.tokenizers.tokenizer_none', 'sacrebleu.tokenizers.tokenizer_13a', 'sacrebleu.tokenizers.tokenizer_re', 'sacrebleu.tokenizers.tokenizer_intl', 'sacrebleu.tokenizers.tokenizer_zh', 'sacrebleu.tokenizers.tokenizer_ja_mecab', 'MeCab', 'MeCab._MeCab', 'sacrebleu.metrics', 'sacrebleu.metrics.bleu', 'sacrebleu.metrics.base', 'sacrebleu.metrics.chrf', 'sacrebleu.compat', 'moverscore_v2', 'pyemd', 'pyemd.__about__', 'pyemd.emd', 'netrc', 'matplotlib', 'matplotlib.cbook', 'matplotlib.cbook.deprecation', 'matplotlib.rcsetup', 'matplotlib.animation', 'matplotlib._animation_data', 'matplotlib.fontconfig_pattern', 'pyparsing', 'matplotlib.colors', 'matplotlib.docstring', 'matplotlib._color_data', 'cycler', 'matplotlib._version', 'matplotlib.ft2font', 'kiwisolver']\n",
            "2024-04-21 22:10:55 __init__ - wrapper: CACHEDIR=/root/.cache/matplotlib\n",
            "2024-04-21 22:10:55 font_manager - <module>: Using fontManager instance from /root/.cache/matplotlib/fontlist-v330.json\n",
            "2024-04-21 22:10:56 pyplot - switch_backend: Loaded backend module://ipykernel.pylab.backend_inline version unknown.\n",
            "2024-04-21 22:10:56 connectionpool - _new_conn: Starting new HTTPS connection (1): s3.amazonaws.com:443\n",
            "2024-04-21 22:10:56 connectionpool - _make_request: https://s3.amazonaws.com:443 \"HEAD /models.huggingface.co/bert/facebook/bart-base/config.json HTTP/1.1\" 200 0\n",
            "2024-04-21 22:10:56 _api - acquire: Attempting to acquire lock 138870379230656 on /root/.cache/torch/transformers/09f4fcaeaf785dd3b97b085d6e3510c7081f586ec8e75981683c6299c0f81d9d.e8d516ad807436d395effad8c2326786872659b7dd1210827ac67c761198a0eb.lock\n",
            "2024-04-21 22:10:56 _api - acquire: Lock 138870379230656 acquired on /root/.cache/torch/transformers/09f4fcaeaf785dd3b97b085d6e3510c7081f586ec8e75981683c6299c0f81d9d.e8d516ad807436d395effad8c2326786872659b7dd1210827ac67c761198a0eb.lock\n",
            "2024-04-21 22:10:56 file_utils - get_from_cache: https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/config.json not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmp8nnrppkm\n",
            "2024-04-21 22:10:56 connectionpool - _new_conn: Starting new HTTPS connection (1): s3.amazonaws.com:443\n",
            "2024-04-21 22:10:56 connectionpool - _make_request: https://s3.amazonaws.com:443 \"GET /models.huggingface.co/bert/facebook/bart-base/config.json HTTP/1.1\" 200 1553\n",
            "Downloading: 100% 1.55k/1.55k [00:00<00:00, 1.13MB/s]\n",
            "2024-04-21 22:10:56 file_utils - get_from_cache: storing https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/config.json in cache at /root/.cache/torch/transformers/09f4fcaeaf785dd3b97b085d6e3510c7081f586ec8e75981683c6299c0f81d9d.e8d516ad807436d395effad8c2326786872659b7dd1210827ac67c761198a0eb\n",
            "2024-04-21 22:10:56 file_utils - get_from_cache: creating metadata file for /root/.cache/torch/transformers/09f4fcaeaf785dd3b97b085d6e3510c7081f586ec8e75981683c6299c0f81d9d.e8d516ad807436d395effad8c2326786872659b7dd1210827ac67c761198a0eb\n",
            "2024-04-21 22:10:56 _api - release: Attempting to release lock 138870379230656 on /root/.cache/torch/transformers/09f4fcaeaf785dd3b97b085d6e3510c7081f586ec8e75981683c6299c0f81d9d.e8d516ad807436d395effad8c2326786872659b7dd1210827ac67c761198a0eb.lock\n",
            "2024-04-21 22:10:56 _api - release: Lock 138870379230656 released on /root/.cache/torch/transformers/09f4fcaeaf785dd3b97b085d6e3510c7081f586ec8e75981683c6299c0f81d9d.e8d516ad807436d395effad8c2326786872659b7dd1210827ac67c761198a0eb.lock\n",
            "2024-04-21 22:10:56 configuration_utils - get_config_dict: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/config.json from cache at /root/.cache/torch/transformers/09f4fcaeaf785dd3b97b085d6e3510c7081f586ec8e75981683c6299c0f81d9d.e8d516ad807436d395effad8c2326786872659b7dd1210827ac67c761198a0eb\n",
            "2024-04-21 22:10:56 configuration_utils - from_dict: Model config BartConfig {\n",
            "  \"activation_dropout\": 0.1,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"add_bias_logits\": false,\n",
            "  \"add_final_layer_norm\": false,\n",
            "  \"architectures\": [\n",
            "    \"BartModel\",\n",
            "    \"BartForConditionalGeneration\",\n",
            "    \"BartForSequenceClassification\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classif_dropout\": 0.0,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_attention_heads\": 12,\n",
            "  \"decoder_ffn_dim\": 3072,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 6,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"dropout\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_heads\": 12,\n",
            "  \"encoder_ffn_dim\": 3072,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 6,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"max_position_embeddings\": 1024,\n",
            "  \"model_type\": \"bart\",\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"normalize_before\": false,\n",
            "  \"normalize_embedding\": true,\n",
            "  \"num_beams\": 4,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"scale_embedding\": false,\n",
            "  \"static_position_embeddings\": false,\n",
            "  \"task_specific_params\": {\n",
            "    \"summarization\": {\n",
            "      \"length_penalty\": 1.0,\n",
            "      \"max_length\": 128,\n",
            "      \"min_length\": 12,\n",
            "      \"num_beams\": 4\n",
            "    },\n",
            "    \"summarization_cnn\": {\n",
            "      \"length_penalty\": 2.0,\n",
            "      \"max_length\": 142,\n",
            "      \"min_length\": 56,\n",
            "      \"num_beams\": 4\n",
            "    },\n",
            "    \"summarization_xsum\": {\n",
            "      \"length_penalty\": 1.0,\n",
            "      \"max_length\": 62,\n",
            "      \"min_length\": 11,\n",
            "      \"num_beams\": 6\n",
            "    }\n",
            "  },\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "2024-04-21 22:10:56 connectionpool - _new_conn: Starting new HTTPS connection (1): s3.amazonaws.com:443\n",
            "2024-04-21 22:10:57 connectionpool - _make_request: https://s3.amazonaws.com:443 \"HEAD /models.huggingface.co/bert/facebook/bart-base/config.json HTTP/1.1\" 200 0\n",
            "2024-04-21 22:10:57 configuration_utils - get_config_dict: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/config.json from cache at /root/.cache/torch/transformers/09f4fcaeaf785dd3b97b085d6e3510c7081f586ec8e75981683c6299c0f81d9d.e8d516ad807436d395effad8c2326786872659b7dd1210827ac67c761198a0eb\n",
            "2024-04-21 22:10:57 configuration_utils - from_dict: Model config BartConfig {\n",
            "  \"activation_dropout\": 0.1,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"add_bias_logits\": false,\n",
            "  \"add_final_layer_norm\": false,\n",
            "  \"architectures\": [\n",
            "    \"BartModel\",\n",
            "    \"BartForConditionalGeneration\",\n",
            "    \"BartForSequenceClassification\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classif_dropout\": 0.0,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_attention_heads\": 12,\n",
            "  \"decoder_ffn_dim\": 3072,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 6,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"dropout\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_heads\": 12,\n",
            "  \"encoder_ffn_dim\": 3072,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 6,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"max_position_embeddings\": 1024,\n",
            "  \"model_type\": \"bart\",\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"normalize_before\": false,\n",
            "  \"normalize_embedding\": true,\n",
            "  \"num_beams\": 4,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"scale_embedding\": false,\n",
            "  \"static_position_embeddings\": false,\n",
            "  \"task_specific_params\": {\n",
            "    \"summarization\": {\n",
            "      \"length_penalty\": 1.0,\n",
            "      \"max_length\": 128,\n",
            "      \"min_length\": 12,\n",
            "      \"num_beams\": 4\n",
            "    },\n",
            "    \"summarization_cnn\": {\n",
            "      \"length_penalty\": 2.0,\n",
            "      \"max_length\": 142,\n",
            "      \"min_length\": 56,\n",
            "      \"num_beams\": 4\n",
            "    },\n",
            "    \"summarization_xsum\": {\n",
            "      \"length_penalty\": 1.0,\n",
            "      \"max_length\": 62,\n",
            "      \"min_length\": 11,\n",
            "      \"num_beams\": 6\n",
            "    }\n",
            "  },\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "2024-04-21 22:10:57 tokenization_utils - _from_pretrained: Model name 'facebook/bart-base' not found in model shortcut name list (bart-large, bart-large-mnli, bart-large-cnn, bart-large-xsum). Assuming 'facebook/bart-base' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "2024-04-21 22:10:57 connectionpool - _new_conn: Starting new HTTPS connection (1): s3.amazonaws.com:443\n",
            "2024-04-21 22:10:57 connectionpool - _make_request: https://s3.amazonaws.com:443 \"HEAD /models.huggingface.co/bert/facebook/bart-base/vocab.json HTTP/1.1\" 200 0\n",
            "2024-04-21 22:10:57 _api - acquire: Attempting to acquire lock 138870379231104 on /root/.cache/torch/transformers/7a7fc1746df9ea150ffd06a23351e5854c2105db3a0be1e0307dfd74fbf4a65c.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b.lock\n",
            "2024-04-21 22:10:57 _api - acquire: Lock 138870379231104 acquired on /root/.cache/torch/transformers/7a7fc1746df9ea150ffd06a23351e5854c2105db3a0be1e0307dfd74fbf4a65c.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b.lock\n",
            "2024-04-21 22:10:57 file_utils - get_from_cache: https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/vocab.json not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmpq8t7n_oi\n",
            "2024-04-21 22:10:57 connectionpool - _new_conn: Starting new HTTPS connection (1): s3.amazonaws.com:443\n",
            "2024-04-21 22:10:57 connectionpool - _make_request: https://s3.amazonaws.com:443 \"GET /models.huggingface.co/bert/facebook/bart-base/vocab.json HTTP/1.1\" 200 898823\n",
            "Downloading: 100% 899k/899k [00:00<00:00, 2.49MB/s]\n",
            "2024-04-21 22:10:58 file_utils - get_from_cache: storing https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/vocab.json in cache at /root/.cache/torch/transformers/7a7fc1746df9ea150ffd06a23351e5854c2105db3a0be1e0307dfd74fbf4a65c.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b\n",
            "2024-04-21 22:10:58 file_utils - get_from_cache: creating metadata file for /root/.cache/torch/transformers/7a7fc1746df9ea150ffd06a23351e5854c2105db3a0be1e0307dfd74fbf4a65c.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b\n",
            "2024-04-21 22:10:58 _api - release: Attempting to release lock 138870379231104 on /root/.cache/torch/transformers/7a7fc1746df9ea150ffd06a23351e5854c2105db3a0be1e0307dfd74fbf4a65c.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b.lock\n",
            "2024-04-21 22:10:58 _api - release: Lock 138870379231104 released on /root/.cache/torch/transformers/7a7fc1746df9ea150ffd06a23351e5854c2105db3a0be1e0307dfd74fbf4a65c.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b.lock\n",
            "2024-04-21 22:10:58 connectionpool - _new_conn: Starting new HTTPS connection (1): s3.amazonaws.com:443\n",
            "2024-04-21 22:10:58 connectionpool - _make_request: https://s3.amazonaws.com:443 \"HEAD /models.huggingface.co/bert/facebook/bart-base/merges.txt HTTP/1.1\" 200 0\n",
            "2024-04-21 22:10:58 _api - acquire: Attempting to acquire lock 138870379231104 on /root/.cache/torch/transformers/3d0d1735635ef097afbf08cf5af21618c94286b8e8b53cfb9bd6cdcfc91d4e14.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda.lock\n",
            "2024-04-21 22:10:58 _api - acquire: Lock 138870379231104 acquired on /root/.cache/torch/transformers/3d0d1735635ef097afbf08cf5af21618c94286b8e8b53cfb9bd6cdcfc91d4e14.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda.lock\n",
            "2024-04-21 22:10:58 file_utils - get_from_cache: https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/merges.txt not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmp4c8lj6kl\n",
            "2024-04-21 22:10:58 connectionpool - _new_conn: Starting new HTTPS connection (1): s3.amazonaws.com:443\n",
            "2024-04-21 22:10:58 connectionpool - _make_request: https://s3.amazonaws.com:443 \"GET /models.huggingface.co/bert/facebook/bart-base/merges.txt HTTP/1.1\" 200 456318\n",
            "Downloading: 100% 456k/456k [00:00<00:00, 1.85MB/s]\n",
            "2024-04-21 22:10:59 file_utils - get_from_cache: storing https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/merges.txt in cache at /root/.cache/torch/transformers/3d0d1735635ef097afbf08cf5af21618c94286b8e8b53cfb9bd6cdcfc91d4e14.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
            "2024-04-21 22:10:59 file_utils - get_from_cache: creating metadata file for /root/.cache/torch/transformers/3d0d1735635ef097afbf08cf5af21618c94286b8e8b53cfb9bd6cdcfc91d4e14.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
            "2024-04-21 22:10:59 _api - release: Attempting to release lock 138870379231104 on /root/.cache/torch/transformers/3d0d1735635ef097afbf08cf5af21618c94286b8e8b53cfb9bd6cdcfc91d4e14.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda.lock\n",
            "2024-04-21 22:10:59 _api - release: Lock 138870379231104 released on /root/.cache/torch/transformers/3d0d1735635ef097afbf08cf5af21618c94286b8e8b53cfb9bd6cdcfc91d4e14.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda.lock\n",
            "2024-04-21 22:10:59 connectionpool - _new_conn: Starting new HTTPS connection (1): s3.amazonaws.com:443\n",
            "2024-04-21 22:10:59 connectionpool - _make_request: https://s3.amazonaws.com:443 \"HEAD /models.huggingface.co/bert/facebook/bart-base/added_tokens.json HTTP/1.1\" 404 0\n",
            "2024-04-21 22:10:59 connectionpool - _new_conn: Starting new HTTPS connection (1): s3.amazonaws.com:443\n",
            "2024-04-21 22:10:59 connectionpool - _make_request: https://s3.amazonaws.com:443 \"HEAD /models.huggingface.co/bert/facebook/bart-base/special_tokens_map.json HTTP/1.1\" 404 0\n",
            "2024-04-21 22:10:59 connectionpool - _new_conn: Starting new HTTPS connection (1): s3.amazonaws.com:443\n",
            "2024-04-21 22:10:59 connectionpool - _make_request: https://s3.amazonaws.com:443 \"HEAD /models.huggingface.co/bert/facebook/bart-base/tokenizer_config.json HTTP/1.1\" 404 0\n",
            "2024-04-21 22:10:59 tokenization_utils - _from_pretrained: loading file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/vocab.json from cache at /root/.cache/torch/transformers/7a7fc1746df9ea150ffd06a23351e5854c2105db3a0be1e0307dfd74fbf4a65c.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b\n",
            "2024-04-21 22:10:59 tokenization_utils - _from_pretrained: loading file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/merges.txt from cache at /root/.cache/torch/transformers/3d0d1735635ef097afbf08cf5af21618c94286b8e8b53cfb9bd6cdcfc91d4e14.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
            "2024-04-21 22:10:59 tokenization_utils - _from_pretrained: loading file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/added_tokens.json from cache at None\n",
            "2024-04-21 22:10:59 tokenization_utils - _from_pretrained: loading file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/special_tokens_map.json from cache at None\n",
            "2024-04-21 22:10:59 tokenization_utils - _from_pretrained: loading file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/tokenizer_config.json from cache at None\n",
            "2024-04-21 22:11:00 tokenization_utils - add_tokens: Adding [R] to the vocabulary\n",
            "2024-04-21 22:11:00 tokenization_utils - add_tokens: Adding [C] to the vocabulary\n",
            "2024-04-21 22:11:00 tokenization_utils - add_tokens: Adding [CAP] to the vocabulary\n",
            "2024-04-21 22:11:00 connectionpool - _new_conn: Starting new HTTPS connection (1): cdn.huggingface.co:443\n",
            "2024-04-21 22:11:00 connectionpool - _make_request: https://cdn.huggingface.co:443 \"HEAD /facebook/bart-base/pytorch_model.bin HTTP/1.1\" 200 0\n",
            "2024-04-21 22:11:00 _api - acquire: Attempting to acquire lock 138870378795640 on /root/.cache/torch/transformers/566c05fb6983817e8ad7a4fa51e3099fe9caa3b31730f964bc5198d71c677523.0a3d95c18c1e434448941bc25accea7b122882be6526fb67c8e8fb6d5ebc711c.lock\n",
            "2024-04-21 22:11:00 _api - acquire: Lock 138870378795640 acquired on /root/.cache/torch/transformers/566c05fb6983817e8ad7a4fa51e3099fe9caa3b31730f964bc5198d71c677523.0a3d95c18c1e434448941bc25accea7b122882be6526fb67c8e8fb6d5ebc711c.lock\n",
            "2024-04-21 22:11:00 file_utils - get_from_cache: https://cdn.huggingface.co/facebook/bart-base/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmpimu_port\n",
            "2024-04-21 22:11:00 connectionpool - _new_conn: Starting new HTTPS connection (1): cdn.huggingface.co:443\n",
            "2024-04-21 22:11:00 connectionpool - _make_request: https://cdn.huggingface.co:443 \"GET /facebook/bart-base/pytorch_model.bin HTTP/1.1\" 200 557941479\n",
            "Downloading: 100% 558M/558M [00:10<00:00, 55.8MB/s]\n",
            "2024-04-21 22:11:10 file_utils - get_from_cache: storing https://cdn.huggingface.co/facebook/bart-base/pytorch_model.bin in cache at /root/.cache/torch/transformers/566c05fb6983817e8ad7a4fa51e3099fe9caa3b31730f964bc5198d71c677523.0a3d95c18c1e434448941bc25accea7b122882be6526fb67c8e8fb6d5ebc711c\n",
            "2024-04-21 22:11:10 file_utils - get_from_cache: creating metadata file for /root/.cache/torch/transformers/566c05fb6983817e8ad7a4fa51e3099fe9caa3b31730f964bc5198d71c677523.0a3d95c18c1e434448941bc25accea7b122882be6526fb67c8e8fb6d5ebc711c\n",
            "2024-04-21 22:11:10 _api - release: Attempting to release lock 138870378795640 on /root/.cache/torch/transformers/566c05fb6983817e8ad7a4fa51e3099fe9caa3b31730f964bc5198d71c677523.0a3d95c18c1e434448941bc25accea7b122882be6526fb67c8e8fb6d5ebc711c.lock\n",
            "2024-04-21 22:11:10 _api - release: Lock 138870378795640 released on /root/.cache/torch/transformers/566c05fb6983817e8ad7a4fa51e3099fe9caa3b31730f964bc5198d71c677523.0a3d95c18c1e434448941bc25accea7b122882be6526fb67c8e8fb6d5ebc711c.lock\n",
            "2024-04-21 22:11:10 modeling_utils - from_pretrained: loading weights file https://cdn.huggingface.co/facebook/bart-base/pytorch_model.bin from cache at /root/.cache/torch/transformers/566c05fb6983817e8ad7a4fa51e3099fe9caa3b31730f964bc5198d71c677523.0a3d95c18c1e434448941bc25accea7b122882be6526fb67c8e8fb6d5ebc711c\n",
            "2024-04-21 22:11:15 lightning_base - __init__: We have added 3 tokens\n",
            "2024-04-21 22:11:16 train_table2text_t5 - __init__: parameters Namespace(adam_epsilon=1e-08, cache_dir='', checkpoint=None, checkpoint_model=None, config_name='', data_dir='data_few_shot', do_predict=True, do_train=True, early_stopping_patience=10, eval_batch_size=4, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, learning_rate=3e-05, max_grad_norm=1.0, max_source_length=384, max_target_length=384, model_name_or_path='facebook/bart-base', n_gpu=1, n_tpu_cores=0, num_train_epochs=30, output_dir='output_train_large', seed=42, test_batch_size=4, tokenizer_name='', train_batch_size=8, warmup_steps=0, weight_decay=0.0)\n",
            "GPU available: True, used: True\n",
            "2024-04-21 22:11:16 distributed - _info: GPU available: True, used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "2024-04-21 22:11:16 distributed - _info: TPU available: False, using: 0 TPU cores\n",
            "CUDA_VISIBLE_DEVICES: [0]\n",
            "2024-04-21 22:11:16 distributed - _info: CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name  | Type                         | Params\n",
            "-------------------------------------------------------\n",
            "0 | model | BartForConditionalGeneration | 139 M \n",
            "2024-04-21 22:11:16 lightning - summarize: \n",
            "  | Name  | Type                         | Params\n",
            "-------------------------------------------------------\n",
            "0 | model | BartForConditionalGeneration | 139 M \n",
            "100% 200/200 [00:00<00:00, 330.67it/s]\n",
            "100% 200/200 [00:00<00:00, 946.56it/s]\n",
            "2024-04-21 22:11:17 train_table2text_t5 - get_dataloader: loading dev dataloader...\n",
            "2024-04-21 22:11:17 train_table2text_t5 - get_dataloader: done\n",
            "Validation sanity check: 100% 4/4 [00:43<00:00, 10.52s/it]2024-04-21 22:12:05 train_table2text_t5 - validation_epoch_end: valid epoch: 0\n",
            "2024-04-21 22:12:05 train_table2text_t5 - validation_epoch_end: 0 bleu_info: 1.357397077334084\n",
            "2024-04-21 22:12:05 train_table2text_t5 - validation_epoch_end: 0 mover score: (0.053, 0.049)\n",
            "100% 200/200 [00:00<00:00, 541.59it/s]\n",
            "100% 200/200 [00:00<00:00, 1664.38it/s]\n",
            "2024-04-21 22:12:06 train_table2text_t5 - get_dataloader: loading dev dataloader...\n",
            "2024-04-21 22:12:06 train_table2text_t5 - get_dataloader: done\n",
            "Training: 0it [00:00, ?it/s]\n",
            "  0% 0/200 [00:00<?, ?it/s]\u001b[A\n",
            " 32% 63/200 [00:00<00:00, 622.78it/s]\u001b[A\n",
            " 63% 126/200 [00:00<00:00, 520.80it/s]\u001b[A\n",
            "100% 200/200 [00:00<00:00, 492.49it/s]\n",
            "\n",
            "  0% 0/200 [00:00<?, ?it/s]\u001b[A\n",
            "100% 200/200 [00:00<00:00, 1797.40it/s]\n",
            "2024-04-21 22:12:06 train_table2text_t5 - get_dataloader: loading train dataloader...\n",
            "2024-04-21 22:12:06 train_table2text_t5 - get_dataloader: done\n",
            "Epoch 1:  33% 25/75 [00:19<00:39,  1.26it/s, loss=4.457, lr=2.9e-5]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 1:  35% 26/75 [00:22<00:42,  1.14it/s, loss=4.457, lr=2.9e-5]\n",
            "Epoch 1:  36% 27/75 [00:25<00:44,  1.07it/s, loss=4.457, lr=2.9e-5]\n",
            "Epoch 1:  37% 28/75 [00:26<00:44,  1.06it/s, loss=4.457, lr=2.9e-5]\n",
            "Epoch 1:  39% 29/75 [00:30<00:47,  1.04s/it, loss=4.457, lr=2.9e-5]\n",
            "Epoch 1:  40% 30/75 [00:32<00:48,  1.08s/it, loss=4.457, lr=2.9e-5]\n",
            "Epoch 1:  41% 31/75 [00:34<00:48,  1.10s/it, loss=4.457, lr=2.9e-5]\n",
            "Epoch 1:  43% 32/75 [00:36<00:48,  1.14s/it, loss=4.457, lr=2.9e-5]\n",
            "Epoch 1:  44% 33/75 [00:38<00:48,  1.16s/it, loss=4.457, lr=2.9e-5]\n",
            "Epoch 1:  45% 34/75 [00:40<00:48,  1.19s/it, loss=4.457, lr=2.9e-5]\n",
            "Epoch 1:  47% 35/75 [00:43<00:49,  1.24s/it, loss=4.457, lr=2.9e-5]\n",
            "Epoch 1:  48% 36/75 [00:44<00:48,  1.24s/it, loss=4.457, lr=2.9e-5]\n",
            "Epoch 1:  49% 37/75 [00:46<00:47,  1.25s/it, loss=4.457, lr=2.9e-5]\n",
            "Epoch 1:  51% 38/75 [00:48<00:47,  1.28s/it, loss=4.457, lr=2.9e-5]\n",
            "Epoch 1:  52% 39/75 [00:50<00:46,  1.29s/it, loss=4.457, lr=2.9e-5]\n",
            "Epoch 1:  53% 40/75 [00:53<00:47,  1.35s/it, loss=4.457, lr=2.9e-5]\n",
            "Epoch 1:  55% 41/75 [00:55<00:46,  1.36s/it, loss=4.457, lr=2.9e-5]\n",
            "Epoch 1:  56% 42/75 [00:56<00:44,  1.36s/it, loss=4.457, lr=2.9e-5]\n",
            "Epoch 1:  57% 43/75 [00:58<00:43,  1.36s/it, loss=4.457, lr=2.9e-5]\n",
            "Epoch 1:  59% 44/75 [01:00<00:42,  1.39s/it, loss=4.457, lr=2.9e-5]\n",
            "Epoch 1:  60% 45/75 [01:03<00:42,  1.41s/it, loss=4.457, lr=2.9e-5]\n",
            "Epoch 1:  61% 46/75 [01:05<00:41,  1.42s/it, loss=4.457, lr=2.9e-5]\n",
            "Epoch 1:  63% 47/75 [01:09<00:41,  1.48s/it, loss=4.457, lr=2.9e-5]\n",
            "Epoch 1:  64% 48/75 [01:11<00:40,  1.50s/it, loss=4.457, lr=2.9e-5]\n",
            "Epoch 1:  65% 49/75 [01:14<00:39,  1.51s/it, loss=4.457, lr=2.9e-5]\n",
            "Epoch 1:  67% 50/75 [01:17<00:38,  1.54s/it, loss=4.457, lr=2.9e-5]\n",
            "Epoch 1:  68% 51/75 [01:20<00:37,  1.57s/it, loss=4.457, lr=2.9e-5]\n",
            "Epoch 1:  69% 52/75 [01:22<00:36,  1.58s/it, loss=4.457, lr=2.9e-5]\n",
            "Epoch 1:  71% 53/75 [01:24<00:34,  1.59s/it, loss=4.457, lr=2.9e-5]\n",
            "Epoch 1:  72% 54/75 [01:25<00:33,  1.59s/it, loss=4.457, lr=2.9e-5]\n",
            "Epoch 1:  73% 55/75 [01:34<00:34,  1.73s/it, loss=4.457, lr=2.9e-5]\n",
            "Epoch 1:  75% 56/75 [01:36<00:32,  1.72s/it, loss=4.457, lr=2.9e-5]\n",
            "Epoch 1:  76% 57/75 [01:39<00:31,  1.74s/it, loss=4.457, lr=2.9e-5]\n",
            "Epoch 1:  77% 58/75 [01:40<00:29,  1.74s/it, loss=4.457, lr=2.9e-5]\n",
            "Epoch 1:  79% 59/75 [01:42<00:27,  1.74s/it, loss=4.457, lr=2.9e-5]\n",
            "Epoch 1:  80% 60/75 [01:45<00:26,  1.76s/it, loss=4.457, lr=2.9e-5]\n",
            "Epoch 1:  81% 61/75 [01:48<00:24,  1.78s/it, loss=4.457, lr=2.9e-5]\n",
            "Epoch 1:  83% 62/75 [01:50<00:23,  1.79s/it, loss=4.457, lr=2.9e-5]\n",
            "Epoch 1:  84% 63/75 [01:53<00:21,  1.80s/it, loss=4.457, lr=2.9e-5]\n",
            "Epoch 1:  85% 64/75 [01:55<00:19,  1.81s/it, loss=4.457, lr=2.9e-5]\n",
            "Epoch 1:  87% 65/75 [01:58<00:18,  1.82s/it, loss=4.457, lr=2.9e-5]\n",
            "Epoch 1:  88% 66/75 [02:00<00:16,  1.83s/it, loss=4.457, lr=2.9e-5]\n",
            "Epoch 1:  89% 67/75 [02:03<00:14,  1.84s/it, loss=4.457, lr=2.9e-5]\n",
            "Epoch 1:  91% 68/75 [02:05<00:12,  1.85s/it, loss=4.457, lr=2.9e-5]\n",
            "Epoch 1:  92% 69/75 [02:09<00:11,  1.88s/it, loss=4.457, lr=2.9e-5]\n",
            "Epoch 1:  93% 70/75 [02:11<00:09,  1.88s/it, loss=4.457, lr=2.9e-5]\n",
            "Epoch 1:  95% 71/75 [02:13<00:07,  1.88s/it, loss=4.457, lr=2.9e-5]\n",
            "Epoch 1:  96% 72/75 [02:15<00:05,  1.89s/it, loss=4.457, lr=2.9e-5]\n",
            "Epoch 1:  97% 73/75 [02:17<00:03,  1.88s/it, loss=4.457, lr=2.9e-5]\n",
            "Epoch 1:  99% 74/75 [02:19<00:01,  1.88s/it, loss=4.457, lr=2.9e-5]\n",
            "Epoch 1: 100% 75/75 [02:22<00:00,  1.90s/it, loss=4.457, lr=2.9e-5]2024-04-21 22:14:30 bleu - corpus_score: That's 100 lines that end in a tokenized period ('.')\n",
            "2024-04-21 22:14:30 bleu - corpus_score: It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2024-04-21 22:14:30 bleu - corpus_score: If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2024-04-21 22:15:03 train_table2text_t5 - validation_epoch_end: valid epoch: 1\n",
            "2024-04-21 22:15:03 train_table2text_t5 - validation_epoch_end: 1 bleu_info: 4.383414559678015\n",
            "2024-04-21 22:15:03 train_table2text_t5 - validation_epoch_end: 1 mover score: (0.097, 0.088)\n",
            "\n",
            "\n",
            "  0% 0/200 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            " 36% 72/200 [00:00<00:00, 718.79it/s]\u001b[A\u001b[A\n",
            "\n",
            "100% 200/200 [00:00<00:00, 509.76it/s]\n",
            "\n",
            "\n",
            "  0% 0/200 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "100% 200/200 [00:00<00:00, 1714.42it/s]\n",
            "2024-04-21 22:15:03 train_table2text_t5 - get_dataloader: loading dev dataloader...\n",
            "2024-04-21 22:15:03 train_table2text_t5 - get_dataloader: done\n",
            "2024-04-21 22:15:03 lightning_base - on_validation_end: ***** Validation results *****\n",
            "2024-04-21 22:15:03 lightning_base - on_validation_end: avg_val_loss = tensor(3.5767, device='cuda:0')\n",
            "2024-04-21 22:15:03 lightning_base - on_validation_end: loss = tensor(3.9215, device='cuda:0')\n",
            "2024-04-21 22:15:03 lightning_base - on_validation_end: step_count = 2\n",
            "2024-04-21 22:15:03 lightning_base - on_validation_end: train_loss = tensor(3.9215, device='cuda:0')\n",
            "2024-04-21 22:15:03 lightning_base - on_validation_end: val_avg_bleu = 4.383414559678015\n",
            "2024-04-21 22:15:03 lightning_base - on_validation_end: val_mover = tensor(0.0970, device='cuda:0')\n",
            "2024-04-21 22:15:03 lightning_base - on_validation_end: val_mover_mean1 = 0.097\n",
            "2024-04-21 22:15:03 lightning_base - on_validation_end: val_mover_median1 = 0.088\n",
            "Epoch 1: 100% 75/75 [02:56<00:00,  2.36s/it, loss=4.457, lr=2.9e-5]\n",
            "                                               \u001b[A\n",
            "  0% 0/200 [00:00<?, ?it/s]\u001b[A\n",
            " 34% 68/200 [00:00<00:00, 645.42it/s]\u001b[A\n",
            " 66% 133/200 [00:00<00:00, 557.44it/s]\u001b[A\n",
            "100% 200/200 [00:00<00:00, 488.30it/s]\n",
            "\n",
            "  0% 0/200 [00:00<?, ?it/s]\u001b[A\n",
            "100% 200/200 [00:00<00:00, 1527.30it/s]\n",
            "2024-04-21 22:15:10 train_table2text_t5 - get_dataloader: loading train dataloader...\n",
            "2024-04-21 22:15:10 train_table2text_t5 - get_dataloader: done\n",
            "Epoch 2:  33% 25/75 [00:22<00:44,  1.12it/s, loss=3.891, lr=2.9e-5]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 2:  35% 26/75 [00:25<00:47,  1.03it/s, loss=3.891, lr=2.9e-5]\n",
            "Epoch 2:  36% 27/75 [00:27<00:48,  1.00s/it, loss=3.891, lr=2.9e-5]\n",
            "Epoch 2:  37% 28/75 [00:28<00:47,  1.00s/it, loss=3.891, lr=2.9e-5]\n",
            "Epoch 2:  39% 29/75 [00:29<00:47,  1.02s/it, loss=3.891, lr=2.9e-5]\n",
            "Epoch 2:  40% 30/75 [00:31<00:46,  1.03s/it, loss=3.891, lr=2.9e-5]\n",
            "Epoch 2:  41% 31/75 [00:32<00:46,  1.05s/it, loss=3.891, lr=2.9e-5]\n",
            "Epoch 2:  43% 32/75 [00:34<00:46,  1.08s/it, loss=3.891, lr=2.9e-5]\n",
            "Epoch 2:  44% 33/75 [00:36<00:46,  1.10s/it, loss=3.891, lr=2.9e-5]\n",
            "Epoch 2:  45% 34/75 [00:37<00:45,  1.10s/it, loss=3.891, lr=2.9e-5]\n",
            "Epoch 2:  47% 35/75 [00:39<00:44,  1.12s/it, loss=3.891, lr=2.9e-5]\n",
            "Epoch 2:  48% 36/75 [00:40<00:43,  1.11s/it, loss=3.891, lr=2.9e-5]\n",
            "Epoch 2:  49% 37/75 [00:41<00:42,  1.11s/it, loss=3.891, lr=2.9e-5]\n",
            "Epoch 2:  51% 38/75 [00:42<00:41,  1.12s/it, loss=3.891, lr=2.9e-5]\n",
            "Epoch 2:  52% 39/75 [00:44<00:40,  1.13s/it, loss=3.891, lr=2.9e-5]\n",
            "Epoch 2:  53% 40/75 [00:46<00:40,  1.16s/it, loss=3.891, lr=2.9e-5]\n",
            "Epoch 2:  55% 41/75 [00:47<00:39,  1.17s/it, loss=3.891, lr=2.9e-5]\n",
            "Epoch 2:  56% 42/75 [00:48<00:38,  1.16s/it, loss=3.891, lr=2.9e-5]\n",
            "Epoch 2:  57% 43/75 [00:49<00:37,  1.16s/it, loss=3.891, lr=2.9e-5]\n",
            "Epoch 2:  59% 44/75 [00:51<00:36,  1.18s/it, loss=3.891, lr=2.9e-5]\n",
            "Epoch 2:  60% 45/75 [00:52<00:35,  1.17s/it, loss=3.891, lr=2.9e-5]\n",
            "Epoch 2:  61% 46/75 [00:54<00:34,  1.18s/it, loss=3.891, lr=2.9e-5]\n",
            "Epoch 2:  63% 47/75 [00:56<00:33,  1.20s/it, loss=3.891, lr=2.9e-5]\n",
            "Epoch 2:  64% 48/75 [00:57<00:32,  1.21s/it, loss=3.891, lr=2.9e-5]\n",
            "Epoch 2:  65% 49/75 [01:00<00:31,  1.23s/it, loss=3.891, lr=2.9e-5]\n",
            "Epoch 2:  67% 50/75 [01:02<00:31,  1.24s/it, loss=3.891, lr=2.9e-5]\n",
            "Epoch 2:  68% 51/75 [01:03<00:30,  1.25s/it, loss=3.891, lr=2.9e-5]\n",
            "Epoch 2:  69% 52/75 [01:05<00:28,  1.26s/it, loss=3.891, lr=2.9e-5]\n",
            "Epoch 2:  71% 53/75 [01:07<00:27,  1.27s/it, loss=3.891, lr=2.9e-5]\n",
            "Epoch 2:  72% 54/75 [01:08<00:26,  1.26s/it, loss=3.891, lr=2.9e-5]\n",
            "Epoch 2:  73% 55/75 [01:09<00:25,  1.27s/it, loss=3.891, lr=2.9e-5]\n",
            "Epoch 2:  75% 56/75 [01:11<00:24,  1.27s/it, loss=3.891, lr=2.9e-5]\n",
            "Epoch 2:  76% 57/75 [01:12<00:22,  1.28s/it, loss=3.891, lr=2.9e-5]\n",
            "Epoch 2:  77% 58/75 [01:14<00:21,  1.28s/it, loss=3.891, lr=2.9e-5]\n",
            "Epoch 2:  79% 59/75 [01:15<00:20,  1.28s/it, loss=3.891, lr=2.9e-5]\n",
            "Epoch 2:  80% 60/75 [01:16<00:19,  1.28s/it, loss=3.891, lr=2.9e-5]\n",
            "Epoch 2:  81% 61/75 [01:18<00:17,  1.28s/it, loss=3.891, lr=2.9e-5]\n",
            "Epoch 2:  83% 62/75 [01:19<00:16,  1.29s/it, loss=3.891, lr=2.9e-5]\n",
            "Epoch 2:  84% 63/75 [01:21<00:15,  1.29s/it, loss=3.891, lr=2.9e-5]\n",
            "Epoch 2:  85% 64/75 [01:22<00:14,  1.29s/it, loss=3.891, lr=2.9e-5]\n",
            "Epoch 2:  87% 65/75 [01:24<00:12,  1.30s/it, loss=3.891, lr=2.9e-5]\n",
            "Epoch 2:  88% 66/75 [01:25<00:11,  1.30s/it, loss=3.891, lr=2.9e-5]\n",
            "Epoch 2:  89% 67/75 [01:26<00:10,  1.30s/it, loss=3.891, lr=2.9e-5]\n",
            "Epoch 2:  91% 68/75 [01:28<00:09,  1.30s/it, loss=3.891, lr=2.9e-5]\n",
            "Epoch 2:  92% 69/75 [01:29<00:07,  1.30s/it, loss=3.891, lr=2.9e-5]\n",
            "Epoch 2:  93% 70/75 [01:31<00:06,  1.30s/it, loss=3.891, lr=2.9e-5]\n",
            "Epoch 2:  95% 71/75 [01:32<00:05,  1.30s/it, loss=3.891, lr=2.9e-5]\n",
            "Epoch 2:  96% 72/75 [01:33<00:03,  1.30s/it, loss=3.891, lr=2.9e-5]\n",
            "Epoch 2:  97% 73/75 [01:34<00:02,  1.30s/it, loss=3.891, lr=2.9e-5]\n",
            "Epoch 2:  99% 74/75 [01:36<00:01,  1.31s/it, loss=3.891, lr=2.9e-5]\n",
            "Epoch 2: 100% 75/75 [01:38<00:00,  1.31s/it, loss=3.891, lr=2.9e-5]2024-04-21 22:16:49 bleu - corpus_score: That's 100 lines that end in a tokenized period ('.')\n",
            "2024-04-21 22:16:49 bleu - corpus_score: It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2024-04-21 22:16:49 bleu - corpus_score: If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2024-04-21 22:17:07 train_table2text_t5 - validation_epoch_end: valid epoch: 2\n",
            "2024-04-21 22:17:07 train_table2text_t5 - validation_epoch_end: 2 bleu_info: 2.560881614952355\n",
            "2024-04-21 22:17:07 train_table2text_t5 - validation_epoch_end: 2 mover score: (0.106, 0.101)\n",
            "\n",
            "\n",
            "  0% 0/200 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            " 34% 68/200 [00:00<00:00, 673.17it/s]\u001b[A\u001b[A\n",
            "\n",
            " 68% 136/200 [00:00<00:00, 581.11it/s]\u001b[A\u001b[A\n",
            "\n",
            "100% 200/200 [00:00<00:00, 511.16it/s]\n",
            "\n",
            "\n",
            "  0% 0/200 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "100% 200/200 [00:00<00:00, 1514.37it/s]\n",
            "2024-04-21 22:17:07 train_table2text_t5 - get_dataloader: loading dev dataloader...\n",
            "2024-04-21 22:17:07 train_table2text_t5 - get_dataloader: done\n",
            "2024-04-21 22:17:07 lightning_base - on_validation_end: ***** Validation results *****\n",
            "2024-04-21 22:17:07 lightning_base - on_validation_end: avg_val_loss = tensor(3.2123, device='cuda:0')\n",
            "2024-04-21 22:17:07 lightning_base - on_validation_end: epoch = 1\n",
            "2024-04-21 22:17:07 lightning_base - on_validation_end: loss = tensor(3.6965, device='cuda:0')\n",
            "2024-04-21 22:17:07 lightning_base - on_validation_end: step_count = 3\n",
            "2024-04-21 22:17:07 lightning_base - on_validation_end: train_loss = tensor(3.6965, device='cuda:0')\n",
            "2024-04-21 22:17:07 lightning_base - on_validation_end: val_avg_bleu = 2.560881614952355\n",
            "2024-04-21 22:17:07 lightning_base - on_validation_end: val_mover = tensor(0.1060, device='cuda:0')\n",
            "2024-04-21 22:17:07 lightning_base - on_validation_end: val_mover_mean1 = 0.106\n",
            "2024-04-21 22:17:07 lightning_base - on_validation_end: val_mover_median1 = 0.101\n",
            "Epoch 2: 100% 75/75 [01:57<00:00,  1.56s/it, loss=3.891, lr=2.9e-5]\n",
            "                                               \u001b[A\n",
            "  0% 0/200 [00:00<?, ?it/s]\u001b[A\n",
            " 32% 63/200 [00:00<00:00, 625.56it/s]\u001b[A\n",
            " 63% 126/200 [00:00<00:00, 558.22it/s]\u001b[A\n",
            "100% 200/200 [00:00<00:00, 513.20it/s]\n",
            "\n",
            "  0% 0/200 [00:00<?, ?it/s]\u001b[A\n",
            "100% 200/200 [00:00<00:00, 1856.00it/s]\n",
            "2024-04-21 22:17:18 train_table2text_t5 - get_dataloader: loading train dataloader...\n",
            "2024-04-21 22:17:18 train_table2text_t5 - get_dataloader: done\n",
            "Epoch 3:  33% 25/75 [00:21<00:43,  1.15it/s, loss=3.574, lr=2.9e-5]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 3:  35% 26/75 [00:23<00:45,  1.09it/s, loss=3.574, lr=2.9e-5]\n",
            "Epoch 3:  36% 27/75 [00:26<00:47,  1.01it/s, loss=3.574, lr=2.9e-5]\n",
            "Epoch 3:  37% 28/75 [00:27<00:46,  1.00it/s, loss=3.574, lr=2.9e-5]\n",
            "Epoch 3:  39% 29/75 [00:30<00:47,  1.04s/it, loss=3.574, lr=2.9e-5]\n",
            "Epoch 3:  40% 30/75 [00:32<00:48,  1.08s/it, loss=3.574, lr=2.9e-5]\n",
            "Epoch 3:  41% 31/75 [00:33<00:48,  1.10s/it, loss=3.574, lr=2.9e-5]\n",
            "Epoch 3:  43% 32/75 [00:35<00:47,  1.11s/it, loss=3.574, lr=2.9e-5]\n",
            "Epoch 3:  44% 33/75 [00:37<00:47,  1.12s/it, loss=3.574, lr=2.9e-5]\n",
            "Epoch 3:  45% 34/75 [00:38<00:46,  1.13s/it, loss=3.574, lr=2.9e-5]\n",
            "Epoch 3:  47% 35/75 [00:40<00:46,  1.16s/it, loss=3.574, lr=2.9e-5]\n",
            "Epoch 3:  48% 36/75 [00:42<00:45,  1.17s/it, loss=3.574, lr=2.9e-5]\n",
            "Epoch 3:  49% 37/75 [00:44<00:46,  1.21s/it, loss=3.574, lr=2.9e-5]\n",
            "Epoch 3:  51% 38/75 [00:47<00:45,  1.24s/it, loss=3.574, lr=2.9e-5]\n",
            "Epoch 3:  52% 39/75 [00:49<00:45,  1.26s/it, loss=3.574, lr=2.9e-5]\n",
            "Epoch 3:  53% 40/75 [00:51<00:45,  1.29s/it, loss=3.574, lr=2.9e-5]\n",
            "Epoch 3:  55% 41/75 [00:53<00:43,  1.29s/it, loss=3.574, lr=2.9e-5]\n",
            "Epoch 3:  56% 42/75 [00:54<00:42,  1.30s/it, loss=3.574, lr=2.9e-5]\n",
            "Epoch 3:  57% 43/75 [00:56<00:41,  1.31s/it, loss=3.574, lr=2.9e-5]\n",
            "Epoch 3:  59% 44/75 [00:58<00:41,  1.34s/it, loss=3.574, lr=2.9e-5]\n",
            "Epoch 3:  60% 45/75 [01:00<00:40,  1.34s/it, loss=3.574, lr=2.9e-5]\n",
            "Epoch 3:  61% 46/75 [01:01<00:38,  1.34s/it, loss=3.574, lr=2.9e-5]\n",
            "Epoch 3:  63% 47/75 [01:03<00:37,  1.35s/it, loss=3.574, lr=2.9e-5]\n",
            "Epoch 3:  64% 48/75 [01:05<00:36,  1.37s/it, loss=3.574, lr=2.9e-5]\n",
            "Epoch 3:  65% 49/75 [01:08<00:36,  1.39s/it, loss=3.574, lr=2.9e-5]\n",
            "Epoch 3:  67% 50/75 [01:10<00:35,  1.41s/it, loss=3.574, lr=2.9e-5]\n",
            "Epoch 3:  68% 51/75 [01:12<00:34,  1.42s/it, loss=3.574, lr=2.9e-5]\n",
            "Epoch 3:  69% 52/75 [01:14<00:32,  1.43s/it, loss=3.574, lr=2.9e-5]\n",
            "Epoch 3:  71% 53/75 [01:16<00:31,  1.45s/it, loss=3.574, lr=2.9e-5]\n",
            "Epoch 3:  72% 54/75 [01:18<00:30,  1.45s/it, loss=3.574, lr=2.9e-5]\n",
            "Epoch 3:  73% 55/75 [01:20<00:29,  1.46s/it, loss=3.574, lr=2.9e-5]\n",
            "Epoch 3:  75% 56/75 [01:21<00:27,  1.46s/it, loss=3.574, lr=2.9e-5]\n",
            "Epoch 3:  76% 57/75 [01:23<00:26,  1.47s/it, loss=3.574, lr=2.9e-5]\n",
            "Epoch 3:  77% 58/75 [01:25<00:24,  1.47s/it, loss=3.574, lr=2.9e-5]\n",
            "Epoch 3:  79% 59/75 [01:26<00:23,  1.47s/it, loss=3.574, lr=2.9e-5]\n",
            "Epoch 3:  80% 60/75 [01:28<00:22,  1.47s/it, loss=3.574, lr=2.9e-5]\n",
            "Epoch 3:  81% 61/75 [01:29<00:20,  1.47s/it, loss=3.574, lr=2.9e-5]\n",
            "Epoch 3:  83% 62/75 [01:32<00:19,  1.49s/it, loss=3.574, lr=2.9e-5]\n",
            "Epoch 3:  84% 63/75 [01:34<00:18,  1.50s/it, loss=3.574, lr=2.9e-5]\n",
            "Epoch 3:  85% 64/75 [01:36<00:16,  1.50s/it, loss=3.574, lr=2.9e-5]\n",
            "Epoch 3:  87% 65/75 [01:37<00:15,  1.50s/it, loss=3.574, lr=2.9e-5]\n",
            "Epoch 3:  88% 66/75 [01:39<00:13,  1.51s/it, loss=3.574, lr=2.9e-5]\n",
            "Epoch 3:  89% 67/75 [01:40<00:12,  1.51s/it, loss=3.574, lr=2.9e-5]\n",
            "Epoch 3:  91% 68/75 [01:42<00:10,  1.51s/it, loss=3.574, lr=2.9e-5]\n",
            "Epoch 3:  92% 69/75 [01:45<00:09,  1.52s/it, loss=3.574, lr=2.9e-5]\n",
            "Epoch 3:  93% 70/75 [01:46<00:07,  1.53s/it, loss=3.574, lr=2.9e-5]\n",
            "Epoch 3:  95% 71/75 [01:48<00:06,  1.53s/it, loss=3.574, lr=2.9e-5]\n",
            "Epoch 3:  96% 72/75 [01:50<00:04,  1.53s/it, loss=3.574, lr=2.9e-5]\n",
            "Epoch 3:  97% 73/75 [01:51<00:03,  1.53s/it, loss=3.574, lr=2.9e-5]\n",
            "Epoch 3:  99% 74/75 [01:53<00:01,  1.53s/it, loss=3.574, lr=2.9e-5]\n",
            "Epoch 3: 100% 75/75 [01:54<00:00,  1.53s/it, loss=3.574, lr=2.9e-5]2024-04-21 22:19:13 bleu - corpus_score: That's 100 lines that end in a tokenized period ('.')\n",
            "2024-04-21 22:19:13 bleu - corpus_score: It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2024-04-21 22:19:13 bleu - corpus_score: If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2024-04-21 22:19:36 train_table2text_t5 - validation_epoch_end: valid epoch: 3\n",
            "2024-04-21 22:19:36 train_table2text_t5 - validation_epoch_end: 3 bleu_info: 4.773337591204451\n",
            "2024-04-21 22:19:36 train_table2text_t5 - validation_epoch_end: 3 mover score: (0.127, 0.119)\n",
            "\n",
            "\n",
            "  0% 0/200 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            " 30% 60/200 [00:00<00:00, 590.39it/s]\u001b[A\u001b[A\n",
            "\n",
            " 60% 120/200 [00:00<00:00, 548.09it/s]\u001b[A\u001b[A\n",
            "\n",
            "100% 200/200 [00:00<00:00, 495.19it/s]\n",
            "\n",
            "\n",
            "  0% 0/200 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "100% 200/200 [00:00<00:00, 1565.08it/s]\n",
            "2024-04-21 22:19:36 train_table2text_t5 - get_dataloader: loading dev dataloader...\n",
            "2024-04-21 22:19:36 train_table2text_t5 - get_dataloader: done\n",
            "2024-04-21 22:19:36 lightning_base - on_validation_end: ***** Validation results *****\n",
            "2024-04-21 22:19:36 lightning_base - on_validation_end: avg_val_loss = tensor(2.9807, device='cuda:0')\n",
            "2024-04-21 22:19:36 lightning_base - on_validation_end: epoch = 2\n",
            "2024-04-21 22:19:36 lightning_base - on_validation_end: loss = tensor(3.6435, device='cuda:0')\n",
            "2024-04-21 22:19:36 lightning_base - on_validation_end: step_count = 4\n",
            "2024-04-21 22:19:36 lightning_base - on_validation_end: train_loss = tensor(3.6435, device='cuda:0')\n",
            "2024-04-21 22:19:36 lightning_base - on_validation_end: val_avg_bleu = 4.773337591204451\n",
            "2024-04-21 22:19:36 lightning_base - on_validation_end: val_mover = tensor(0.1270, device='cuda:0')\n",
            "2024-04-21 22:19:36 lightning_base - on_validation_end: val_mover_mean1 = 0.127\n",
            "2024-04-21 22:19:36 lightning_base - on_validation_end: val_mover_median1 = 0.119\n",
            "Epoch 3: 100% 75/75 [02:18<00:00,  1.84s/it, loss=3.574, lr=2.9e-5]\n",
            "                                               \u001b[A\n",
            "  0% 0/200 [00:00<?, ?it/s]\u001b[A\n",
            " 34% 68/200 [00:00<00:00, 660.98it/s]\u001b[A\n",
            " 68% 135/200 [00:00<00:00, 556.53it/s]\u001b[A\n",
            "100% 200/200 [00:00<00:00, 501.32it/s]\n",
            "\n",
            "  0% 0/200 [00:00<?, ?it/s]\u001b[A\n",
            "100% 200/200 [00:00<00:00, 1477.76it/s]\n",
            "2024-04-21 22:19:48 train_table2text_t5 - get_dataloader: loading train dataloader...\n",
            "2024-04-21 22:19:48 train_table2text_t5 - get_dataloader: done\n",
            "Epoch 4:  33% 25/75 [00:22<00:44,  1.14it/s, loss=3.353, lr=2.9e-5]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 4:  35% 26/75 [00:24<00:45,  1.07it/s, loss=3.353, lr=2.9e-5]\n",
            "Epoch 4:  36% 27/75 [00:25<00:45,  1.05it/s, loss=3.353, lr=2.9e-5]\n",
            "Epoch 4:  37% 28/75 [00:26<00:44,  1.05it/s, loss=3.353, lr=2.9e-5]\n",
            "Epoch 4:  39% 29/75 [00:28<00:44,  1.03it/s, loss=3.353, lr=2.9e-5]\n",
            "Epoch 4:  40% 30/75 [00:30<00:45,  1.01s/it, loss=3.353, lr=2.9e-5]\n",
            "Epoch 4:  41% 31/75 [00:31<00:45,  1.03s/it, loss=3.353, lr=2.9e-5]\n",
            "Epoch 4:  43% 32/75 [00:33<00:44,  1.04s/it, loss=3.353, lr=2.9e-5]\n",
            "Epoch 4:  44% 33/75 [00:34<00:43,  1.04s/it, loss=3.353, lr=2.9e-5]\n",
            "Epoch 4:  45% 34/75 [00:35<00:42,  1.05s/it, loss=3.353, lr=2.9e-5]\n",
            "Epoch 4:  47% 35/75 [00:36<00:42,  1.05s/it, loss=3.353, lr=2.9e-5]\n",
            "Epoch 4:  48% 36/75 [00:38<00:41,  1.06s/it, loss=3.353, lr=2.9e-5]\n",
            "Epoch 4:  49% 37/75 [00:39<00:40,  1.07s/it, loss=3.353, lr=2.9e-5]\n",
            "Epoch 4:  51% 38/75 [00:41<00:40,  1.09s/it, loss=3.353, lr=2.9e-5]\n",
            "Epoch 4:  52% 39/75 [00:42<00:39,  1.10s/it, loss=3.353, lr=2.9e-5]\n",
            "Epoch 4:  53% 40/75 [00:44<00:38,  1.11s/it, loss=3.353, lr=2.9e-5]\n",
            "Epoch 4:  55% 41/75 [00:45<00:37,  1.11s/it, loss=3.353, lr=2.9e-5]\n",
            "Epoch 4:  56% 42/75 [00:46<00:36,  1.11s/it, loss=3.353, lr=2.9e-5]\n",
            "Epoch 4:  57% 43/75 [00:47<00:35,  1.10s/it, loss=3.353, lr=2.9e-5]\n",
            "Epoch 4:  59% 44/75 [00:49<00:34,  1.12s/it, loss=3.353, lr=2.9e-5]\n",
            "Epoch 4:  60% 45/75 [00:50<00:33,  1.11s/it, loss=3.353, lr=2.9e-5]\n",
            "Epoch 4:  61% 46/75 [00:51<00:32,  1.13s/it, loss=3.353, lr=2.9e-5]\n",
            "Epoch 4:  63% 47/75 [00:53<00:31,  1.14s/it, loss=3.353, lr=2.9e-5]\n",
            "Epoch 4:  64% 48/75 [00:55<00:31,  1.15s/it, loss=3.353, lr=2.9e-5]\n",
            "Epoch 4:  65% 49/75 [00:57<00:30,  1.17s/it, loss=3.353, lr=2.9e-5]\n",
            "Epoch 4:  67% 50/75 [00:58<00:29,  1.17s/it, loss=3.353, lr=2.9e-5]\n",
            "Epoch 4:  68% 51/75 [01:00<00:28,  1.18s/it, loss=3.353, lr=2.9e-5]\n",
            "Epoch 4:  69% 52/75 [01:01<00:27,  1.19s/it, loss=3.353, lr=2.9e-5]\n",
            "Epoch 4:  71% 53/75 [01:02<00:26,  1.19s/it, loss=3.353, lr=2.9e-5]\n",
            "Epoch 4:  72% 54/75 [01:04<00:25,  1.19s/it, loss=3.353, lr=2.9e-5]\n",
            "Epoch 4:  73% 55/75 [01:05<00:23,  1.20s/it, loss=3.353, lr=2.9e-5]\n",
            "Epoch 4:  75% 56/75 [01:07<00:22,  1.20s/it, loss=3.353, lr=2.9e-5]\n",
            "Epoch 4:  76% 57/75 [01:09<00:21,  1.21s/it, loss=3.353, lr=2.9e-5]\n",
            "Epoch 4:  77% 58/75 [01:10<00:20,  1.21s/it, loss=3.353, lr=2.9e-5]\n",
            "Epoch 4:  79% 59/75 [01:11<00:19,  1.21s/it, loss=3.353, lr=2.9e-5]\n",
            "Epoch 4:  80% 60/75 [01:12<00:18,  1.21s/it, loss=3.353, lr=2.9e-5]\n",
            "Epoch 4:  81% 61/75 [01:13<00:16,  1.21s/it, loss=3.353, lr=2.9e-5]\n",
            "Epoch 4:  83% 62/75 [01:15<00:15,  1.21s/it, loss=3.353, lr=2.9e-5]\n",
            "Epoch 4:  84% 63/75 [01:16<00:14,  1.22s/it, loss=3.353, lr=2.9e-5]\n",
            "Epoch 4:  85% 64/75 [01:17<00:13,  1.22s/it, loss=3.353, lr=2.9e-5]\n",
            "Epoch 4:  87% 65/75 [01:19<00:12,  1.22s/it, loss=3.353, lr=2.9e-5]\n",
            "Epoch 4:  88% 66/75 [01:21<00:11,  1.23s/it, loss=3.353, lr=2.9e-5]\n",
            "Epoch 4:  89% 67/75 [01:22<00:09,  1.23s/it, loss=3.353, lr=2.9e-5]\n",
            "Epoch 4:  91% 68/75 [01:23<00:08,  1.23s/it, loss=3.353, lr=2.9e-5]\n",
            "Epoch 4:  92% 69/75 [01:24<00:07,  1.23s/it, loss=3.353, lr=2.9e-5]\n",
            "Epoch 4:  93% 70/75 [01:26<00:06,  1.23s/it, loss=3.353, lr=2.9e-5]\n",
            "Epoch 4:  95% 71/75 [01:27<00:04,  1.23s/it, loss=3.353, lr=2.9e-5]\n",
            "Epoch 4:  96% 72/75 [01:28<00:03,  1.24s/it, loss=3.353, lr=2.9e-5]\n",
            "Epoch 4:  97% 73/75 [01:29<00:02,  1.23s/it, loss=3.353, lr=2.9e-5]\n",
            "Epoch 4:  99% 74/75 [01:31<00:01,  1.23s/it, loss=3.353, lr=2.9e-5]\n",
            "Epoch 4: 100% 75/75 [01:32<00:00,  1.24s/it, loss=3.353, lr=2.9e-5]2024-04-21 22:21:22 bleu - corpus_score: That's 100 lines that end in a tokenized period ('.')\n",
            "2024-04-21 22:21:22 bleu - corpus_score: It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2024-04-21 22:21:22 bleu - corpus_score: If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2024-04-21 22:21:38 train_table2text_t5 - validation_epoch_end: valid epoch: 4\n",
            "2024-04-21 22:21:38 train_table2text_t5 - validation_epoch_end: 4 bleu_info: 3.037266839686714\n",
            "2024-04-21 22:21:38 train_table2text_t5 - validation_epoch_end: 4 mover score: (0.13, 0.119)\n",
            "\n",
            "\n",
            "  0% 0/200 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            " 34% 68/200 [00:00<00:00, 678.05it/s]\u001b[A\u001b[A\n",
            "\n",
            " 68% 136/200 [00:00<00:00, 609.17it/s]\u001b[A\u001b[A\n",
            "\n",
            "100% 200/200 [00:00<00:00, 524.95it/s]\n",
            "\n",
            "\n",
            "  0% 0/200 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "100% 200/200 [00:00<00:00, 1773.37it/s]\n",
            "2024-04-21 22:21:39 train_table2text_t5 - get_dataloader: loading dev dataloader...\n",
            "2024-04-21 22:21:39 train_table2text_t5 - get_dataloader: done\n",
            "2024-04-21 22:21:39 lightning_base - on_validation_end: ***** Validation results *****\n",
            "2024-04-21 22:21:39 lightning_base - on_validation_end: avg_val_loss = tensor(2.7646, device='cuda:0')\n",
            "2024-04-21 22:21:39 lightning_base - on_validation_end: epoch = 3\n",
            "2024-04-21 22:21:39 lightning_base - on_validation_end: loss = tensor(3.6871, device='cuda:0')\n",
            "2024-04-21 22:21:39 lightning_base - on_validation_end: step_count = 5\n",
            "2024-04-21 22:21:39 lightning_base - on_validation_end: train_loss = tensor(3.6871, device='cuda:0')\n",
            "2024-04-21 22:21:39 lightning_base - on_validation_end: val_avg_bleu = 3.037266839686714\n",
            "2024-04-21 22:21:39 lightning_base - on_validation_end: val_mover = tensor(0.1300, device='cuda:0')\n",
            "2024-04-21 22:21:39 lightning_base - on_validation_end: val_mover_mean1 = 0.13\n",
            "2024-04-21 22:21:39 lightning_base - on_validation_end: val_mover_median1 = 0.119\n",
            "Epoch 4: 100% 75/75 [01:50<00:00,  1.48s/it, loss=3.353, lr=2.9e-5]\n",
            "                                               \u001b[A\n",
            "  0% 0/200 [00:00<?, ?it/s]\u001b[A\n",
            " 12% 24/200 [00:00<00:00, 229.69it/s]\u001b[A\n",
            " 31% 62/200 [00:00<00:00, 311.21it/s]\u001b[A\n",
            " 48% 96/200 [00:00<00:00, 320.98it/s]\u001b[A\n",
            " 64% 129/200 [00:00<00:00, 270.76it/s]\u001b[A\n",
            " 79% 158/200 [00:00<00:00, 217.60it/s]\u001b[A\n",
            "100% 200/200 [00:00<00:00, 255.31it/s]\n",
            "\n",
            "  0% 0/200 [00:00<?, ?it/s]\u001b[A\n",
            "100% 200/200 [00:00<00:00, 1249.28it/s]\n",
            "2024-04-21 22:21:46 train_table2text_t5 - get_dataloader: loading train dataloader...\n",
            "2024-04-21 22:21:46 train_table2text_t5 - get_dataloader: done\n",
            "Epoch 5:  33% 25/75 [00:21<00:43,  1.14it/s, loss=3.149, lr=2.9e-5]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 5:  35% 26/75 [00:25<00:47,  1.02it/s, loss=3.149, lr=2.9e-5]\n",
            "Epoch 5:  36% 27/75 [00:27<00:49,  1.03s/it, loss=3.149, lr=2.9e-5]\n",
            "Epoch 5:  37% 28/75 [00:29<00:48,  1.04s/it, loss=3.149, lr=2.9e-5]\n",
            "Epoch 5:  39% 29/75 [00:30<00:48,  1.05s/it, loss=3.149, lr=2.9e-5]\n",
            "Epoch 5:  40% 30/75 [00:32<00:48,  1.07s/it, loss=3.149, lr=2.9e-5]\n",
            "Epoch 5:  41% 31/75 [00:33<00:47,  1.08s/it, loss=3.149, lr=2.9e-5]\n",
            "Epoch 5:  43% 32/75 [00:35<00:47,  1.10s/it, loss=3.149, lr=2.9e-5]\n",
            "Epoch 5:  44% 33/75 [00:37<00:47,  1.12s/it, loss=3.149, lr=2.9e-5]\n",
            "Epoch 5:  45% 34/75 [00:38<00:46,  1.14s/it, loss=3.149, lr=2.9e-5]\n",
            "Epoch 5:  47% 35/75 [00:40<00:46,  1.15s/it, loss=3.149, lr=2.9e-5]\n",
            "Epoch 5:  48% 36/75 [00:41<00:45,  1.16s/it, loss=3.149, lr=2.9e-5]\n",
            "Epoch 5:  49% 37/75 [00:43<00:44,  1.18s/it, loss=3.149, lr=2.9e-5]\n",
            "Epoch 5:  51% 38/75 [00:45<00:44,  1.20s/it, loss=3.149, lr=2.9e-5]\n",
            "Epoch 5:  52% 39/75 [00:46<00:43,  1.20s/it, loss=3.149, lr=2.9e-5]\n",
            "Epoch 5:  53% 40/75 [00:48<00:42,  1.22s/it, loss=3.149, lr=2.9e-5]\n",
            "Epoch 5:  55% 41/75 [00:50<00:41,  1.23s/it, loss=3.149, lr=2.9e-5]\n",
            "Epoch 5:  56% 42/75 [00:51<00:40,  1.23s/it, loss=3.149, lr=2.9e-5]\n",
            "Epoch 5:  57% 43/75 [00:52<00:39,  1.23s/it, loss=3.149, lr=2.9e-5]\n",
            "Epoch 5:  59% 44/75 [00:54<00:38,  1.24s/it, loss=3.149, lr=2.9e-5]\n",
            "Epoch 5:  60% 45/75 [00:55<00:37,  1.24s/it, loss=3.149, lr=2.9e-5]\n",
            "Epoch 5:  61% 46/75 [00:57<00:36,  1.25s/it, loss=3.149, lr=2.9e-5]\n",
            "Epoch 5:  63% 47/75 [00:58<00:35,  1.25s/it, loss=3.149, lr=2.9e-5]\n",
            "Epoch 5:  64% 48/75 [01:00<00:34,  1.27s/it, loss=3.149, lr=2.9e-5]\n",
            "Epoch 5:  65% 49/75 [01:03<00:33,  1.30s/it, loss=3.149, lr=2.9e-5]\n",
            "Epoch 5:  67% 50/75 [01:05<00:32,  1.31s/it, loss=3.149, lr=2.9e-5]\n",
            "Epoch 5:  68% 51/75 [01:07<00:31,  1.32s/it, loss=3.149, lr=2.9e-5]\n",
            "Epoch 5:  69% 52/75 [01:09<00:30,  1.33s/it, loss=3.149, lr=2.9e-5]\n",
            "Epoch 5:  71% 53/75 [01:11<00:29,  1.35s/it, loss=3.149, lr=2.9e-5]\n",
            "Epoch 5:  72% 54/75 [01:12<00:28,  1.35s/it, loss=3.149, lr=2.9e-5]\n",
            "Epoch 5:  73% 55/75 [01:15<00:27,  1.37s/it, loss=3.149, lr=2.9e-5]\n",
            "Epoch 5:  75% 56/75 [01:16<00:26,  1.37s/it, loss=3.149, lr=2.9e-5]\n",
            "Epoch 5:  76% 57/75 [01:18<00:24,  1.38s/it, loss=3.149, lr=2.9e-5]\n",
            "Epoch 5:  77% 58/75 [01:19<00:23,  1.38s/it, loss=3.149, lr=2.9e-5]\n",
            "Epoch 5:  79% 59/75 [01:21<00:22,  1.38s/it, loss=3.149, lr=2.9e-5]\n",
            "Epoch 5:  80% 60/75 [01:23<00:20,  1.39s/it, loss=3.149, lr=2.9e-5]\n",
            "Epoch 5:  81% 61/75 [01:24<00:19,  1.39s/it, loss=3.149, lr=2.9e-5]\n",
            "Epoch 5:  83% 62/75 [01:27<00:18,  1.41s/it, loss=3.149, lr=2.9e-5]\n",
            "Epoch 5:  84% 63/75 [01:29<00:17,  1.42s/it, loss=3.149, lr=2.9e-5]\n",
            "Epoch 5:  85% 64/75 [01:30<00:15,  1.42s/it, loss=3.149, lr=2.9e-5]\n",
            "Epoch 5:  87% 65/75 [01:32<00:14,  1.42s/it, loss=3.149, lr=2.9e-5]\n",
            "Epoch 5:  88% 66/75 [01:34<00:12,  1.43s/it, loss=3.149, lr=2.9e-5]\n",
            "Epoch 5:  89% 67/75 [01:35<00:11,  1.43s/it, loss=3.149, lr=2.9e-5]\n",
            "Epoch 5:  91% 68/75 [01:36<00:09,  1.43s/it, loss=3.149, lr=2.9e-5]\n",
            "Epoch 5:  92% 69/75 [01:39<00:08,  1.44s/it, loss=3.149, lr=2.9e-5]\n",
            "Epoch 5:  93% 70/75 [01:40<00:07,  1.44s/it, loss=3.149, lr=2.9e-5]\n",
            "Epoch 5:  95% 71/75 [01:42<00:05,  1.44s/it, loss=3.149, lr=2.9e-5]\n",
            "Epoch 5:  96% 72/75 [01:44<00:04,  1.45s/it, loss=3.149, lr=2.9e-5]\n",
            "Epoch 5:  97% 73/75 [01:45<00:02,  1.44s/it, loss=3.149, lr=2.9e-5]\n",
            "Epoch 5:  99% 74/75 [01:46<00:01,  1.44s/it, loss=3.149, lr=2.9e-5]\n",
            "Epoch 5: 100% 75/75 [01:48<00:00,  1.44s/it, loss=3.149, lr=2.9e-5]2024-04-21 22:23:35 bleu - corpus_score: That's 100 lines that end in a tokenized period ('.')\n",
            "2024-04-21 22:23:35 bleu - corpus_score: It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2024-04-21 22:23:35 bleu - corpus_score: If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2024-04-21 22:23:55 train_table2text_t5 - validation_epoch_end: valid epoch: 5\n",
            "2024-04-21 22:23:55 train_table2text_t5 - validation_epoch_end: 5 bleu_info: 4.588724160527631\n",
            "2024-04-21 22:23:55 train_table2text_t5 - validation_epoch_end: 5 mover score: (0.144, 0.137)\n",
            "\n",
            "\n",
            "  0% 0/200 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            " 36% 73/200 [00:00<00:00, 726.50it/s]\u001b[A\u001b[A\n",
            "\n",
            "100% 200/200 [00:00<00:00, 537.83it/s]\n",
            "\n",
            "\n",
            "  0% 0/200 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "100% 200/200 [00:00<00:00, 1726.30it/s]\n",
            "2024-04-21 22:23:56 train_table2text_t5 - get_dataloader: loading dev dataloader...\n",
            "2024-04-21 22:23:56 train_table2text_t5 - get_dataloader: done\n",
            "2024-04-21 22:23:56 lightning_base - on_validation_end: ***** Validation results *****\n",
            "2024-04-21 22:23:56 lightning_base - on_validation_end: avg_val_loss = tensor(2.6286, device='cuda:0')\n",
            "2024-04-21 22:23:56 lightning_base - on_validation_end: epoch = 4\n",
            "2024-04-21 22:23:56 lightning_base - on_validation_end: loss = tensor(3.3078, device='cuda:0')\n",
            "2024-04-21 22:23:56 lightning_base - on_validation_end: step_count = 6\n",
            "2024-04-21 22:23:56 lightning_base - on_validation_end: train_loss = tensor(3.3078, device='cuda:0')\n",
            "2024-04-21 22:23:56 lightning_base - on_validation_end: val_avg_bleu = 4.588724160527631\n",
            "2024-04-21 22:23:56 lightning_base - on_validation_end: val_mover = tensor(0.1440, device='cuda:0')\n",
            "2024-04-21 22:23:56 lightning_base - on_validation_end: val_mover_mean1 = 0.144\n",
            "2024-04-21 22:23:56 lightning_base - on_validation_end: val_mover_median1 = 0.137\n",
            "Epoch 5: 100% 75/75 [02:09<00:00,  1.73s/it, loss=3.149, lr=2.9e-5]\n",
            "                                               \u001b[A\n",
            "  0% 0/200 [00:00<?, ?it/s]\u001b[A\n",
            " 16% 33/200 [00:00<00:00, 329.44it/s]\u001b[A\n",
            " 47% 94/200 [00:00<00:00, 491.63it/s]\u001b[A\n",
            " 72% 144/200 [00:00<00:00, 439.90it/s]\u001b[A\n",
            "100% 200/200 [00:00<00:00, 452.94it/s]\n",
            "\n",
            "  0% 0/200 [00:00<?, ?it/s]\u001b[A\n",
            "100% 200/200 [00:00<00:00, 1879.62it/s]\n",
            "2024-04-21 22:24:08 train_table2text_t5 - get_dataloader: loading train dataloader...\n",
            "2024-04-21 22:24:08 train_table2text_t5 - get_dataloader: done\n",
            "Epoch 6:  33% 25/75 [00:22<00:44,  1.13it/s, loss=3.025, lr=2.9e-5]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 6:  35% 26/75 [00:24<00:46,  1.06it/s, loss=3.025, lr=2.9e-5]\n",
            "Epoch 6:  36% 27/75 [00:26<00:47,  1.01it/s, loss=3.025, lr=2.9e-5]\n",
            "Epoch 6:  37% 28/75 [00:27<00:46,  1.00it/s, loss=3.025, lr=2.9e-5]\n",
            "Epoch 6:  39% 29/75 [00:29<00:46,  1.02s/it, loss=3.025, lr=2.9e-5]\n",
            "Epoch 6:  40% 30/75 [00:31<00:47,  1.06s/it, loss=3.025, lr=2.9e-5]\n",
            "Epoch 6:  41% 31/75 [00:33<00:47,  1.08s/it, loss=3.025, lr=2.9e-5]\n",
            "Epoch 6:  43% 32/75 [00:35<00:47,  1.09s/it, loss=3.025, lr=2.9e-5]\n",
            "Epoch 6:  44% 33/75 [00:36<00:45,  1.09s/it, loss=3.025, lr=2.9e-5]\n",
            "Epoch 6:  45% 34/75 [00:37<00:44,  1.10s/it, loss=3.025, lr=2.9e-5]\n",
            "Epoch 6:  47% 35/75 [00:38<00:44,  1.10s/it, loss=3.025, lr=2.9e-5]\n",
            "Epoch 6:  48% 36/75 [00:40<00:43,  1.11s/it, loss=3.025, lr=2.9e-5]\n",
            "Epoch 6:  49% 37/75 [00:41<00:42,  1.13s/it, loss=3.025, lr=2.9e-5]\n",
            "Epoch 6:  51% 38/75 [00:44<00:43,  1.17s/it, loss=3.025, lr=2.9e-5]\n",
            "Epoch 6:  52% 39/75 [00:46<00:42,  1.18s/it, loss=3.025, lr=2.9e-5]\n",
            "Epoch 6:  53% 40/75 [00:47<00:41,  1.20s/it, loss=3.025, lr=2.9e-5]\n",
            "Epoch 6:  55% 41/75 [00:49<00:40,  1.20s/it, loss=3.025, lr=2.9e-5]\n",
            "Epoch 6:  56% 42/75 [00:50<00:39,  1.20s/it, loss=3.025, lr=2.9e-5]\n",
            "Epoch 6:  57% 43/75 [00:51<00:38,  1.20s/it, loss=3.025, lr=2.9e-5]\n",
            "Epoch 6:  59% 44/75 [00:53<00:37,  1.22s/it, loss=3.025, lr=2.9e-5]\n",
            "Epoch 6:  60% 45/75 [00:54<00:36,  1.22s/it, loss=3.025, lr=2.9e-5]\n",
            "Epoch 6:  61% 46/75 [00:57<00:36,  1.24s/it, loss=3.025, lr=2.9e-5]\n",
            "Epoch 6:  63% 47/75 [00:59<00:35,  1.26s/it, loss=3.025, lr=2.9e-5]\n",
            "Epoch 6:  64% 48/75 [01:01<00:34,  1.27s/it, loss=3.025, lr=2.9e-5]\n",
            "Epoch 6:  65% 49/75 [01:03<00:33,  1.30s/it, loss=3.025, lr=2.9e-5]\n",
            "Epoch 6:  67% 50/75 [01:05<00:32,  1.31s/it, loss=3.025, lr=2.9e-5]\n",
            "Epoch 6:  68% 51/75 [01:07<00:31,  1.32s/it, loss=3.025, lr=2.9e-5]\n",
            "Epoch 6:  69% 52/75 [01:09<00:30,  1.33s/it, loss=3.025, lr=2.9e-5]\n",
            "Epoch 6:  71% 53/75 [01:10<00:29,  1.34s/it, loss=3.025, lr=2.9e-5]\n",
            "Epoch 6:  72% 54/75 [01:12<00:28,  1.35s/it, loss=3.025, lr=2.9e-5]\n",
            "Epoch 6:  73% 55/75 [01:14<00:27,  1.36s/it, loss=3.025, lr=2.9e-5]\n",
            "Epoch 6:  75% 56/75 [01:16<00:26,  1.37s/it, loss=3.025, lr=2.9e-5]\n",
            "Epoch 6:  76% 57/75 [01:17<00:24,  1.37s/it, loss=3.025, lr=2.9e-5]\n",
            "Epoch 6:  77% 58/75 [01:19<00:23,  1.37s/it, loss=3.025, lr=2.9e-5]\n",
            "Epoch 6:  79% 59/75 [01:20<00:21,  1.37s/it, loss=3.025, lr=2.9e-5]\n",
            "Epoch 6:  80% 60/75 [01:22<00:20,  1.38s/it, loss=3.025, lr=2.9e-5]\n",
            "Epoch 6:  81% 61/75 [01:24<00:19,  1.38s/it, loss=3.025, lr=2.9e-5]\n",
            "Epoch 6:  83% 62/75 [01:26<00:18,  1.39s/it, loss=3.025, lr=2.9e-5]\n",
            "Epoch 6:  84% 63/75 [01:28<00:16,  1.40s/it, loss=3.025, lr=2.9e-5]\n",
            "Epoch 6:  85% 64/75 [01:29<00:15,  1.40s/it, loss=3.025, lr=2.9e-5]\n",
            "Epoch 6:  87% 65/75 [01:31<00:14,  1.40s/it, loss=3.025, lr=2.9e-5]\n",
            "Epoch 6:  88% 66/75 [01:32<00:12,  1.41s/it, loss=3.025, lr=2.9e-5]\n",
            "Epoch 6:  89% 67/75 [01:34<00:11,  1.41s/it, loss=3.025, lr=2.9e-5]\n",
            "Epoch 6:  91% 68/75 [01:36<00:09,  1.42s/it, loss=3.025, lr=2.9e-5]\n",
            "Epoch 6:  92% 69/75 [01:38<00:08,  1.42s/it, loss=3.025, lr=2.9e-5]\n",
            "Epoch 6:  93% 70/75 [01:39<00:07,  1.42s/it, loss=3.025, lr=2.9e-5]\n",
            "Epoch 6:  95% 71/75 [01:41<00:05,  1.43s/it, loss=3.025, lr=2.9e-5]\n",
            "Epoch 6:  96% 72/75 [01:43<00:04,  1.43s/it, loss=3.025, lr=2.9e-5]\n",
            "Epoch 6:  97% 73/75 [01:44<00:02,  1.43s/it, loss=3.025, lr=2.9e-5]\n",
            "Epoch 6:  99% 74/75 [01:46<00:01,  1.43s/it, loss=3.025, lr=2.9e-5]\n",
            "Epoch 6: 100% 75/75 [01:48<00:00,  1.45s/it, loss=3.025, lr=2.9e-5]2024-04-21 22:25:57 bleu - corpus_score: That's 100 lines that end in a tokenized period ('.')\n",
            "2024-04-21 22:25:57 bleu - corpus_score: It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2024-04-21 22:25:57 bleu - corpus_score: If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2024-04-21 22:26:17 train_table2text_t5 - validation_epoch_end: valid epoch: 6\n",
            "2024-04-21 22:26:17 train_table2text_t5 - validation_epoch_end: 6 bleu_info: 5.556400208446179\n",
            "2024-04-21 22:26:17 train_table2text_t5 - validation_epoch_end: 6 mover score: (0.154, 0.153)\n",
            "\n",
            "\n",
            "  0% 0/200 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            " 34% 68/200 [00:00<00:00, 677.96it/s]\u001b[A\u001b[A\n",
            "\n",
            " 68% 136/200 [00:00<00:00, 570.70it/s]\u001b[A\u001b[A\n",
            "\n",
            "100% 200/200 [00:00<00:00, 512.74it/s]\n",
            "\n",
            "\n",
            "  0% 0/200 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "100% 200/200 [00:00<00:00, 1475.40it/s]\n",
            "2024-04-21 22:26:18 train_table2text_t5 - get_dataloader: loading dev dataloader...\n",
            "2024-04-21 22:26:18 train_table2text_t5 - get_dataloader: done\n",
            "2024-04-21 22:26:18 lightning_base - on_validation_end: ***** Validation results *****\n",
            "2024-04-21 22:26:18 lightning_base - on_validation_end: avg_val_loss = tensor(2.4556, device='cuda:0')\n",
            "2024-04-21 22:26:18 lightning_base - on_validation_end: epoch = 5\n",
            "2024-04-21 22:26:18 lightning_base - on_validation_end: loss = tensor(3.7016, device='cuda:0')\n",
            "2024-04-21 22:26:18 lightning_base - on_validation_end: step_count = 7\n",
            "2024-04-21 22:26:18 lightning_base - on_validation_end: train_loss = tensor(3.7016, device='cuda:0')\n",
            "2024-04-21 22:26:18 lightning_base - on_validation_end: val_avg_bleu = 5.556400208446179\n",
            "2024-04-21 22:26:18 lightning_base - on_validation_end: val_mover = tensor(0.1540, device='cuda:0')\n",
            "2024-04-21 22:26:18 lightning_base - on_validation_end: val_mover_mean1 = 0.154\n",
            "2024-04-21 22:26:18 lightning_base - on_validation_end: val_mover_median1 = 0.153\n",
            "Epoch 6: 100% 75/75 [02:09<00:00,  1.73s/it, loss=3.025, lr=2.9e-5]\n",
            "                                               \u001b[A\n",
            "  0% 0/200 [00:00<?, ?it/s]\u001b[A\n",
            " 38% 75/200 [00:00<00:00, 745.39it/s]\u001b[A\n",
            "100% 200/200 [00:00<00:00, 424.28it/s]\n",
            "\n",
            "  0% 0/200 [00:00<?, ?it/s]\u001b[A\n",
            "100% 200/200 [00:00<00:00, 1020.65it/s]\n",
            "2024-04-21 22:26:30 train_table2text_t5 - get_dataloader: loading train dataloader...\n",
            "2024-04-21 22:26:30 train_table2text_t5 - get_dataloader: done\n",
            "Epoch 7:  33% 25/75 [00:22<00:44,  1.12it/s, loss=2.901, lr=2.9e-5]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 7:  35% 26/75 [00:24<00:46,  1.05it/s, loss=2.901, lr=2.9e-5]\n",
            "Epoch 7:  36% 27/75 [00:28<00:50,  1.06s/it, loss=2.901, lr=2.9e-5]\n",
            "Epoch 7:  37% 28/75 [00:29<00:50,  1.07s/it, loss=2.901, lr=2.9e-5]\n",
            "Epoch 7:  39% 29/75 [00:31<00:50,  1.09s/it, loss=2.901, lr=2.9e-5]\n",
            "Epoch 7:  40% 30/75 [00:33<00:49,  1.11s/it, loss=2.901, lr=2.9e-5]\n",
            "Epoch 7:  41% 31/75 [00:34<00:49,  1.12s/it, loss=2.901, lr=2.9e-5]\n",
            "Epoch 7:  43% 32/75 [00:36<00:48,  1.14s/it, loss=2.901, lr=2.9e-5]\n",
            "Epoch 7:  44% 33/75 [00:37<00:48,  1.15s/it, loss=2.901, lr=2.9e-5]\n",
            "Epoch 7:  45% 34/75 [00:40<00:48,  1.18s/it, loss=2.901, lr=2.9e-5]\n",
            "Epoch 7:  47% 35/75 [00:41<00:47,  1.20s/it, loss=2.901, lr=2.9e-5]\n",
            "Epoch 7:  48% 36/75 [00:44<00:47,  1.23s/it, loss=2.901, lr=2.9e-5]\n",
            "Epoch 7:  49% 37/75 [00:46<00:47,  1.25s/it, loss=2.901, lr=2.9e-5]\n",
            "Epoch 7:  51% 38/75 [00:48<00:47,  1.29s/it, loss=2.901, lr=2.9e-5]\n",
            "Epoch 7:  52% 39/75 [00:51<00:47,  1.32s/it, loss=2.901, lr=2.9e-5]\n",
            "Epoch 7:  53% 40/75 [00:53<00:46,  1.34s/it, loss=2.901, lr=2.9e-5]\n",
            "Epoch 7:  55% 41/75 [00:55<00:45,  1.35s/it, loss=2.901, lr=2.9e-5]\n",
            "Epoch 7:  56% 42/75 [00:56<00:44,  1.35s/it, loss=2.901, lr=2.9e-5]\n",
            "Epoch 7:  57% 43/75 [00:58<00:43,  1.36s/it, loss=2.901, lr=2.9e-5]\n",
            "Epoch 7:  59% 44/75 [01:00<00:42,  1.38s/it, loss=2.901, lr=2.9e-5]\n",
            "Epoch 7:  60% 45/75 [01:01<00:41,  1.37s/it, loss=2.901, lr=2.9e-5]\n",
            "Epoch 7:  61% 46/75 [01:04<00:40,  1.39s/it, loss=2.901, lr=2.9e-5]\n",
            "Epoch 7:  63% 47/75 [01:06<00:39,  1.41s/it, loss=2.901, lr=2.9e-5]\n",
            "Epoch 7:  64% 48/75 [01:08<00:38,  1.42s/it, loss=2.901, lr=2.9e-5]\n",
            "Epoch 7:  65% 49/75 [01:10<00:37,  1.44s/it, loss=2.901, lr=2.9e-5]\n",
            "Epoch 7:  67% 50/75 [01:13<00:36,  1.47s/it, loss=2.901, lr=2.9e-5]\n",
            "Epoch 7:  68% 51/75 [01:15<00:35,  1.48s/it, loss=2.901, lr=2.9e-5]\n",
            "Epoch 7:  69% 52/75 [01:17<00:34,  1.50s/it, loss=2.901, lr=2.9e-5]\n",
            "Epoch 7:  71% 53/75 [01:19<00:33,  1.50s/it, loss=2.901, lr=2.9e-5]\n",
            "Epoch 7:  72% 54/75 [01:21<00:31,  1.51s/it, loss=2.901, lr=2.9e-5]\n",
            "Epoch 7:  73% 55/75 [01:23<00:30,  1.51s/it, loss=2.901, lr=2.9e-5]\n",
            "Epoch 7:  75% 56/75 [01:25<00:28,  1.53s/it, loss=2.901, lr=2.9e-5]\n",
            "Epoch 7:  76% 57/75 [01:27<00:27,  1.54s/it, loss=2.901, lr=2.9e-5]\n",
            "Epoch 7:  77% 58/75 [01:30<00:26,  1.56s/it, loss=2.901, lr=2.9e-5]\n",
            "Epoch 7:  79% 59/75 [01:31<00:24,  1.56s/it, loss=2.901, lr=2.9e-5]\n",
            "Epoch 7:  80% 60/75 [01:33<00:23,  1.57s/it, loss=2.901, lr=2.9e-5]\n",
            "Epoch 7:  81% 61/75 [01:36<00:22,  1.58s/it, loss=2.901, lr=2.9e-5]\n",
            "Epoch 7:  83% 62/75 [01:38<00:20,  1.58s/it, loss=2.901, lr=2.9e-5]\n",
            "Epoch 7:  84% 63/75 [01:40<00:19,  1.60s/it, loss=2.901, lr=2.9e-5]\n",
            "Epoch 7:  85% 64/75 [01:43<00:17,  1.62s/it, loss=2.901, lr=2.9e-5]\n",
            "Epoch 7:  87% 65/75 [01:45<00:16,  1.62s/it, loss=2.901, lr=2.9e-5]\n",
            "Epoch 7:  88% 66/75 [01:47<00:14,  1.63s/it, loss=2.901, lr=2.9e-5]\n",
            "Epoch 7:  89% 67/75 [01:49<00:13,  1.63s/it, loss=2.901, lr=2.9e-5]\n",
            "Epoch 7:  91% 68/75 [01:51<00:11,  1.63s/it, loss=2.901, lr=2.9e-5]\n",
            "Epoch 7:  92% 69/75 [01:53<00:09,  1.65s/it, loss=2.901, lr=2.9e-5]\n",
            "Epoch 7:  93% 70/75 [01:56<00:08,  1.66s/it, loss=2.901, lr=2.9e-5]\n",
            "Epoch 7:  95% 71/75 [01:58<00:06,  1.67s/it, loss=2.901, lr=2.9e-5]\n",
            "Epoch 7:  96% 72/75 [02:00<00:05,  1.67s/it, loss=2.901, lr=2.9e-5]\n",
            "Epoch 7:  97% 73/75 [02:02<00:03,  1.68s/it, loss=2.901, lr=2.9e-5]\n",
            "Epoch 7:  99% 74/75 [02:03<00:01,  1.67s/it, loss=2.901, lr=2.9e-5]\n",
            "Epoch 7: 100% 75/75 [02:05<00:00,  1.67s/it, loss=2.901, lr=2.9e-5]2024-04-21 22:28:37 bleu - corpus_score: That's 100 lines that end in a tokenized period ('.')\n",
            "2024-04-21 22:28:37 bleu - corpus_score: It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2024-04-21 22:28:37 bleu - corpus_score: If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2024-04-21 22:29:03 train_table2text_t5 - validation_epoch_end: valid epoch: 7\n",
            "2024-04-21 22:29:03 train_table2text_t5 - validation_epoch_end: 7 bleu_info: 6.671669369255128\n",
            "2024-04-21 22:29:03 train_table2text_t5 - validation_epoch_end: 7 mover score: (0.163, 0.159)\n",
            "\n",
            "\n",
            "  0% 0/200 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            " 32% 63/200 [00:00<00:00, 627.86it/s]\u001b[A\u001b[A\n",
            "\n",
            " 63% 126/200 [00:00<00:00, 533.51it/s]\u001b[A\u001b[A\n",
            "\n",
            "100% 200/200 [00:00<00:00, 495.83it/s]\n",
            "\n",
            "\n",
            "  0% 0/200 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "100% 200/200 [00:00<00:00, 1824.21it/s]\n",
            "2024-04-21 22:29:03 train_table2text_t5 - get_dataloader: loading dev dataloader...\n",
            "2024-04-21 22:29:03 train_table2text_t5 - get_dataloader: done\n",
            "2024-04-21 22:29:03 lightning_base - on_validation_end: ***** Validation results *****\n",
            "2024-04-21 22:29:03 lightning_base - on_validation_end: avg_val_loss = tensor(2.3234, device='cuda:0')\n",
            "2024-04-21 22:29:03 lightning_base - on_validation_end: epoch = 6\n",
            "2024-04-21 22:29:03 lightning_base - on_validation_end: loss = tensor(2.8178, device='cuda:0')\n",
            "2024-04-21 22:29:03 lightning_base - on_validation_end: step_count = 8\n",
            "2024-04-21 22:29:03 lightning_base - on_validation_end: train_loss = tensor(2.8178, device='cuda:0')\n",
            "2024-04-21 22:29:03 lightning_base - on_validation_end: val_avg_bleu = 6.671669369255128\n",
            "2024-04-21 22:29:03 lightning_base - on_validation_end: val_mover = tensor(0.1630, device='cuda:0')\n",
            "2024-04-21 22:29:03 lightning_base - on_validation_end: val_mover_mean1 = 0.163\n",
            "2024-04-21 22:29:03 lightning_base - on_validation_end: val_mover_median1 = 0.159\n",
            "Epoch 7: 100% 75/75 [02:33<00:00,  2.05s/it, loss=2.901, lr=2.9e-5]\n",
            "                                               \u001b[A\n",
            "  0% 0/200 [00:00<?, ?it/s]\u001b[A\n",
            " 34% 68/200 [00:00<00:00, 664.15it/s]\u001b[A\n",
            " 68% 135/200 [00:00<00:00, 587.10it/s]\u001b[A\n",
            "100% 200/200 [00:00<00:00, 527.15it/s]\n",
            "\n",
            "  0% 0/200 [00:00<?, ?it/s]\u001b[A\n",
            "100% 200/200 [00:00<00:00, 1803.92it/s]\n",
            "2024-04-21 22:29:23 train_table2text_t5 - get_dataloader: loading train dataloader...\n",
            "2024-04-21 22:29:23 train_table2text_t5 - get_dataloader: done\n",
            "Epoch 8:  33% 25/75 [00:21<00:42,  1.19it/s, loss=2.713, lr=2.9e-5]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 8:  35% 26/75 [00:23<00:45,  1.09it/s, loss=2.713, lr=2.9e-5]\n",
            "Epoch 8:  36% 27/75 [00:27<00:48,  1.00s/it, loss=2.713, lr=2.9e-5]\n",
            "Epoch 8:  37% 28/75 [00:29<00:48,  1.04s/it, loss=2.713, lr=2.9e-5]\n",
            "Epoch 8:  39% 29/75 [00:31<00:49,  1.08s/it, loss=2.713, lr=2.9e-5]\n",
            "Epoch 8:  40% 30/75 [00:33<00:49,  1.11s/it, loss=2.713, lr=2.9e-5]\n",
            "Epoch 8:  41% 31/75 [00:34<00:49,  1.12s/it, loss=2.713, lr=2.9e-5]\n",
            "Epoch 8:  43% 32/75 [00:36<00:48,  1.14s/it, loss=2.713, lr=2.9e-5]\n",
            "Epoch 8:  44% 33/75 [00:38<00:48,  1.16s/it, loss=2.713, lr=2.9e-5]\n",
            "Epoch 8:  45% 34/75 [00:40<00:48,  1.19s/it, loss=2.713, lr=2.9e-5]\n",
            "Epoch 8:  47% 35/75 [00:42<00:48,  1.21s/it, loss=2.713, lr=2.9e-5]\n",
            "Epoch 8:  48% 36/75 [00:44<00:48,  1.23s/it, loss=2.713, lr=2.9e-5]\n",
            "Epoch 8:  49% 37/75 [00:46<00:47,  1.26s/it, loss=2.713, lr=2.9e-5]\n",
            "Epoch 8:  51% 38/75 [00:48<00:47,  1.29s/it, loss=2.713, lr=2.9e-5]\n",
            "Epoch 8:  52% 39/75 [00:50<00:46,  1.29s/it, loss=2.713, lr=2.9e-5]\n",
            "Epoch 8:  53% 40/75 [00:52<00:46,  1.32s/it, loss=2.713, lr=2.9e-5]\n",
            "Epoch 8:  55% 41/75 [00:54<00:45,  1.33s/it, loss=2.713, lr=2.9e-5]\n",
            "Epoch 8:  56% 42/75 [00:55<00:43,  1.33s/it, loss=2.713, lr=2.9e-5]\n",
            "Epoch 8:  57% 43/75 [00:57<00:42,  1.33s/it, loss=2.713, lr=2.9e-5]\n",
            "Epoch 8:  59% 44/75 [00:59<00:42,  1.36s/it, loss=2.713, lr=2.9e-5]\n",
            "Epoch 8:  60% 45/75 [01:00<00:40,  1.35s/it, loss=2.713, lr=2.9e-5]\n",
            "Epoch 8:  61% 46/75 [01:02<00:39,  1.36s/it, loss=2.713, lr=2.9e-5]\n",
            "Epoch 8:  63% 47/75 [01:04<00:38,  1.38s/it, loss=2.713, lr=2.9e-5]\n",
            "Epoch 8:  64% 48/75 [01:06<00:37,  1.39s/it, loss=2.713, lr=2.9e-5]\n",
            "Epoch 8:  65% 49/75 [01:09<00:36,  1.41s/it, loss=2.713, lr=2.9e-5]\n",
            "Epoch 8:  67% 50/75 [01:11<00:35,  1.42s/it, loss=2.713, lr=2.9e-5]\n",
            "Epoch 8:  68% 51/75 [01:12<00:34,  1.42s/it, loss=2.713, lr=2.9e-5]\n",
            "Epoch 8:  69% 52/75 [01:14<00:32,  1.43s/it, loss=2.713, lr=2.9e-5]\n",
            "Epoch 8:  71% 53/75 [01:15<00:31,  1.43s/it, loss=2.713, lr=2.9e-5]\n",
            "Epoch 8:  72% 54/75 [01:18<00:30,  1.45s/it, loss=2.713, lr=2.9e-5]\n",
            "Epoch 8:  73% 55/75 [01:20<00:29,  1.46s/it, loss=2.713, lr=2.9e-5]\n",
            "Epoch 8:  75% 56/75 [01:22<00:27,  1.47s/it, loss=2.713, lr=2.9e-5]\n",
            "Epoch 8:  76% 57/75 [01:23<00:26,  1.47s/it, loss=2.713, lr=2.9e-5]\n",
            "Epoch 8:  77% 58/75 [01:25<00:25,  1.48s/it, loss=2.713, lr=2.9e-5]\n",
            "Epoch 8:  79% 59/75 [01:27<00:23,  1.49s/it, loss=2.713, lr=2.9e-5]\n",
            "Epoch 8:  80% 60/75 [01:29<00:22,  1.50s/it, loss=2.713, lr=2.9e-5]\n",
            "Epoch 8:  81% 61/75 [01:31<00:21,  1.50s/it, loss=2.713, lr=2.9e-5]\n",
            "Epoch 8:  83% 62/75 [01:33<00:19,  1.51s/it, loss=2.713, lr=2.9e-5]\n",
            "Epoch 8:  84% 63/75 [01:35<00:18,  1.51s/it, loss=2.713, lr=2.9e-5]\n",
            "Epoch 8:  85% 64/75 [01:37<00:16,  1.53s/it, loss=2.713, lr=2.9e-5]\n",
            "Epoch 8:  87% 65/75 [01:39<00:15,  1.53s/it, loss=2.713, lr=2.9e-5]\n",
            "Epoch 8:  88% 66/75 [01:41<00:13,  1.54s/it, loss=2.713, lr=2.9e-5]\n",
            "Epoch 8:  89% 67/75 [01:44<00:12,  1.55s/it, loss=2.713, lr=2.9e-5]\n",
            "Epoch 8:  91% 68/75 [01:45<00:10,  1.55s/it, loss=2.713, lr=2.9e-5]\n",
            "Epoch 8:  92% 69/75 [01:47<00:09,  1.56s/it, loss=2.713, lr=2.9e-5]\n",
            "Epoch 8:  93% 70/75 [01:49<00:07,  1.56s/it, loss=2.713, lr=2.9e-5]\n",
            "Epoch 8:  95% 71/75 [01:51<00:06,  1.57s/it, loss=2.713, lr=2.9e-5]\n",
            "Epoch 8:  96% 72/75 [01:53<00:04,  1.58s/it, loss=2.713, lr=2.9e-5]\n",
            "Epoch 8:  97% 73/75 [01:55<00:03,  1.59s/it, loss=2.713, lr=2.9e-5]\n",
            "Epoch 8:  99% 74/75 [01:57<00:01,  1.58s/it, loss=2.713, lr=2.9e-5]\n",
            "Epoch 8: 100% 75/75 [01:58<00:00,  1.58s/it, loss=2.713, lr=2.9e-5]2024-04-21 22:31:22 bleu - corpus_score: That's 100 lines that end in a tokenized period ('.')\n",
            "2024-04-21 22:31:22 bleu - corpus_score: It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2024-04-21 22:31:22 bleu - corpus_score: If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2024-04-21 22:31:45 train_table2text_t5 - validation_epoch_end: valid epoch: 8\n",
            "2024-04-21 22:31:45 train_table2text_t5 - validation_epoch_end: 8 bleu_info: 6.963022196595325\n",
            "2024-04-21 22:31:45 train_table2text_t5 - validation_epoch_end: 8 mover score: (0.163, 0.163)\n",
            "\n",
            "\n",
            "  0% 0/200 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            " 38% 76/200 [00:00<00:00, 754.56it/s]\u001b[A\u001b[A\n",
            "\n",
            "100% 200/200 [00:00<00:00, 509.19it/s]\n",
            "\n",
            "\n",
            "  0% 0/200 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "100% 200/200 [00:00<00:00, 1859.94it/s]\n",
            "2024-04-21 22:31:46 train_table2text_t5 - get_dataloader: loading dev dataloader...\n",
            "2024-04-21 22:31:46 train_table2text_t5 - get_dataloader: done\n",
            "2024-04-21 22:31:46 lightning_base - on_validation_end: ***** Validation results *****\n",
            "2024-04-21 22:31:46 lightning_base - on_validation_end: avg_val_loss = tensor(2.1415, device='cuda:0')\n",
            "2024-04-21 22:31:46 lightning_base - on_validation_end: epoch = 7\n",
            "2024-04-21 22:31:46 lightning_base - on_validation_end: loss = tensor(2.5432, device='cuda:0')\n",
            "2024-04-21 22:31:46 lightning_base - on_validation_end: step_count = 9\n",
            "2024-04-21 22:31:46 lightning_base - on_validation_end: train_loss = tensor(2.5432, device='cuda:0')\n",
            "2024-04-21 22:31:46 lightning_base - on_validation_end: val_avg_bleu = 6.963022196595325\n",
            "2024-04-21 22:31:46 lightning_base - on_validation_end: val_mover = tensor(0.1630, device='cuda:0')\n",
            "2024-04-21 22:31:46 lightning_base - on_validation_end: val_mover_mean1 = 0.163\n",
            "2024-04-21 22:31:46 lightning_base - on_validation_end: val_mover_median1 = 0.163\n",
            "Epoch 8: 100% 75/75 [02:22<00:00,  1.90s/it, loss=2.713, lr=2.9e-5]\n",
            "                                               \u001b[A\n",
            "  0% 0/200 [00:00<?, ?it/s]\u001b[A\n",
            " 34% 69/200 [00:00<00:00, 687.01it/s]\u001b[A\n",
            " 69% 138/200 [00:00<00:00, 559.80it/s]\u001b[A\n",
            "100% 200/200 [00:00<00:00, 514.40it/s]\n",
            "\n",
            "  0% 0/200 [00:00<?, ?it/s]\u001b[A\n",
            "100% 200/200 [00:00<00:00, 1943.75it/s]\n",
            "2024-04-21 22:32:03 train_table2text_t5 - get_dataloader: loading train dataloader...\n",
            "2024-04-21 22:32:03 train_table2text_t5 - get_dataloader: done\n",
            "Epoch 9:  33% 25/75 [00:21<00:43,  1.16it/s, loss=2.547, lr=2.9e-5]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 9:  35% 26/75 [00:23<00:44,  1.09it/s, loss=2.547, lr=2.9e-5]\n",
            "Epoch 9:  36% 27/75 [00:25<00:46,  1.04it/s, loss=2.547, lr=2.9e-5]\n",
            "Epoch 9:  37% 28/75 [00:27<00:45,  1.03it/s, loss=2.547, lr=2.9e-5]\n",
            "Epoch 9:  39% 29/75 [00:29<00:47,  1.03s/it, loss=2.547, lr=2.9e-5]\n",
            "Epoch 9:  40% 30/75 [00:31<00:46,  1.04s/it, loss=2.547, lr=2.9e-5]\n",
            "Epoch 9:  41% 31/75 [00:32<00:46,  1.06s/it, loss=2.547, lr=2.9e-5]\n",
            "Epoch 9:  43% 32/75 [00:34<00:46,  1.07s/it, loss=2.547, lr=2.9e-5]\n",
            "Epoch 9:  44% 33/75 [00:35<00:45,  1.08s/it, loss=2.547, lr=2.9e-5]\n",
            "Epoch 9:  45% 34/75 [00:37<00:44,  1.10s/it, loss=2.547, lr=2.9e-5]\n",
            "Epoch 9:  47% 35/75 [00:38<00:44,  1.11s/it, loss=2.547, lr=2.9e-5]\n",
            "Epoch 9:  48% 36/75 [00:40<00:43,  1.12s/it, loss=2.547, lr=2.9e-5]\n",
            "Epoch 9:  49% 37/75 [00:43<00:44,  1.17s/it, loss=2.547, lr=2.9e-5]\n",
            "Epoch 9:  51% 38/75 [00:45<00:44,  1.20s/it, loss=2.547, lr=2.9e-5]\n",
            "Epoch 9:  52% 39/75 [00:47<00:43,  1.21s/it, loss=2.547, lr=2.9e-5]\n",
            "Epoch 9:  53% 40/75 [00:48<00:42,  1.22s/it, loss=2.547, lr=2.9e-5]\n",
            "Epoch 9:  55% 41/75 [00:51<00:42,  1.25s/it, loss=2.547, lr=2.9e-5]\n",
            "Epoch 9:  56% 42/75 [00:52<00:41,  1.26s/it, loss=2.547, lr=2.9e-5]\n",
            "Epoch 9:  57% 43/75 [00:54<00:40,  1.28s/it, loss=2.547, lr=2.9e-5]\n",
            "Epoch 9:  59% 44/75 [00:56<00:39,  1.28s/it, loss=2.547, lr=2.9e-5]\n",
            "Epoch 9:  60% 45/75 [00:57<00:38,  1.28s/it, loss=2.547, lr=2.9e-5]\n",
            "Epoch 9:  61% 46/75 [00:59<00:37,  1.29s/it, loss=2.547, lr=2.9e-5]\n",
            "Epoch 9:  63% 47/75 [01:01<00:36,  1.31s/it, loss=2.547, lr=2.9e-5]\n",
            "Epoch 9:  64% 48/75 [01:02<00:35,  1.31s/it, loss=2.547, lr=2.9e-5]\n",
            "Epoch 9:  65% 49/75 [01:05<00:34,  1.34s/it, loss=2.547, lr=2.9e-5]\n",
            "Epoch 9:  67% 50/75 [01:08<00:34,  1.37s/it, loss=2.547, lr=2.9e-5]\n",
            "Epoch 9:  68% 51/75 [01:10<00:33,  1.38s/it, loss=2.547, lr=2.9e-5]\n",
            "Epoch 9:  69% 52/75 [01:11<00:31,  1.38s/it, loss=2.547, lr=2.9e-5]\n",
            "Epoch 9:  71% 53/75 [01:14<00:30,  1.40s/it, loss=2.547, lr=2.9e-5]\n",
            "Epoch 9:  72% 54/75 [01:16<00:29,  1.41s/it, loss=2.547, lr=2.9e-5]\n",
            "Epoch 9:  73% 55/75 [01:18<00:28,  1.43s/it, loss=2.547, lr=2.9e-5]\n",
            "Epoch 9:  75% 56/75 [01:21<00:27,  1.45s/it, loss=2.547, lr=2.9e-5]\n",
            "Epoch 9:  76% 57/75 [01:22<00:26,  1.45s/it, loss=2.547, lr=2.9e-5]\n",
            "Epoch 9:  77% 58/75 [01:24<00:24,  1.46s/it, loss=2.547, lr=2.9e-5]\n",
            "Epoch 9:  79% 59/75 [01:26<00:23,  1.46s/it, loss=2.547, lr=2.9e-5]\n",
            "Epoch 9:  80% 60/75 [01:28<00:22,  1.47s/it, loss=2.547, lr=2.9e-5]\n",
            "Epoch 9:  81% 61/75 [01:29<00:20,  1.47s/it, loss=2.547, lr=2.9e-5]\n",
            "Epoch 9:  83% 62/75 [01:32<00:19,  1.49s/it, loss=2.547, lr=2.9e-5]\n",
            "Epoch 9:  84% 63/75 [01:34<00:18,  1.51s/it, loss=2.547, lr=2.9e-5]\n",
            "Epoch 9:  85% 64/75 [01:37<00:16,  1.52s/it, loss=2.547, lr=2.9e-5]\n",
            "Epoch 9:  87% 65/75 [01:38<00:15,  1.52s/it, loss=2.547, lr=2.9e-5]\n",
            "Epoch 9:  88% 66/75 [01:40<00:13,  1.53s/it, loss=2.547, lr=2.9e-5]\n",
            "Epoch 9:  89% 67/75 [01:42<00:12,  1.54s/it, loss=2.547, lr=2.9e-5]\n",
            "Epoch 9:  91% 68/75 [01:44<00:10,  1.54s/it, loss=2.547, lr=2.9e-5]\n",
            "Epoch 9:  92% 69/75 [01:47<00:09,  1.55s/it, loss=2.547, lr=2.9e-5]\n",
            "Epoch 9:  93% 70/75 [01:48<00:07,  1.55s/it, loss=2.547, lr=2.9e-5]\n",
            "Epoch 9:  95% 71/75 [01:50<00:06,  1.56s/it, loss=2.547, lr=2.9e-5]\n",
            "Epoch 9:  96% 72/75 [01:52<00:04,  1.57s/it, loss=2.547, lr=2.9e-5]\n",
            "Epoch 9:  97% 73/75 [01:54<00:03,  1.57s/it, loss=2.547, lr=2.9e-5]\n",
            "Epoch 9:  99% 74/75 [01:56<00:01,  1.57s/it, loss=2.547, lr=2.9e-5]\n",
            "Epoch 9: 100% 75/75 [01:57<00:00,  1.57s/it, loss=2.547, lr=2.9e-5]2024-04-21 22:34:01 bleu - corpus_score: That's 100 lines that end in a tokenized period ('.')\n",
            "2024-04-21 22:34:01 bleu - corpus_score: It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2024-04-21 22:34:01 bleu - corpus_score: If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2024-04-21 22:34:25 train_table2text_t5 - validation_epoch_end: valid epoch: 9\n",
            "2024-04-21 22:34:25 train_table2text_t5 - validation_epoch_end: 9 bleu_info: 6.6244755416601535\n",
            "2024-04-21 22:34:25 train_table2text_t5 - validation_epoch_end: 9 mover score: (0.165, 0.153)\n",
            "\n",
            "\n",
            "  0% 0/200 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            " 36% 71/200 [00:00<00:00, 706.98it/s]\u001b[A\u001b[A\n",
            "\n",
            " 71% 142/200 [00:00<00:00, 546.01it/s]\u001b[A\u001b[A\n",
            "\n",
            "100% 200/200 [00:00<00:00, 506.56it/s]\n",
            "\n",
            "\n",
            "  0% 0/200 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "100% 200/200 [00:00<00:00, 1742.16it/s]\n",
            "2024-04-21 22:34:25 train_table2text_t5 - get_dataloader: loading dev dataloader...\n",
            "2024-04-21 22:34:25 train_table2text_t5 - get_dataloader: done\n",
            "2024-04-21 22:34:25 lightning_base - on_validation_end: ***** Validation results *****\n",
            "2024-04-21 22:34:25 lightning_base - on_validation_end: avg_val_loss = tensor(1.9996, device='cuda:0')\n",
            "2024-04-21 22:34:25 lightning_base - on_validation_end: epoch = 8\n",
            "2024-04-21 22:34:25 lightning_base - on_validation_end: loss = tensor(2.3523, device='cuda:0')\n",
            "2024-04-21 22:34:25 lightning_base - on_validation_end: step_count = 10\n",
            "2024-04-21 22:34:25 lightning_base - on_validation_end: train_loss = tensor(2.3523, device='cuda:0')\n",
            "2024-04-21 22:34:25 lightning_base - on_validation_end: val_avg_bleu = 6.6244755416601535\n",
            "2024-04-21 22:34:25 lightning_base - on_validation_end: val_mover = tensor(0.1650, device='cuda:0')\n",
            "2024-04-21 22:34:25 lightning_base - on_validation_end: val_mover_mean1 = 0.165\n",
            "2024-04-21 22:34:25 lightning_base - on_validation_end: val_mover_median1 = 0.153\n",
            "Epoch 9: 100% 75/75 [02:22<00:00,  1.90s/it, loss=2.547, lr=2.9e-5]\n",
            "                                               \u001b[A\n",
            "  0% 0/200 [00:00<?, ?it/s]\u001b[A\n",
            " 33% 66/200 [00:00<00:00, 659.45it/s]\u001b[A\n",
            " 66% 132/200 [00:00<00:00, 529.65it/s]\u001b[A\n",
            "100% 200/200 [00:00<00:00, 507.86it/s]\n",
            "\n",
            "  0% 0/200 [00:00<?, ?it/s]\u001b[A\n",
            "100% 200/200 [00:00<00:00, 1905.21it/s]\n",
            "2024-04-21 22:34:37 train_table2text_t5 - get_dataloader: loading train dataloader...\n",
            "2024-04-21 22:34:37 train_table2text_t5 - get_dataloader: done\n",
            "Epoch 10:  33% 25/75 [00:21<00:43,  1.16it/s, loss=2.451, lr=2.9e-5]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 10:  35% 26/75 [00:24<00:46,  1.07it/s, loss=2.451, lr=2.9e-5]\n",
            "Epoch 10:  36% 27/75 [00:25<00:46,  1.04it/s, loss=2.451, lr=2.9e-5]\n",
            "Epoch 10:  37% 28/75 [00:27<00:45,  1.03it/s, loss=2.451, lr=2.9e-5]\n",
            "Epoch 10:  39% 29/75 [00:29<00:46,  1.00s/it, loss=2.451, lr=2.9e-5]\n",
            "Epoch 10:  40% 30/75 [00:30<00:45,  1.02s/it, loss=2.451, lr=2.9e-5]\n",
            "Epoch 10:  41% 31/75 [00:31<00:44,  1.02s/it, loss=2.451, lr=2.9e-5]\n",
            "Epoch 10:  43% 32/75 [00:32<00:44,  1.03s/it, loss=2.451, lr=2.9e-5]\n",
            "Epoch 10:  44% 33/75 [00:33<00:43,  1.03s/it, loss=2.451, lr=2.9e-5]\n",
            "Epoch 10:  45% 34/75 [00:35<00:43,  1.05s/it, loss=2.451, lr=2.9e-5]\n",
            "Epoch 10:  47% 35/75 [00:37<00:42,  1.07s/it, loss=2.451, lr=2.9e-5]\n",
            "Epoch 10:  48% 36/75 [00:38<00:42,  1.08s/it, loss=2.451, lr=2.9e-5]\n",
            "Epoch 10:  49% 37/75 [00:40<00:41,  1.10s/it, loss=2.451, lr=2.9e-5]\n",
            "Epoch 10:  51% 38/75 [00:42<00:41,  1.12s/it, loss=2.451, lr=2.9e-5]\n",
            "Epoch 10:  52% 39/75 [00:43<00:40,  1.13s/it, loss=2.451, lr=2.9e-5]\n",
            "Epoch 10:  53% 40/75 [00:45<00:39,  1.14s/it, loss=2.451, lr=2.9e-5]\n",
            "Epoch 10:  55% 41/75 [00:47<00:38,  1.15s/it, loss=2.451, lr=2.9e-5]\n",
            "Epoch 10:  56% 42/75 [00:48<00:38,  1.16s/it, loss=2.451, lr=2.9e-5]\n",
            "Epoch 10:  57% 43/75 [00:49<00:37,  1.16s/it, loss=2.451, lr=2.9e-5]\n",
            "Epoch 10:  59% 44/75 [00:51<00:36,  1.16s/it, loss=2.451, lr=2.9e-5]\n",
            "Epoch 10:  60% 45/75 [00:52<00:34,  1.16s/it, loss=2.451, lr=2.9e-5]\n",
            "Epoch 10:  61% 46/75 [00:53<00:33,  1.17s/it, loss=2.451, lr=2.9e-5]\n",
            "Epoch 10:  63% 47/75 [00:55<00:33,  1.19s/it, loss=2.451, lr=2.9e-5]\n",
            "Epoch 10:  64% 48/75 [00:57<00:32,  1.20s/it, loss=2.451, lr=2.9e-5]\n",
            "Epoch 10:  65% 49/75 [00:59<00:31,  1.21s/it, loss=2.451, lr=2.9e-5]\n",
            "Epoch 10:  67% 50/75 [01:01<00:30,  1.23s/it, loss=2.451, lr=2.9e-5]\n",
            "Epoch 10:  68% 51/75 [01:03<00:29,  1.24s/it, loss=2.451, lr=2.9e-5]\n",
            "Epoch 10:  69% 52/75 [01:04<00:28,  1.24s/it, loss=2.451, lr=2.9e-5]\n",
            "Epoch 10:  71% 53/75 [01:06<00:27,  1.25s/it, loss=2.451, lr=2.9e-5]\n",
            "Epoch 10:  72% 54/75 [01:07<00:26,  1.26s/it, loss=2.451, lr=2.9e-5]\n",
            "Epoch 10:  73% 55/75 [01:09<00:25,  1.26s/it, loss=2.451, lr=2.9e-5]\n",
            "Epoch 10:  75% 56/75 [01:11<00:24,  1.27s/it, loss=2.451, lr=2.9e-5]\n",
            "Epoch 10:  76% 57/75 [01:12<00:22,  1.27s/it, loss=2.451, lr=2.9e-5]\n",
            "Epoch 10:  77% 58/75 [01:14<00:21,  1.28s/it, loss=2.451, lr=2.9e-5]\n",
            "Epoch 10:  79% 59/75 [01:16<00:20,  1.29s/it, loss=2.451, lr=2.9e-5]\n",
            "Epoch 10:  80% 60/75 [01:17<00:19,  1.29s/it, loss=2.451, lr=2.9e-5]\n",
            "Epoch 10:  81% 61/75 [01:19<00:18,  1.30s/it, loss=2.451, lr=2.9e-5]\n",
            "Epoch 10:  83% 62/75 [01:20<00:16,  1.30s/it, loss=2.451, lr=2.9e-5]\n",
            "Epoch 10:  84% 63/75 [01:22<00:15,  1.31s/it, loss=2.451, lr=2.9e-5]\n",
            "Epoch 10:  85% 64/75 [01:24<00:14,  1.32s/it, loss=2.451, lr=2.9e-5]\n",
            "Epoch 10:  87% 65/75 [01:26<00:13,  1.33s/it, loss=2.451, lr=2.9e-5]\n",
            "Epoch 10:  88% 66/75 [01:28<00:12,  1.34s/it, loss=2.451, lr=2.9e-5]\n",
            "Epoch 10:  89% 67/75 [01:29<00:10,  1.34s/it, loss=2.451, lr=2.9e-5]\n",
            "Epoch 10:  91% 68/75 [01:30<00:09,  1.33s/it, loss=2.451, lr=2.9e-5]\n",
            "Epoch 10:  92% 69/75 [01:32<00:08,  1.34s/it, loss=2.451, lr=2.9e-5]\n",
            "Epoch 10:  93% 70/75 [01:34<00:06,  1.34s/it, loss=2.451, lr=2.9e-5]\n",
            "Epoch 10:  95% 71/75 [01:35<00:05,  1.35s/it, loss=2.451, lr=2.9e-5]\n",
            "Epoch 10:  96% 72/75 [01:37<00:04,  1.35s/it, loss=2.451, lr=2.9e-5]\n",
            "Epoch 10:  97% 73/75 [01:38<00:02,  1.35s/it, loss=2.451, lr=2.9e-5]\n",
            "Epoch 10:  99% 74/75 [01:40<00:01,  1.35s/it, loss=2.451, lr=2.9e-5]\n",
            "Epoch 10: 100% 75/75 [01:41<00:00,  1.35s/it, loss=2.451, lr=2.9e-5]2024-04-21 22:36:19 bleu - corpus_score: That's 100 lines that end in a tokenized period ('.')\n",
            "2024-04-21 22:36:19 bleu - corpus_score: It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2024-04-21 22:36:19 bleu - corpus_score: If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2024-04-21 22:36:39 train_table2text_t5 - validation_epoch_end: valid epoch: 10\n",
            "2024-04-21 22:36:39 train_table2text_t5 - validation_epoch_end: 10 bleu_info: 5.450954310225653\n",
            "2024-04-21 22:36:39 train_table2text_t5 - validation_epoch_end: 10 mover score: (0.17, 0.166)\n",
            "\n",
            "\n",
            "  0% 0/200 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            " 38% 75/200 [00:00<00:00, 740.59it/s]\u001b[A\u001b[A\n",
            "\n",
            "100% 200/200 [00:00<00:00, 535.42it/s]\n",
            "\n",
            "\n",
            "  0% 0/200 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "100% 200/200 [00:00<00:00, 1546.46it/s]\n",
            "2024-04-21 22:36:39 train_table2text_t5 - get_dataloader: loading dev dataloader...\n",
            "2024-04-21 22:36:39 train_table2text_t5 - get_dataloader: done\n",
            "2024-04-21 22:36:39 lightning_base - on_validation_end: ***** Validation results *****\n",
            "2024-04-21 22:36:39 lightning_base - on_validation_end: avg_val_loss = tensor(1.8607, device='cuda:0')\n",
            "2024-04-21 22:36:39 lightning_base - on_validation_end: epoch = 9\n",
            "2024-04-21 22:36:39 lightning_base - on_validation_end: loss = tensor(2.1184, device='cuda:0')\n",
            "2024-04-21 22:36:39 lightning_base - on_validation_end: step_count = 11\n",
            "2024-04-21 22:36:39 lightning_base - on_validation_end: train_loss = tensor(2.1184, device='cuda:0')\n",
            "2024-04-21 22:36:39 lightning_base - on_validation_end: val_avg_bleu = 5.450954310225653\n",
            "2024-04-21 22:36:39 lightning_base - on_validation_end: val_mover = tensor(0.1700, device='cuda:0')\n",
            "2024-04-21 22:36:39 lightning_base - on_validation_end: val_mover_mean1 = 0.17\n",
            "2024-04-21 22:36:39 lightning_base - on_validation_end: val_mover_median1 = 0.166\n",
            "Epoch 10: 100% 75/75 [02:01<00:00,  1.63s/it, loss=2.451, lr=2.9e-5]\n",
            "                                               \u001b[A\n",
            "  0% 0/200 [00:00<?, ?it/s]\u001b[A\n",
            " 34% 68/200 [00:00<00:00, 656.65it/s]\u001b[A\n",
            " 67% 134/200 [00:00<00:00, 484.29it/s]\u001b[A\n",
            "100% 200/200 [00:00<00:00, 479.77it/s]\n",
            "\n",
            "  0% 0/200 [00:00<?, ?it/s]\u001b[A\n",
            "100% 200/200 [00:00<00:00, 1908.10it/s]\n",
            "2024-04-21 22:36:50 train_table2text_t5 - get_dataloader: loading train dataloader...\n",
            "2024-04-21 22:36:50 train_table2text_t5 - get_dataloader: done\n",
            "Epoch 11:  33% 25/75 [00:21<00:43,  1.14it/s, loss=2.282, lr=2.9e-5]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 11:  35% 26/75 [00:24<00:45,  1.07it/s, loss=2.282, lr=2.9e-5]\n",
            "Epoch 11:  36% 27/75 [00:26<00:46,  1.03it/s, loss=2.282, lr=2.9e-5]\n",
            "Epoch 11:  37% 28/75 [00:27<00:46,  1.01it/s, loss=2.282, lr=2.9e-5]\n",
            "Epoch 11:  39% 29/75 [00:31<00:49,  1.08s/it, loss=2.282, lr=2.9e-5]\n",
            "Epoch 11:  40% 30/75 [00:32<00:49,  1.09s/it, loss=2.282, lr=2.9e-5]\n",
            "Epoch 11:  41% 31/75 [00:34<00:48,  1.10s/it, loss=2.282, lr=2.9e-5]\n",
            "Epoch 11:  43% 32/75 [00:35<00:47,  1.11s/it, loss=2.282, lr=2.9e-5]\n",
            "Epoch 11:  44% 33/75 [00:36<00:46,  1.11s/it, loss=2.282, lr=2.9e-5]\n",
            "Epoch 11:  45% 34/75 [00:38<00:46,  1.13s/it, loss=2.282, lr=2.9e-5]\n",
            "Epoch 11:  47% 35/75 [00:41<00:47,  1.18s/it, loss=2.282, lr=2.9e-5]\n",
            "Epoch 11:  48% 36/75 [00:43<00:46,  1.20s/it, loss=2.282, lr=2.9e-5]\n",
            "Epoch 11:  49% 37/75 [00:46<00:47,  1.25s/it, loss=2.282, lr=2.9e-5]\n",
            "Epoch 11:  51% 38/75 [00:49<00:47,  1.29s/it, loss=2.282, lr=2.9e-5]\n",
            "Epoch 11:  52% 39/75 [00:50<00:46,  1.29s/it, loss=2.282, lr=2.9e-5]\n",
            "Epoch 11:  53% 40/75 [00:53<00:46,  1.33s/it, loss=2.282, lr=2.9e-5]\n",
            "Epoch 11:  55% 41/75 [00:54<00:45,  1.34s/it, loss=2.282, lr=2.9e-5]\n",
            "Epoch 11:  56% 42/75 [00:56<00:44,  1.34s/it, loss=2.282, lr=2.9e-5]\n",
            "Epoch 11:  57% 43/75 [00:57<00:42,  1.34s/it, loss=2.282, lr=2.9e-5]\n",
            "Epoch 11:  59% 44/75 [00:59<00:41,  1.35s/it, loss=2.282, lr=2.9e-5]\n",
            "Epoch 11:  60% 45/75 [01:00<00:40,  1.35s/it, loss=2.282, lr=2.9e-5]\n",
            "Epoch 11:  61% 46/75 [01:02<00:39,  1.36s/it, loss=2.282, lr=2.9e-5]\n",
            "Epoch 11:  63% 47/75 [01:06<00:39,  1.41s/it, loss=2.282, lr=2.9e-5]\n",
            "Epoch 11:  64% 48/75 [01:07<00:38,  1.41s/it, loss=2.282, lr=2.9e-5]\n",
            "Epoch 11:  65% 49/75 [01:10<00:37,  1.43s/it, loss=2.282, lr=2.9e-5]\n",
            "Epoch 11:  67% 50/75 [01:12<00:36,  1.46s/it, loss=2.282, lr=2.9e-5]\n",
            "Epoch 11:  68% 51/75 [01:15<00:35,  1.47s/it, loss=2.282, lr=2.9e-5]\n",
            "Epoch 11:  69% 52/75 [01:16<00:33,  1.47s/it, loss=2.282, lr=2.9e-5]\n",
            "Epoch 11:  71% 53/75 [01:19<00:32,  1.50s/it, loss=2.282, lr=2.9e-5]\n",
            "Epoch 11:  72% 54/75 [01:21<00:31,  1.51s/it, loss=2.282, lr=2.9e-5]\n",
            "Epoch 11:  73% 55/75 [01:22<00:30,  1.51s/it, loss=2.282, lr=2.9e-5]\n",
            "Epoch 11:  75% 56/75 [01:24<00:28,  1.51s/it, loss=2.282, lr=2.9e-5]\n",
            "Epoch 11:  76% 57/75 [01:26<00:27,  1.51s/it, loss=2.282, lr=2.9e-5]\n",
            "Epoch 11:  77% 58/75 [01:27<00:25,  1.51s/it, loss=2.282, lr=2.9e-5]\n",
            "Epoch 11:  79% 59/75 [01:29<00:24,  1.52s/it, loss=2.282, lr=2.9e-5]\n",
            "Epoch 11:  80% 60/75 [01:31<00:22,  1.53s/it, loss=2.282, lr=2.9e-5]\n",
            "Epoch 11:  81% 61/75 [01:34<00:21,  1.55s/it, loss=2.282, lr=2.9e-5]\n",
            "Epoch 11:  83% 62/75 [01:36<00:20,  1.56s/it, loss=2.282, lr=2.9e-5]\n",
            "Epoch 11:  84% 63/75 [01:38<00:18,  1.57s/it, loss=2.282, lr=2.9e-5]\n",
            "Epoch 11:  85% 64/75 [01:41<00:17,  1.58s/it, loss=2.282, lr=2.9e-5]\n",
            "Epoch 11:  87% 65/75 [01:43<00:15,  1.59s/it, loss=2.282, lr=2.9e-5]\n",
            "Epoch 11:  88% 66/75 [01:45<00:14,  1.60s/it, loss=2.282, lr=2.9e-5]\n",
            "Epoch 11:  89% 67/75 [01:47<00:12,  1.60s/it, loss=2.282, lr=2.9e-5]\n",
            "Epoch 11:  91% 68/75 [01:48<00:11,  1.60s/it, loss=2.282, lr=2.9e-5]\n",
            "Epoch 11:  92% 69/75 [01:51<00:09,  1.62s/it, loss=2.282, lr=2.9e-5]\n",
            "Epoch 11:  93% 70/75 [01:53<00:08,  1.62s/it, loss=2.282, lr=2.9e-5]\n",
            "Epoch 11:  95% 71/75 [01:56<00:06,  1.64s/it, loss=2.282, lr=2.9e-5]\n",
            "Epoch 11:  96% 72/75 [01:57<00:04,  1.64s/it, loss=2.282, lr=2.9e-5]\n",
            "Epoch 11:  97% 73/75 [01:59<00:03,  1.63s/it, loss=2.282, lr=2.9e-5]\n",
            "Epoch 11:  99% 74/75 [02:01<00:01,  1.64s/it, loss=2.282, lr=2.9e-5]\n",
            "Epoch 11: 100% 75/75 [02:03<00:00,  1.64s/it, loss=2.282, lr=2.9e-5]2024-04-21 22:38:55 bleu - corpus_score: That's 100 lines that end in a tokenized period ('.')\n",
            "2024-04-21 22:38:55 bleu - corpus_score: It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2024-04-21 22:38:55 bleu - corpus_score: If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2024-04-21 22:39:20 train_table2text_t5 - validation_epoch_end: valid epoch: 11\n",
            "2024-04-21 22:39:20 train_table2text_t5 - validation_epoch_end: 11 bleu_info: 8.94112115415741\n",
            "2024-04-21 22:39:20 train_table2text_t5 - validation_epoch_end: 11 mover score: (0.196, 0.195)\n",
            "\n",
            "\n",
            "  0% 0/200 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            " 36% 72/200 [00:00<00:00, 719.68it/s]\u001b[A\u001b[A\n",
            "\n",
            " 72% 144/200 [00:00<00:00, 525.61it/s]\u001b[A\u001b[A\n",
            "\n",
            "100% 200/200 [00:00<00:00, 472.08it/s]\n",
            "\n",
            "\n",
            "  0% 0/200 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "100% 200/200 [00:00<00:00, 1819.70it/s]\n",
            "2024-04-21 22:39:20 train_table2text_t5 - get_dataloader: loading dev dataloader...\n",
            "2024-04-21 22:39:20 train_table2text_t5 - get_dataloader: done\n",
            "2024-04-21 22:39:20 lightning_base - on_validation_end: ***** Validation results *****\n",
            "2024-04-21 22:39:20 lightning_base - on_validation_end: avg_val_loss = tensor(1.7111, device='cuda:0')\n",
            "2024-04-21 22:39:20 lightning_base - on_validation_end: epoch = 10\n",
            "2024-04-21 22:39:20 lightning_base - on_validation_end: loss = tensor(2.1460, device='cuda:0')\n",
            "2024-04-21 22:39:20 lightning_base - on_validation_end: step_count = 12\n",
            "2024-04-21 22:39:20 lightning_base - on_validation_end: train_loss = tensor(2.1460, device='cuda:0')\n",
            "2024-04-21 22:39:20 lightning_base - on_validation_end: val_avg_bleu = 8.94112115415741\n",
            "2024-04-21 22:39:20 lightning_base - on_validation_end: val_mover = tensor(0.1960, device='cuda:0')\n",
            "2024-04-21 22:39:20 lightning_base - on_validation_end: val_mover_mean1 = 0.196\n",
            "2024-04-21 22:39:20 lightning_base - on_validation_end: val_mover_median1 = 0.195\n",
            "Epoch 11: 100% 75/75 [02:29<00:00,  2.00s/it, loss=2.282, lr=2.9e-5]\n",
            "                                               \u001b[A\n",
            "  0% 0/200 [00:00<?, ?it/s]\u001b[A\n",
            " 26% 53/200 [00:00<00:00, 516.49it/s]\u001b[A\n",
            " 52% 105/200 [00:00<00:00, 388.07it/s]\u001b[A\n",
            " 73% 146/200 [00:00<00:00, 302.18it/s]\u001b[A\n",
            "100% 200/200 [00:00<00:00, 314.97it/s]\n",
            "\n",
            "  0% 0/200 [00:00<?, ?it/s]\u001b[A\n",
            " 46% 92/200 [00:00<00:00, 894.38it/s]\u001b[A\n",
            "100% 200/200 [00:00<00:00, 914.32it/s]\n",
            "2024-04-21 22:39:45 train_table2text_t5 - get_dataloader: loading train dataloader...\n",
            "2024-04-21 22:39:45 train_table2text_t5 - get_dataloader: done\n",
            "Epoch 12:  33% 25/75 [00:22<00:44,  1.11it/s, loss=2.199, lr=2.9e-5]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 12:  35% 26/75 [00:25<00:47,  1.03it/s, loss=2.199, lr=2.9e-5]\n",
            "Epoch 12:  36% 27/75 [00:28<00:50,  1.06s/it, loss=2.199, lr=2.9e-5]\n",
            "Epoch 12:  37% 28/75 [00:29<00:50,  1.07s/it, loss=2.199, lr=2.9e-5]\n",
            "Epoch 12:  39% 29/75 [00:32<00:52,  1.13s/it, loss=2.199, lr=2.9e-5]\n",
            "Epoch 12:  40% 30/75 [00:34<00:51,  1.14s/it, loss=2.199, lr=2.9e-5]\n",
            "Epoch 12:  41% 31/75 [00:36<00:51,  1.17s/it, loss=2.199, lr=2.9e-5]\n",
            "Epoch 12:  43% 32/75 [00:38<00:51,  1.19s/it, loss=2.199, lr=2.9e-5]\n",
            "Epoch 12:  44% 33/75 [00:39<00:50,  1.21s/it, loss=2.199, lr=2.9e-5]\n",
            "Epoch 12:  45% 34/75 [00:41<00:49,  1.22s/it, loss=2.199, lr=2.9e-5]\n",
            "Epoch 12:  47% 35/75 [00:42<00:48,  1.22s/it, loss=2.199, lr=2.9e-5]\n",
            "Epoch 12:  48% 36/75 [00:44<00:48,  1.24s/it, loss=2.199, lr=2.9e-5]\n",
            "Epoch 12:  49% 37/75 [00:46<00:47,  1.25s/it, loss=2.199, lr=2.9e-5]\n",
            "Epoch 12:  51% 38/75 [00:48<00:47,  1.28s/it, loss=2.199, lr=2.9e-5]\n",
            "Epoch 12:  52% 39/75 [00:51<00:47,  1.32s/it, loss=2.199, lr=2.9e-5]\n",
            "Epoch 12:  53% 40/75 [00:53<00:46,  1.33s/it, loss=2.199, lr=2.9e-5]\n",
            "Epoch 12:  55% 41/75 [00:55<00:46,  1.35s/it, loss=2.199, lr=2.9e-5]\n",
            "Epoch 12:  56% 42/75 [00:56<00:44,  1.35s/it, loss=2.199, lr=2.9e-5]\n",
            "Epoch 12:  57% 43/75 [00:58<00:43,  1.36s/it, loss=2.199, lr=2.9e-5]\n",
            "Epoch 12:  59% 44/75 [01:00<00:42,  1.38s/it, loss=2.199, lr=2.9e-5]\n",
            "Epoch 12:  60% 45/75 [01:01<00:41,  1.38s/it, loss=2.199, lr=2.9e-5]\n",
            "Epoch 12:  61% 46/75 [01:04<00:40,  1.39s/it, loss=2.199, lr=2.9e-5]\n",
            "Epoch 12:  63% 47/75 [01:06<00:39,  1.41s/it, loss=2.199, lr=2.9e-5]\n",
            "Epoch 12:  64% 48/75 [01:07<00:38,  1.42s/it, loss=2.199, lr=2.9e-5]\n",
            "Epoch 12:  65% 49/75 [01:10<00:37,  1.43s/it, loss=2.199, lr=2.9e-5]\n",
            "Epoch 12:  67% 50/75 [01:13<00:36,  1.48s/it, loss=2.199, lr=2.9e-5]\n",
            "Epoch 12:  68% 51/75 [01:16<00:36,  1.51s/it, loss=2.199, lr=2.9e-5]\n",
            "Epoch 12:  69% 52/75 [01:18<00:34,  1.50s/it, loss=2.199, lr=2.9e-5]\n",
            "Epoch 12:  71% 53/75 [01:21<00:33,  1.53s/it, loss=2.199, lr=2.9e-5]\n",
            "Epoch 12:  72% 54/75 [01:23<00:32,  1.56s/it, loss=2.199, lr=2.9e-5]\n",
            "Epoch 12:  73% 55/75 [01:25<00:30,  1.55s/it, loss=2.199, lr=2.9e-5]\n",
            "Epoch 12:  75% 56/75 [01:28<00:29,  1.58s/it, loss=2.199, lr=2.9e-5]\n",
            "Epoch 12:  76% 57/75 [01:29<00:28,  1.58s/it, loss=2.199, lr=2.9e-5]\n",
            "Epoch 12:  77% 58/75 [01:31<00:26,  1.58s/it, loss=2.199, lr=2.9e-5]\n",
            "Epoch 12:  79% 59/75 [01:33<00:25,  1.58s/it, loss=2.199, lr=2.9e-5]\n",
            "Epoch 12:  80% 60/75 [01:34<00:23,  1.58s/it, loss=2.199, lr=2.9e-5]\n",
            "Epoch 12:  81% 61/75 [01:36<00:22,  1.58s/it, loss=2.199, lr=2.9e-5]\n",
            "Epoch 12:  83% 62/75 [01:38<00:20,  1.58s/it, loss=2.199, lr=2.9e-5]\n",
            "Epoch 12:  84% 63/75 [01:40<00:19,  1.60s/it, loss=2.199, lr=2.9e-5]\n",
            "Epoch 12:  85% 64/75 [01:43<00:17,  1.62s/it, loss=2.199, lr=2.9e-5]\n",
            "Epoch 12:  87% 65/75 [01:45<00:16,  1.62s/it, loss=2.199, lr=2.9e-5]\n",
            "Epoch 12:  88% 66/75 [01:47<00:14,  1.62s/it, loss=2.199, lr=2.9e-5]\n",
            "Epoch 12:  89% 67/75 [01:48<00:12,  1.62s/it, loss=2.199, lr=2.9e-5]\n",
            "Epoch 12:  91% 68/75 [01:50<00:11,  1.62s/it, loss=2.199, lr=2.9e-5]\n",
            "Epoch 12:  92% 69/75 [01:53<00:09,  1.65s/it, loss=2.199, lr=2.9e-5]\n",
            "Epoch 12:  93% 70/75 [01:55<00:08,  1.65s/it, loss=2.199, lr=2.9e-5]\n",
            "Epoch 12:  95% 71/75 [01:57<00:06,  1.66s/it, loss=2.199, lr=2.9e-5]\n",
            "Epoch 12:  96% 72/75 [01:59<00:04,  1.66s/it, loss=2.199, lr=2.9e-5]\n",
            "Epoch 12:  97% 73/75 [02:00<00:03,  1.65s/it, loss=2.199, lr=2.9e-5]\n",
            "Epoch 12:  99% 74/75 [02:02<00:01,  1.65s/it, loss=2.199, lr=2.9e-5]\n",
            "Epoch 12: 100% 75/75 [02:03<00:00,  1.65s/it, loss=2.199, lr=2.9e-5]2024-04-21 22:41:50 bleu - corpus_score: That's 100 lines that end in a tokenized period ('.')\n",
            "2024-04-21 22:41:50 bleu - corpus_score: It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2024-04-21 22:41:50 bleu - corpus_score: If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2024-04-21 22:42:18 train_table2text_t5 - validation_epoch_end: valid epoch: 12\n",
            "2024-04-21 22:42:18 train_table2text_t5 - validation_epoch_end: 12 bleu_info: 9.822356995999694\n",
            "2024-04-21 22:42:18 train_table2text_t5 - validation_epoch_end: 12 mover score: (0.199, 0.186)\n",
            "\n",
            "\n",
            "  0% 0/200 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            " 35% 70/200 [00:00<00:00, 698.82it/s]\u001b[A\u001b[A\n",
            "\n",
            " 70% 140/200 [00:00<00:00, 529.65it/s]\u001b[A\u001b[A\n",
            "\n",
            "100% 200/200 [00:00<00:00, 527.25it/s]\n",
            "\n",
            "\n",
            "  0% 0/200 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "100% 200/200 [00:00<00:00, 1690.71it/s]\n",
            "2024-04-21 22:42:18 train_table2text_t5 - get_dataloader: loading dev dataloader...\n",
            "2024-04-21 22:42:18 train_table2text_t5 - get_dataloader: done\n",
            "2024-04-21 22:42:18 lightning_base - on_validation_end: ***** Validation results *****\n",
            "2024-04-21 22:42:18 lightning_base - on_validation_end: avg_val_loss = tensor(1.5881, device='cuda:0')\n",
            "2024-04-21 22:42:18 lightning_base - on_validation_end: epoch = 11\n",
            "2024-04-21 22:42:18 lightning_base - on_validation_end: loss = tensor(2.5053, device='cuda:0')\n",
            "2024-04-21 22:42:18 lightning_base - on_validation_end: step_count = 13\n",
            "2024-04-21 22:42:18 lightning_base - on_validation_end: train_loss = tensor(2.5053, device='cuda:0')\n",
            "2024-04-21 22:42:18 lightning_base - on_validation_end: val_avg_bleu = 9.822356995999694\n",
            "2024-04-21 22:42:18 lightning_base - on_validation_end: val_mover = tensor(0.1990, device='cuda:0')\n",
            "2024-04-21 22:42:18 lightning_base - on_validation_end: val_mover_mean1 = 0.199\n",
            "2024-04-21 22:42:18 lightning_base - on_validation_end: val_mover_median1 = 0.186\n",
            "Epoch 12: 100% 75/75 [02:32<00:00,  2.04s/it, loss=2.199, lr=2.9e-5]\n",
            "                                               \u001b[A\n",
            "  0% 0/200 [00:00<?, ?it/s]\u001b[A\n",
            " 32% 65/200 [00:00<00:00, 649.16it/s]\u001b[A\n",
            " 65% 130/200 [00:00<00:00, 559.54it/s]\u001b[A\n",
            "100% 200/200 [00:00<00:00, 511.12it/s]\n",
            "\n",
            "  0% 0/200 [00:00<?, ?it/s]\u001b[A\n",
            "100% 200/200 [00:00<00:00, 1941.93it/s]\n",
            "2024-04-21 22:42:31 train_table2text_t5 - get_dataloader: loading train dataloader...\n",
            "2024-04-21 22:42:31 train_table2text_t5 - get_dataloader: done\n",
            "Epoch 13:  33% 25/75 [00:21<00:43,  1.14it/s, loss=2.094, lr=2.9e-5]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 13:  35% 26/75 [00:24<00:46,  1.06it/s, loss=2.094, lr=2.9e-5]\n",
            "Epoch 13:  36% 27/75 [00:26<00:47,  1.01it/s, loss=2.094, lr=2.9e-5]\n",
            "Epoch 13:  37% 28/75 [00:28<00:47,  1.00s/it, loss=2.094, lr=2.9e-5]\n",
            "Epoch 13:  39% 29/75 [00:30<00:48,  1.05s/it, loss=2.094, lr=2.9e-5]\n",
            "Epoch 13:  40% 30/75 [00:32<00:48,  1.07s/it, loss=2.094, lr=2.9e-5]\n",
            "Epoch 13:  41% 31/75 [00:34<00:48,  1.10s/it, loss=2.094, lr=2.9e-5]\n",
            "Epoch 13:  43% 32/75 [00:35<00:48,  1.12s/it, loss=2.094, lr=2.9e-5]\n",
            "Epoch 13:  44% 33/75 [00:37<00:47,  1.13s/it, loss=2.094, lr=2.9e-5]\n",
            "Epoch 13:  45% 34/75 [00:38<00:46,  1.14s/it, loss=2.094, lr=2.9e-5]\n",
            "Epoch 13:  47% 35/75 [00:39<00:45,  1.14s/it, loss=2.094, lr=2.9e-5]\n",
            "Epoch 13:  48% 36/75 [00:42<00:45,  1.17s/it, loss=2.094, lr=2.9e-5]\n",
            "Epoch 13:  49% 37/75 [00:43<00:44,  1.18s/it, loss=2.094, lr=2.9e-5]\n",
            "Epoch 13:  51% 38/75 [00:46<00:45,  1.22s/it, loss=2.094, lr=2.9e-5]\n",
            "Epoch 13:  52% 39/75 [00:48<00:44,  1.24s/it, loss=2.094, lr=2.9e-5]\n",
            "Epoch 13:  53% 40/75 [00:50<00:43,  1.26s/it, loss=2.094, lr=2.9e-5]\n",
            "Epoch 13:  55% 41/75 [00:52<00:43,  1.27s/it, loss=2.094, lr=2.9e-5]\n",
            "Epoch 13:  56% 42/75 [00:53<00:42,  1.28s/it, loss=2.094, lr=2.9e-5]\n",
            "Epoch 13:  57% 43/75 [00:55<00:40,  1.28s/it, loss=2.094, lr=2.9e-5]\n",
            "Epoch 13:  59% 44/75 [00:57<00:40,  1.30s/it, loss=2.094, lr=2.9e-5]\n",
            "Epoch 13:  60% 45/75 [00:58<00:38,  1.30s/it, loss=2.094, lr=2.9e-5]\n",
            "Epoch 13:  61% 46/75 [01:00<00:38,  1.31s/it, loss=2.094, lr=2.9e-5]\n",
            "Epoch 13:  63% 47/75 [01:02<00:37,  1.34s/it, loss=2.094, lr=2.9e-5]\n",
            "Epoch 13:  64% 48/75 [01:04<00:36,  1.34s/it, loss=2.094, lr=2.9e-5]\n",
            "Epoch 13:  65% 49/75 [01:08<00:36,  1.39s/it, loss=2.094, lr=2.9e-5]\n",
            "Epoch 13:  67% 50/75 [01:12<00:36,  1.44s/it, loss=2.094, lr=2.9e-5]\n",
            "Epoch 13:  68% 51/75 [01:13<00:34,  1.45s/it, loss=2.094, lr=2.9e-5]\n",
            "Epoch 13:  69% 52/75 [01:15<00:33,  1.45s/it, loss=2.094, lr=2.9e-5]\n",
            "Epoch 13:  71% 53/75 [01:17<00:32,  1.47s/it, loss=2.094, lr=2.9e-5]\n",
            "Epoch 13:  72% 54/75 [01:20<00:31,  1.49s/it, loss=2.094, lr=2.9e-5]\n",
            "Epoch 13:  73% 55/75 [01:22<00:29,  1.49s/it, loss=2.094, lr=2.9e-5]\n",
            "Epoch 13:  75% 56/75 [01:25<00:28,  1.52s/it, loss=2.094, lr=2.9e-5]\n",
            "Epoch 13:  76% 57/75 [01:26<00:27,  1.52s/it, loss=2.094, lr=2.9e-5]\n",
            "Epoch 13:  77% 58/75 [01:28<00:25,  1.53s/it, loss=2.094, lr=2.9e-5]\n",
            "Epoch 13:  79% 59/75 [01:30<00:24,  1.53s/it, loss=2.094, lr=2.9e-5]\n",
            "Epoch 13:  80% 60/75 [01:31<00:22,  1.53s/it, loss=2.094, lr=2.9e-5]\n",
            "Epoch 13:  81% 61/75 [01:33<00:21,  1.53s/it, loss=2.094, lr=2.9e-5]\n",
            "Epoch 13:  83% 62/75 [01:35<00:20,  1.54s/it, loss=2.094, lr=2.9e-5]\n",
            "Epoch 13:  84% 63/75 [01:38<00:18,  1.56s/it, loss=2.094, lr=2.9e-5]\n",
            "Epoch 13:  85% 64/75 [01:40<00:17,  1.57s/it, loss=2.094, lr=2.9e-5]\n",
            "Epoch 13:  87% 65/75 [01:42<00:15,  1.57s/it, loss=2.094, lr=2.9e-5]\n",
            "Epoch 13:  88% 66/75 [01:44<00:14,  1.59s/it, loss=2.094, lr=2.9e-5]\n",
            "Epoch 13:  89% 67/75 [01:46<00:12,  1.59s/it, loss=2.094, lr=2.9e-5]\n",
            "Epoch 13:  91% 68/75 [01:48<00:11,  1.59s/it, loss=2.094, lr=2.9e-5]\n",
            "Epoch 13:  92% 69/75 [01:50<00:09,  1.60s/it, loss=2.094, lr=2.9e-5]\n",
            "Epoch 13:  93% 70/75 [01:52<00:08,  1.61s/it, loss=2.094, lr=2.9e-5]\n",
            "Epoch 13:  95% 71/75 [01:54<00:06,  1.62s/it, loss=2.094, lr=2.9e-5]\n",
            "Epoch 13:  96% 72/75 [01:56<00:04,  1.61s/it, loss=2.094, lr=2.9e-5]\n",
            "Epoch 13:  97% 73/75 [01:58<00:03,  1.62s/it, loss=2.094, lr=2.9e-5]\n",
            "Epoch 13:  99% 74/75 [01:59<00:01,  1.62s/it, loss=2.094, lr=2.9e-5]\n",
            "Epoch 13: 100% 75/75 [02:02<00:00,  1.63s/it, loss=2.094, lr=2.9e-5]2024-04-21 22:44:34 bleu - corpus_score: That's 100 lines that end in a tokenized period ('.')\n",
            "2024-04-21 22:44:34 bleu - corpus_score: It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2024-04-21 22:44:34 bleu - corpus_score: If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2024-04-21 22:45:00 train_table2text_t5 - validation_epoch_end: valid epoch: 13\n",
            "2024-04-21 22:45:00 train_table2text_t5 - validation_epoch_end: 13 bleu_info: 10.043592907538995\n",
            "2024-04-21 22:45:00 train_table2text_t5 - validation_epoch_end: 13 mover score: (0.198, 0.192)\n",
            "\n",
            "\n",
            "  0% 0/200 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            " 38% 75/200 [00:00<00:00, 745.36it/s]\u001b[A\u001b[A\n",
            "\n",
            "100% 200/200 [00:00<00:00, 542.36it/s]\n",
            "\n",
            "\n",
            "  0% 0/200 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "100% 200/200 [00:00<00:00, 1954.99it/s]\n",
            "2024-04-21 22:45:00 train_table2text_t5 - get_dataloader: loading dev dataloader...\n",
            "2024-04-21 22:45:00 train_table2text_t5 - get_dataloader: done\n",
            "2024-04-21 22:45:00 lightning_base - on_validation_end: ***** Validation results *****\n",
            "2024-04-21 22:45:00 lightning_base - on_validation_end: avg_val_loss = tensor(1.4686, device='cuda:0')\n",
            "2024-04-21 22:45:00 lightning_base - on_validation_end: epoch = 12\n",
            "2024-04-21 22:45:00 lightning_base - on_validation_end: loss = tensor(1.9923, device='cuda:0')\n",
            "2024-04-21 22:45:00 lightning_base - on_validation_end: step_count = 14\n",
            "2024-04-21 22:45:00 lightning_base - on_validation_end: train_loss = tensor(1.9923, device='cuda:0')\n",
            "2024-04-21 22:45:00 lightning_base - on_validation_end: val_avg_bleu = 10.043592907538995\n",
            "2024-04-21 22:45:00 lightning_base - on_validation_end: val_mover = tensor(0.1980, device='cuda:0')\n",
            "2024-04-21 22:45:00 lightning_base - on_validation_end: val_mover_mean1 = 0.198\n",
            "2024-04-21 22:45:00 lightning_base - on_validation_end: val_mover_median1 = 0.192\n",
            "Epoch 13: 100% 75/75 [02:29<00:00,  2.00s/it, loss=2.094, lr=2.9e-5]\n",
            "                                               \u001b[A\n",
            "  0% 0/200 [00:00<?, ?it/s]\u001b[A\n",
            " 18% 36/200 [00:00<00:00, 352.05it/s]\u001b[A\n",
            " 38% 75/200 [00:00<00:00, 373.41it/s]\u001b[A\n",
            " 56% 113/200 [00:00<00:00, 303.05it/s]\u001b[A\n",
            " 72% 145/200 [00:00<00:00, 289.64it/s]\u001b[A\n",
            "100% 200/200 [00:00<00:00, 289.82it/s]\n",
            "\n",
            "  0% 0/200 [00:00<?, ?it/s]\u001b[A\n",
            "100% 200/200 [00:00<00:00, 1015.13it/s]\n",
            "2024-04-21 22:45:09 train_table2text_t5 - get_dataloader: loading train dataloader...\n",
            "2024-04-21 22:45:09 train_table2text_t5 - get_dataloader: done\n",
            "Epoch 14:  33% 25/75 [00:21<00:43,  1.15it/s, loss=1.986, lr=2.9e-5]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 14:  35% 26/75 [00:24<00:46,  1.05it/s, loss=1.986, lr=2.9e-5]\n",
            "Epoch 14:  36% 27/75 [00:27<00:48,  1.00s/it, loss=1.986, lr=2.9e-5]\n",
            "Epoch 14:  37% 28/75 [00:28<00:47,  1.02s/it, loss=1.986, lr=2.9e-5]\n",
            "Epoch 14:  39% 29/75 [00:31<00:49,  1.07s/it, loss=1.986, lr=2.9e-5]\n",
            "Epoch 14:  40% 30/75 [00:33<00:49,  1.10s/it, loss=1.986, lr=2.9e-5]\n",
            "Epoch 14:  41% 31/75 [00:34<00:48,  1.11s/it, loss=1.986, lr=2.9e-5]\n",
            "Epoch 14:  43% 32/75 [00:36<00:48,  1.13s/it, loss=1.986, lr=2.9e-5]\n",
            "Epoch 14:  44% 33/75 [00:37<00:48,  1.15s/it, loss=1.986, lr=2.9e-5]\n",
            "Epoch 14:  45% 34/75 [00:39<00:47,  1.17s/it, loss=1.986, lr=2.9e-5]\n",
            "Epoch 14:  47% 35/75 [00:41<00:47,  1.19s/it, loss=1.986, lr=2.9e-5]\n",
            "Epoch 14:  48% 36/75 [00:43<00:47,  1.22s/it, loss=1.986, lr=2.9e-5]\n",
            "Epoch 14:  49% 37/75 [00:45<00:47,  1.24s/it, loss=1.986, lr=2.9e-5]\n",
            "Epoch 14:  51% 38/75 [00:47<00:46,  1.25s/it, loss=1.986, lr=2.9e-5]\n",
            "Epoch 14:  52% 39/75 [00:49<00:45,  1.27s/it, loss=1.986, lr=2.9e-5]\n",
            "Epoch 14:  53% 40/75 [00:51<00:45,  1.29s/it, loss=1.986, lr=2.9e-5]\n",
            "Epoch 14:  55% 41/75 [00:53<00:44,  1.31s/it, loss=1.986, lr=2.9e-5]\n",
            "Epoch 14:  56% 42/75 [00:55<00:43,  1.31s/it, loss=1.986, lr=2.9e-5]\n",
            "Epoch 14:  57% 43/75 [00:56<00:41,  1.31s/it, loss=1.986, lr=2.9e-5]\n",
            "Epoch 14:  59% 44/75 [00:57<00:40,  1.32s/it, loss=1.986, lr=2.9e-5]\n",
            "Epoch 14:  60% 45/75 [00:59<00:39,  1.33s/it, loss=1.986, lr=2.9e-5]\n",
            "Epoch 14:  61% 46/75 [01:01<00:38,  1.34s/it, loss=1.986, lr=2.9e-5]\n",
            "Epoch 14:  63% 47/75 [01:04<00:38,  1.38s/it, loss=1.986, lr=2.9e-5]\n",
            "Epoch 14:  64% 48/75 [01:06<00:37,  1.38s/it, loss=1.986, lr=2.9e-5]\n",
            "Epoch 14:  65% 49/75 [01:10<00:37,  1.44s/it, loss=1.986, lr=2.9e-5]\n",
            "Epoch 14:  67% 50/75 [01:15<00:37,  1.51s/it, loss=1.986, lr=2.9e-5]\n",
            "Epoch 14:  68% 51/75 [01:17<00:36,  1.51s/it, loss=1.986, lr=2.9e-5]\n",
            "Epoch 14:  69% 52/75 [01:18<00:34,  1.51s/it, loss=1.986, lr=2.9e-5]\n",
            "Epoch 14:  71% 53/75 [01:22<00:34,  1.55s/it, loss=1.986, lr=2.9e-5]\n",
            "Epoch 14:  72% 54/75 [01:24<00:32,  1.56s/it, loss=1.986, lr=2.9e-5]\n",
            "Epoch 14:  73% 55/75 [01:25<00:31,  1.56s/it, loss=1.986, lr=2.9e-5]\n",
            "Epoch 14:  75% 56/75 [01:29<00:30,  1.59s/it, loss=1.986, lr=2.9e-5]\n",
            "Epoch 14:  76% 57/75 [01:30<00:28,  1.59s/it, loss=1.986, lr=2.9e-5]\n",
            "Epoch 14:  77% 58/75 [01:31<00:26,  1.59s/it, loss=1.986, lr=2.9e-5]\n",
            "Epoch 14:  79% 59/75 [01:33<00:25,  1.58s/it, loss=1.986, lr=2.9e-5]\n",
            "Epoch 14:  80% 60/75 [01:35<00:23,  1.59s/it, loss=1.986, lr=2.9e-5]\n",
            "Epoch 14:  81% 61/75 [01:37<00:22,  1.59s/it, loss=1.986, lr=2.9e-5]\n",
            "Epoch 14:  83% 62/75 [01:39<00:20,  1.60s/it, loss=1.986, lr=2.9e-5]\n",
            "Epoch 14:  84% 63/75 [01:41<00:19,  1.61s/it, loss=1.986, lr=2.9e-5]\n",
            "Epoch 14:  85% 64/75 [01:44<00:17,  1.63s/it, loss=1.986, lr=2.9e-5]\n",
            "Epoch 14:  87% 65/75 [01:45<00:16,  1.63s/it, loss=1.986, lr=2.9e-5]\n",
            "Epoch 14:  88% 66/75 [01:47<00:14,  1.64s/it, loss=1.986, lr=2.9e-5]\n",
            "Epoch 14:  89% 67/75 [01:49<00:13,  1.64s/it, loss=1.986, lr=2.9e-5]\n",
            "Epoch 14:  91% 68/75 [01:51<00:11,  1.64s/it, loss=1.986, lr=2.9e-5]\n",
            "Epoch 14:  92% 69/75 [01:54<00:09,  1.66s/it, loss=1.986, lr=2.9e-5]\n",
            "Epoch 14:  93% 70/75 [01:56<00:08,  1.67s/it, loss=1.986, lr=2.9e-5]\n",
            "Epoch 14:  95% 71/75 [01:59<00:06,  1.68s/it, loss=1.986, lr=2.9e-5]\n",
            "Epoch 14:  96% 72/75 [02:01<00:05,  1.68s/it, loss=1.986, lr=2.9e-5]\n",
            "Epoch 14:  97% 73/75 [02:02<00:03,  1.68s/it, loss=1.986, lr=2.9e-5]\n",
            "Epoch 14:  99% 74/75 [02:05<00:01,  1.70s/it, loss=1.986, lr=2.9e-5]\n",
            "Epoch 14: 100% 75/75 [02:07<00:00,  1.70s/it, loss=1.986, lr=2.9e-5]2024-04-21 22:47:17 bleu - corpus_score: That's 100 lines that end in a tokenized period ('.')\n",
            "2024-04-21 22:47:17 bleu - corpus_score: It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2024-04-21 22:47:17 bleu - corpus_score: If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2024-04-21 22:47:45 train_table2text_t5 - validation_epoch_end: valid epoch: 14\n",
            "2024-04-21 22:47:45 train_table2text_t5 - validation_epoch_end: 14 bleu_info: 10.745207065524582\n",
            "2024-04-21 22:47:45 train_table2text_t5 - validation_epoch_end: 14 mover score: (0.21, 0.203)\n",
            "\n",
            "\n",
            "  0% 0/200 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            " 34% 67/200 [00:00<00:00, 666.29it/s]\u001b[A\u001b[A\n",
            "\n",
            " 67% 134/200 [00:00<00:00, 549.41it/s]\u001b[A\u001b[A\n",
            "\n",
            "100% 200/200 [00:00<00:00, 491.17it/s]\n",
            "\n",
            "\n",
            "  0% 0/200 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "100% 200/200 [00:00<00:00, 1519.79it/s]\n",
            "2024-04-21 22:47:45 train_table2text_t5 - get_dataloader: loading dev dataloader...\n",
            "2024-04-21 22:47:45 train_table2text_t5 - get_dataloader: done\n",
            "2024-04-21 22:47:45 lightning_base - on_validation_end: ***** Validation results *****\n",
            "2024-04-21 22:47:45 lightning_base - on_validation_end: avg_val_loss = tensor(1.3542, device='cuda:0')\n",
            "2024-04-21 22:47:45 lightning_base - on_validation_end: epoch = 13\n",
            "2024-04-21 22:47:45 lightning_base - on_validation_end: loss = tensor(1.7522, device='cuda:0')\n",
            "2024-04-21 22:47:45 lightning_base - on_validation_end: step_count = 15\n",
            "2024-04-21 22:47:45 lightning_base - on_validation_end: train_loss = tensor(1.7522, device='cuda:0')\n",
            "2024-04-21 22:47:45 lightning_base - on_validation_end: val_avg_bleu = 10.745207065524582\n",
            "2024-04-21 22:47:45 lightning_base - on_validation_end: val_mover = tensor(0.2100, device='cuda:0')\n",
            "2024-04-21 22:47:45 lightning_base - on_validation_end: val_mover_mean1 = 0.21\n",
            "2024-04-21 22:47:45 lightning_base - on_validation_end: val_mover_median1 = 0.203\n",
            "Epoch 14: 100% 75/75 [02:36<00:00,  2.09s/it, loss=1.986, lr=2.9e-5]\n",
            "                                               \u001b[A\n",
            "  0% 0/200 [00:00<?, ?it/s]\u001b[A\n",
            " 19% 38/200 [00:00<00:00, 372.78it/s]\u001b[A\n",
            " 40% 79/200 [00:00<00:00, 393.26it/s]\u001b[A\n",
            " 60% 119/200 [00:00<00:00, 291.60it/s]\u001b[A\n",
            "100% 200/200 [00:00<00:00, 260.74it/s]\n",
            "\n",
            "  0% 0/200 [00:00<?, ?it/s]\u001b[A\n",
            "100% 200/200 [00:00<00:00, 1810.15it/s]\n",
            "2024-04-21 22:48:03 train_table2text_t5 - get_dataloader: loading train dataloader...\n",
            "2024-04-21 22:48:03 train_table2text_t5 - get_dataloader: done\n",
            "Epoch 15:  33% 25/75 [00:22<00:44,  1.13it/s, loss=1.863, lr=2.9e-5]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 15:  35% 26/75 [00:25<00:47,  1.04it/s, loss=1.863, lr=2.9e-5]\n",
            "Epoch 15:  36% 27/75 [00:27<00:48,  1.00s/it, loss=1.863, lr=2.9e-5]\n",
            "Epoch 15:  37% 28/75 [00:28<00:47,  1.02s/it, loss=1.863, lr=2.9e-5]\n",
            "Epoch 15:  39% 29/75 [00:31<00:49,  1.08s/it, loss=1.863, lr=2.9e-5]\n",
            "Epoch 15:  40% 30/75 [00:33<00:49,  1.11s/it, loss=1.863, lr=2.9e-5]\n",
            "Epoch 15:  41% 31/75 [00:35<00:49,  1.13s/it, loss=1.863, lr=2.9e-5]\n",
            "Epoch 15:  43% 32/75 [00:37<00:49,  1.16s/it, loss=1.863, lr=2.9e-5]\n",
            "Epoch 15:  44% 33/75 [00:38<00:48,  1.16s/it, loss=1.863, lr=2.9e-5]\n",
            "Epoch 15:  45% 34/75 [00:39<00:47,  1.16s/it, loss=1.863, lr=2.9e-5]\n",
            "Epoch 15:  47% 35/75 [00:40<00:46,  1.17s/it, loss=1.863, lr=2.9e-5]\n",
            "Epoch 15:  48% 36/75 [00:42<00:46,  1.18s/it, loss=1.863, lr=2.9e-5]\n",
            "Epoch 15:  49% 37/75 [00:43<00:45,  1.18s/it, loss=1.863, lr=2.9e-5]\n",
            "Epoch 15:  51% 38/75 [00:45<00:44,  1.20s/it, loss=1.863, lr=2.9e-5]\n",
            "Epoch 15:  52% 39/75 [00:47<00:44,  1.22s/it, loss=1.863, lr=2.9e-5]\n",
            "Epoch 15:  53% 40/75 [00:50<00:43,  1.25s/it, loss=1.863, lr=2.9e-5]\n",
            "Epoch 15:  55% 41/75 [00:52<00:43,  1.28s/it, loss=1.863, lr=2.9e-5]\n",
            "Epoch 15:  56% 42/75 [00:53<00:41,  1.27s/it, loss=1.863, lr=2.9e-5]\n",
            "Epoch 15:  57% 43/75 [00:54<00:40,  1.27s/it, loss=1.863, lr=2.9e-5]\n",
            "Epoch 15:  59% 44/75 [00:56<00:39,  1.28s/it, loss=1.863, lr=2.9e-5]\n",
            "Epoch 15:  60% 45/75 [00:58<00:38,  1.29s/it, loss=1.863, lr=2.9e-5]\n",
            "Epoch 15:  61% 46/75 [00:59<00:37,  1.30s/it, loss=1.863, lr=2.9e-5]\n",
            "Epoch 15:  63% 47/75 [01:01<00:36,  1.31s/it, loss=1.863, lr=2.9e-5]\n",
            "Epoch 15:  64% 48/75 [01:03<00:35,  1.32s/it, loss=1.863, lr=2.9e-5]\n",
            "Epoch 15:  65% 49/75 [01:06<00:35,  1.35s/it, loss=1.863, lr=2.9e-5]\n",
            "Epoch 15:  67% 50/75 [01:10<00:35,  1.40s/it, loss=1.863, lr=2.9e-5]\n",
            "Epoch 15:  68% 51/75 [01:11<00:33,  1.41s/it, loss=1.863, lr=2.9e-5]\n",
            "Epoch 15:  69% 52/75 [01:13<00:32,  1.42s/it, loss=1.863, lr=2.9e-5]\n",
            "Epoch 15:  71% 53/75 [01:15<00:31,  1.43s/it, loss=1.863, lr=2.9e-5]\n",
            "Epoch 15:  72% 54/75 [01:17<00:30,  1.44s/it, loss=1.863, lr=2.9e-5]\n",
            "Epoch 15:  73% 55/75 [01:19<00:28,  1.44s/it, loss=1.863, lr=2.9e-5]\n",
            "Epoch 15:  75% 56/75 [01:21<00:27,  1.45s/it, loss=1.863, lr=2.9e-5]\n",
            "Epoch 15:  76% 57/75 [01:22<00:26,  1.45s/it, loss=1.863, lr=2.9e-5]\n",
            "Epoch 15:  77% 58/75 [01:24<00:24,  1.45s/it, loss=1.863, lr=2.9e-5]\n",
            "Epoch 15:  79% 59/75 [01:25<00:23,  1.45s/it, loss=1.863, lr=2.9e-5]\n",
            "Epoch 15:  80% 60/75 [01:27<00:21,  1.46s/it, loss=1.863, lr=2.9e-5]\n",
            "Epoch 15:  81% 61/75 [01:29<00:20,  1.46s/it, loss=1.863, lr=2.9e-5]\n",
            "Epoch 15:  83% 62/75 [01:30<00:19,  1.46s/it, loss=1.863, lr=2.9e-5]\n",
            "Epoch 15:  84% 63/75 [01:32<00:17,  1.47s/it, loss=1.863, lr=2.9e-5]\n",
            "Epoch 15:  85% 64/75 [01:34<00:16,  1.48s/it, loss=1.863, lr=2.9e-5]\n",
            "Epoch 15:  87% 65/75 [01:35<00:14,  1.48s/it, loss=1.863, lr=2.9e-5]\n",
            "Epoch 15:  88% 66/75 [01:39<00:13,  1.51s/it, loss=1.863, lr=2.9e-5]\n",
            "Epoch 15:  89% 67/75 [01:40<00:12,  1.51s/it, loss=1.863, lr=2.9e-5]\n",
            "Epoch 15:  91% 68/75 [01:42<00:10,  1.50s/it, loss=1.863, lr=2.9e-5]\n",
            "Epoch 15:  92% 69/75 [01:44<00:09,  1.51s/it, loss=1.863, lr=2.9e-5]\n",
            "Epoch 15:  93% 70/75 [01:47<00:07,  1.53s/it, loss=1.863, lr=2.9e-5]\n",
            "Epoch 15:  95% 71/75 [01:48<00:06,  1.53s/it, loss=1.863, lr=2.9e-5]\n",
            "Epoch 15:  96% 72/75 [01:50<00:04,  1.53s/it, loss=1.863, lr=2.9e-5]\n",
            "Epoch 15:  97% 73/75 [01:52<00:03,  1.54s/it, loss=1.863, lr=2.9e-5]\n",
            "Epoch 15:  99% 74/75 [01:53<00:01,  1.53s/it, loss=1.863, lr=2.9e-5]\n",
            "Epoch 15: 100% 75/75 [01:54<00:00,  1.53s/it, loss=1.863, lr=2.9e-5]2024-04-21 22:49:58 bleu - corpus_score: That's 100 lines that end in a tokenized period ('.')\n",
            "2024-04-21 22:49:58 bleu - corpus_score: It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2024-04-21 22:49:58 bleu - corpus_score: If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2024-04-21 22:50:24 train_table2text_t5 - validation_epoch_end: valid epoch: 15\n",
            "2024-04-21 22:50:24 train_table2text_t5 - validation_epoch_end: 15 bleu_info: 9.386896450101048\n",
            "2024-04-21 22:50:24 train_table2text_t5 - validation_epoch_end: 15 mover score: (0.219, 0.207)\n",
            "\n",
            "\n",
            "  0% 0/200 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            " 35% 70/200 [00:00<00:00, 693.44it/s]\u001b[A\u001b[A\n",
            "\n",
            " 70% 140/200 [00:00<00:00, 570.24it/s]\u001b[A\u001b[A\n",
            "\n",
            "100% 200/200 [00:00<00:00, 516.82it/s]\n",
            "\n",
            "\n",
            "  0% 0/200 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "100% 200/200 [00:00<00:00, 1625.36it/s]\n",
            "2024-04-21 22:50:25 train_table2text_t5 - get_dataloader: loading dev dataloader...\n",
            "2024-04-21 22:50:25 train_table2text_t5 - get_dataloader: done\n",
            "2024-04-21 22:50:25 lightning_base - on_validation_end: ***** Validation results *****\n",
            "2024-04-21 22:50:25 lightning_base - on_validation_end: avg_val_loss = tensor(1.2382, device='cuda:0')\n",
            "2024-04-21 22:50:25 lightning_base - on_validation_end: epoch = 14\n",
            "2024-04-21 22:50:25 lightning_base - on_validation_end: loss = tensor(2.0401, device='cuda:0')\n",
            "2024-04-21 22:50:25 lightning_base - on_validation_end: step_count = 16\n",
            "2024-04-21 22:50:25 lightning_base - on_validation_end: train_loss = tensor(2.0401, device='cuda:0')\n",
            "2024-04-21 22:50:25 lightning_base - on_validation_end: val_avg_bleu = 9.386896450101048\n",
            "2024-04-21 22:50:25 lightning_base - on_validation_end: val_mover = tensor(0.2190, device='cuda:0')\n",
            "2024-04-21 22:50:25 lightning_base - on_validation_end: val_mover_mean1 = 0.219\n",
            "2024-04-21 22:50:25 lightning_base - on_validation_end: val_mover_median1 = 0.207\n",
            "Epoch 15: 100% 75/75 [02:21<00:00,  1.89s/it, loss=1.863, lr=2.9e-5]\n",
            "                                               \u001b[A\n",
            "  0% 0/200 [00:00<?, ?it/s]\u001b[A\n",
            " 35% 70/200 [00:00<00:00, 694.92it/s]\u001b[A\n",
            " 70% 140/200 [00:00<00:00, 578.67it/s]\u001b[A\n",
            "100% 200/200 [00:00<00:00, 519.80it/s]\n",
            "\n",
            "  0% 0/200 [00:00<?, ?it/s]\u001b[A\n",
            "100% 200/200 [00:00<00:00, 1934.65it/s]\n",
            "2024-04-21 22:50:48 train_table2text_t5 - get_dataloader: loading train dataloader...\n",
            "2024-04-21 22:50:48 train_table2text_t5 - get_dataloader: done\n",
            "Epoch 16:  33% 25/75 [00:22<00:44,  1.12it/s, loss=1.712, lr=2.9e-5]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 16:  35% 26/75 [00:24<00:46,  1.05it/s, loss=1.712, lr=2.9e-5]\n",
            "Epoch 16:  36% 27/75 [00:27<00:48,  1.01s/it, loss=1.712, lr=2.9e-5]\n",
            "Epoch 16:  37% 28/75 [00:28<00:48,  1.03s/it, loss=1.712, lr=2.9e-5]\n",
            "Epoch 16:  39% 29/75 [00:31<00:49,  1.09s/it, loss=1.712, lr=2.9e-5]\n",
            "Epoch 16:  40% 30/75 [00:33<00:50,  1.12s/it, loss=1.712, lr=2.9e-5]\n",
            "Epoch 16:  41% 31/75 [00:35<00:50,  1.14s/it, loss=1.712, lr=2.9e-5]\n",
            "Epoch 16:  43% 32/75 [00:36<00:49,  1.15s/it, loss=1.712, lr=2.9e-5]\n",
            "Epoch 16:  44% 33/75 [00:38<00:48,  1.16s/it, loss=1.712, lr=2.9e-5]\n",
            "Epoch 16:  45% 34/75 [00:39<00:48,  1.17s/it, loss=1.712, lr=2.9e-5]\n",
            "Epoch 16:  47% 35/75 [00:41<00:47,  1.19s/it, loss=1.712, lr=2.9e-5]\n",
            "Epoch 16:  48% 36/75 [00:44<00:48,  1.24s/it, loss=1.712, lr=2.9e-5]\n",
            "Epoch 16:  49% 37/75 [00:46<00:47,  1.25s/it, loss=1.712, lr=2.9e-5]\n",
            "Epoch 16:  51% 38/75 [00:47<00:46,  1.26s/it, loss=1.712, lr=2.9e-5]\n",
            "Epoch 16:  52% 39/75 [00:49<00:45,  1.27s/it, loss=1.712, lr=2.9e-5]\n",
            "Epoch 16:  53% 40/75 [00:51<00:44,  1.28s/it, loss=1.712, lr=2.9e-5]\n",
            "Epoch 16:  55% 41/75 [00:52<00:43,  1.29s/it, loss=1.712, lr=2.9e-5]\n",
            "Epoch 16:  56% 42/75 [00:54<00:43,  1.31s/it, loss=1.712, lr=2.9e-5]\n",
            "Epoch 16:  57% 43/75 [00:56<00:42,  1.32s/it, loss=1.712, lr=2.9e-5]\n",
            "Epoch 16:  59% 44/75 [00:58<00:41,  1.33s/it, loss=1.712, lr=2.9e-5]\n",
            "Epoch 16:  60% 45/75 [01:00<00:40,  1.34s/it, loss=1.712, lr=2.9e-5]\n",
            "Epoch 16:  61% 46/75 [01:01<00:38,  1.34s/it, loss=1.712, lr=2.9e-5]\n",
            "Epoch 16:  63% 47/75 [01:03<00:37,  1.35s/it, loss=1.712, lr=2.9e-5]\n",
            "Epoch 16:  64% 48/75 [01:04<00:36,  1.35s/it, loss=1.712, lr=2.9e-5]\n",
            "Epoch 16:  65% 49/75 [01:10<00:37,  1.43s/it, loss=1.712, lr=2.9e-5]\n",
            "Epoch 16:  67% 50/75 [01:13<00:36,  1.46s/it, loss=1.712, lr=2.9e-5]\n",
            "Epoch 16:  68% 51/75 [01:14<00:35,  1.47s/it, loss=1.712, lr=2.9e-5]\n",
            "Epoch 16:  69% 52/75 [01:16<00:33,  1.47s/it, loss=1.712, lr=2.9e-5]\n",
            "Epoch 16:  71% 53/75 [01:19<00:33,  1.51s/it, loss=1.712, lr=2.9e-5]\n",
            "Epoch 16:  72% 54/75 [01:22<00:32,  1.53s/it, loss=1.712, lr=2.9e-5]\n",
            "Epoch 16:  73% 55/75 [01:24<00:30,  1.53s/it, loss=1.712, lr=2.9e-5]\n",
            "Epoch 16:  75% 56/75 [01:26<00:29,  1.54s/it, loss=1.712, lr=2.9e-5]\n",
            "Epoch 16:  76% 57/75 [01:27<00:27,  1.54s/it, loss=1.712, lr=2.9e-5]\n",
            "Epoch 16:  77% 58/75 [01:28<00:26,  1.53s/it, loss=1.712, lr=2.9e-5]\n",
            "Epoch 16:  79% 59/75 [01:30<00:24,  1.53s/it, loss=1.712, lr=2.9e-5]\n",
            "Epoch 16:  80% 60/75 [01:32<00:23,  1.54s/it, loss=1.712, lr=2.9e-5]\n",
            "Epoch 16:  81% 61/75 [01:34<00:21,  1.55s/it, loss=1.712, lr=2.9e-5]\n",
            "Epoch 16:  83% 62/75 [01:36<00:20,  1.55s/it, loss=1.712, lr=2.9e-5]\n",
            "Epoch 16:  84% 63/75 [01:38<00:18,  1.56s/it, loss=1.712, lr=2.9e-5]\n",
            "Epoch 16:  85% 64/75 [01:40<00:17,  1.57s/it, loss=1.712, lr=2.9e-5]\n",
            "Epoch 16:  87% 65/75 [01:41<00:15,  1.57s/it, loss=1.712, lr=2.9e-5]\n",
            "Epoch 16:  88% 66/75 [01:43<00:14,  1.57s/it, loss=1.712, lr=2.9e-5]\n",
            "Epoch 16:  89% 67/75 [01:45<00:12,  1.58s/it, loss=1.712, lr=2.9e-5]\n",
            "Epoch 16:  91% 68/75 [01:47<00:11,  1.58s/it, loss=1.712, lr=2.9e-5]\n",
            "Epoch 16:  92% 69/75 [01:49<00:09,  1.59s/it, loss=1.712, lr=2.9e-5]\n",
            "Epoch 16:  93% 70/75 [01:52<00:08,  1.61s/it, loss=1.712, lr=2.9e-5]\n",
            "Epoch 16:  95% 71/75 [01:54<00:06,  1.61s/it, loss=1.712, lr=2.9e-5]\n",
            "Epoch 16:  96% 72/75 [01:56<00:04,  1.61s/it, loss=1.712, lr=2.9e-5]\n",
            "Epoch 16:  97% 73/75 [01:57<00:03,  1.62s/it, loss=1.712, lr=2.9e-5]\n",
            "Epoch 16:  99% 74/75 [01:59<00:01,  1.62s/it, loss=1.712, lr=2.9e-5]\n",
            "Epoch 16: 100% 75/75 [02:01<00:00,  1.62s/it, loss=1.712, lr=2.9e-5]2024-04-21 22:52:51 bleu - corpus_score: That's 100 lines that end in a tokenized period ('.')\n",
            "2024-04-21 22:52:51 bleu - corpus_score: It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2024-04-21 22:52:51 bleu - corpus_score: If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2024-04-21 22:53:19 train_table2text_t5 - validation_epoch_end: valid epoch: 16\n",
            "2024-04-21 22:53:19 train_table2text_t5 - validation_epoch_end: 16 bleu_info: 12.406766026611642\n",
            "2024-04-21 22:53:19 train_table2text_t5 - validation_epoch_end: 16 mover score: (0.236, 0.228)\n",
            "\n",
            "\n",
            "  0% 0/200 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            " 38% 75/200 [00:00<00:00, 743.14it/s]\u001b[A\u001b[A\n",
            "\n",
            "100% 200/200 [00:00<00:00, 537.69it/s]\n",
            "\n",
            "\n",
            "  0% 0/200 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "100% 200/200 [00:00<00:00, 1881.51it/s]\n",
            "2024-04-21 22:53:20 train_table2text_t5 - get_dataloader: loading dev dataloader...\n",
            "2024-04-21 22:53:20 train_table2text_t5 - get_dataloader: done\n",
            "2024-04-21 22:53:20 lightning_base - on_validation_end: ***** Validation results *****\n",
            "2024-04-21 22:53:20 lightning_base - on_validation_end: avg_val_loss = tensor(1.1318, device='cuda:0')\n",
            "2024-04-21 22:53:20 lightning_base - on_validation_end: epoch = 15\n",
            "2024-04-21 22:53:20 lightning_base - on_validation_end: loss = tensor(1.5335, device='cuda:0')\n",
            "2024-04-21 22:53:20 lightning_base - on_validation_end: step_count = 17\n",
            "2024-04-21 22:53:20 lightning_base - on_validation_end: train_loss = tensor(1.5335, device='cuda:0')\n",
            "2024-04-21 22:53:20 lightning_base - on_validation_end: val_avg_bleu = 12.406766026611642\n",
            "2024-04-21 22:53:20 lightning_base - on_validation_end: val_mover = tensor(0.2360, device='cuda:0')\n",
            "2024-04-21 22:53:20 lightning_base - on_validation_end: val_mover_mean1 = 0.236\n",
            "2024-04-21 22:53:20 lightning_base - on_validation_end: val_mover_median1 = 0.228\n",
            "Epoch 16: 100% 75/75 [02:31<00:00,  2.02s/it, loss=1.712, lr=2.9e-5]\n",
            "                                               \u001b[A\n",
            "  0% 0/200 [00:00<?, ?it/s]\u001b[A\n",
            " 34% 68/200 [00:00<00:00, 661.05it/s]\u001b[A\n",
            " 68% 135/200 [00:00<00:00, 575.28it/s]\u001b[A\n",
            "100% 200/200 [00:00<00:00, 506.95it/s]\n",
            "\n",
            "  0% 0/200 [00:00<?, ?it/s]\u001b[A\n",
            "100% 200/200 [00:00<00:00, 1767.28it/s]\n",
            "2024-04-21 22:53:32 train_table2text_t5 - get_dataloader: loading train dataloader...\n",
            "2024-04-21 22:53:32 train_table2text_t5 - get_dataloader: done\n",
            "Epoch 17:  33% 25/75 [00:21<00:43,  1.14it/s, loss=1.649, lr=2.9e-5]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 17:  35% 26/75 [00:24<00:45,  1.07it/s, loss=1.649, lr=2.9e-5]\n",
            "Epoch 17:  36% 27/75 [00:27<00:48,  1.01s/it, loss=1.649, lr=2.9e-5]\n",
            "Epoch 17:  37% 28/75 [00:29<00:49,  1.05s/it, loss=1.649, lr=2.9e-5]\n",
            "Epoch 17:  39% 29/75 [00:31<00:50,  1.09s/it, loss=1.649, lr=2.9e-5]\n",
            "Epoch 17:  40% 30/75 [00:34<00:51,  1.15s/it, loss=1.649, lr=2.9e-5]\n",
            "Epoch 17:  41% 31/75 [00:35<00:50,  1.16s/it, loss=1.649, lr=2.9e-5]\n",
            "Epoch 17:  43% 32/75 [00:37<00:49,  1.16s/it, loss=1.649, lr=2.9e-5]\n",
            "Epoch 17:  44% 33/75 [00:38<00:48,  1.16s/it, loss=1.649, lr=2.9e-5]\n",
            "Epoch 17:  45% 34/75 [00:39<00:48,  1.18s/it, loss=1.649, lr=2.9e-5]\n",
            "Epoch 17:  47% 35/75 [00:41<00:47,  1.20s/it, loss=1.649, lr=2.9e-5]\n",
            "Epoch 17:  48% 36/75 [00:43<00:47,  1.21s/it, loss=1.649, lr=2.9e-5]\n",
            "Epoch 17:  49% 37/75 [00:45<00:46,  1.23s/it, loss=1.649, lr=2.9e-5]\n",
            "Epoch 17:  51% 38/75 [00:47<00:46,  1.25s/it, loss=1.649, lr=2.9e-5]\n",
            "Epoch 17:  52% 39/75 [00:49<00:45,  1.27s/it, loss=1.649, lr=2.9e-5]\n",
            "Epoch 17:  53% 40/75 [00:51<00:44,  1.28s/it, loss=1.649, lr=2.9e-5]\n",
            "Epoch 17:  55% 41/75 [00:54<00:45,  1.32s/it, loss=1.649, lr=2.9e-5]\n",
            "Epoch 17:  56% 42/75 [00:55<00:43,  1.32s/it, loss=1.649, lr=2.9e-5]\n",
            "Epoch 17:  57% 43/75 [00:56<00:42,  1.32s/it, loss=1.649, lr=2.9e-5]\n",
            "Epoch 17:  59% 44/75 [00:58<00:41,  1.33s/it, loss=1.649, lr=2.9e-5]\n",
            "Epoch 17:  60% 45/75 [01:00<00:40,  1.34s/it, loss=1.649, lr=2.9e-5]\n",
            "Epoch 17:  61% 46/75 [01:01<00:38,  1.34s/it, loss=1.649, lr=2.9e-5]\n",
            "Epoch 17:  63% 47/75 [01:04<00:38,  1.37s/it, loss=1.649, lr=2.9e-5]\n",
            "Epoch 17:  64% 48/75 [01:06<00:37,  1.38s/it, loss=1.649, lr=2.9e-5]\n",
            "Epoch 17:  65% 49/75 [01:10<00:37,  1.45s/it, loss=1.649, lr=2.9e-5]\n",
            "Epoch 17:  67% 50/75 [01:14<00:37,  1.49s/it, loss=1.649, lr=2.9e-5]\n",
            "Epoch 17:  68% 51/75 [01:16<00:35,  1.50s/it, loss=1.649, lr=2.9e-5]\n",
            "Epoch 17:  69% 52/75 [01:18<00:34,  1.50s/it, loss=1.649, lr=2.9e-5]\n",
            "Epoch 17:  71% 53/75 [01:22<00:34,  1.56s/it, loss=1.649, lr=2.9e-5]\n",
            "Epoch 17:  72% 54/75 [01:25<00:33,  1.59s/it, loss=1.649, lr=2.9e-5]\n",
            "Epoch 17:  73% 55/75 [01:27<00:31,  1.59s/it, loss=1.649, lr=2.9e-5]\n",
            "Epoch 17:  75% 56/75 [01:31<00:31,  1.63s/it, loss=1.649, lr=2.9e-5]\n",
            "Epoch 17:  76% 57/75 [01:32<00:29,  1.63s/it, loss=1.649, lr=2.9e-5]\n",
            "Epoch 17:  77% 58/75 [01:34<00:27,  1.63s/it, loss=1.649, lr=2.9e-5]\n",
            "Epoch 17:  79% 59/75 [01:35<00:25,  1.62s/it, loss=1.649, lr=2.9e-5]\n",
            "Epoch 17:  80% 60/75 [01:37<00:24,  1.62s/it, loss=1.649, lr=2.9e-5]\n",
            "Epoch 17:  81% 61/75 [01:38<00:22,  1.62s/it, loss=1.649, lr=2.9e-5]\n",
            "Epoch 17:  83% 62/75 [01:40<00:21,  1.62s/it, loss=1.649, lr=2.9e-5]\n",
            "Epoch 17:  84% 63/75 [01:43<00:19,  1.64s/it, loss=1.649, lr=2.9e-5]\n",
            "Epoch 17:  85% 64/75 [01:45<00:18,  1.64s/it, loss=1.649, lr=2.9e-5]\n",
            "Epoch 17:  87% 65/75 [01:46<00:16,  1.64s/it, loss=1.649, lr=2.9e-5]\n",
            "Epoch 17:  88% 66/75 [01:48<00:14,  1.65s/it, loss=1.649, lr=2.9e-5]\n",
            "Epoch 17:  89% 67/75 [01:50<00:13,  1.65s/it, loss=1.649, lr=2.9e-5]\n",
            "Epoch 17:  91% 68/75 [01:51<00:11,  1.65s/it, loss=1.649, lr=2.9e-5]\n",
            "Epoch 17:  92% 69/75 [01:55<00:10,  1.67s/it, loss=1.649, lr=2.9e-5]\n",
            "Epoch 17:  93% 70/75 [01:58<00:08,  1.69s/it, loss=1.649, lr=2.9e-5]\n",
            "Epoch 17:  95% 71/75 [02:00<00:06,  1.70s/it, loss=1.649, lr=2.9e-5]\n",
            "Epoch 17:  96% 72/75 [02:01<00:05,  1.69s/it, loss=1.649, lr=2.9e-5]\n",
            "Epoch 17:  97% 73/75 [02:03<00:03,  1.69s/it, loss=1.649, lr=2.9e-5]\n",
            "Epoch 17:  99% 74/75 [02:04<00:01,  1.69s/it, loss=1.649, lr=2.9e-5]\n",
            "Epoch 17: 100% 75/75 [02:06<00:00,  1.68s/it, loss=1.649, lr=2.9e-5]2024-04-21 22:55:40 bleu - corpus_score: That's 100 lines that end in a tokenized period ('.')\n",
            "2024-04-21 22:55:40 bleu - corpus_score: It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2024-04-21 22:55:40 bleu - corpus_score: If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2024-04-21 22:56:10 train_table2text_t5 - validation_epoch_end: valid epoch: 17\n",
            "2024-04-21 22:56:10 train_table2text_t5 - validation_epoch_end: 17 bleu_info: 14.330747484637481\n",
            "2024-04-21 22:56:10 train_table2text_t5 - validation_epoch_end: 17 mover score: (0.25, 0.241)\n",
            "\n",
            "\n",
            "  0% 0/200 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            " 34% 68/200 [00:00<00:00, 673.31it/s]\u001b[A\u001b[A\n",
            "\n",
            " 68% 136/200 [00:00<00:00, 602.88it/s]\u001b[A\u001b[A\n",
            "\n",
            "100% 200/200 [00:00<00:00, 515.07it/s]\n",
            "\n",
            "\n",
            "  0% 0/200 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "100% 200/200 [00:00<00:00, 1817.58it/s]\n",
            "2024-04-21 22:56:10 train_table2text_t5 - get_dataloader: loading dev dataloader...\n",
            "2024-04-21 22:56:10 train_table2text_t5 - get_dataloader: done\n",
            "2024-04-21 22:56:10 lightning_base - on_validation_end: ***** Validation results *****\n",
            "2024-04-21 22:56:10 lightning_base - on_validation_end: avg_val_loss = tensor(1.0304, device='cuda:0')\n",
            "2024-04-21 22:56:10 lightning_base - on_validation_end: epoch = 16\n",
            "2024-04-21 22:56:10 lightning_base - on_validation_end: loss = tensor(1.5627, device='cuda:0')\n",
            "2024-04-21 22:56:10 lightning_base - on_validation_end: step_count = 18\n",
            "2024-04-21 22:56:10 lightning_base - on_validation_end: train_loss = tensor(1.5627, device='cuda:0')\n",
            "2024-04-21 22:56:10 lightning_base - on_validation_end: val_avg_bleu = 14.330747484637481\n",
            "2024-04-21 22:56:10 lightning_base - on_validation_end: val_mover = tensor(0.2500, device='cuda:0')\n",
            "2024-04-21 22:56:10 lightning_base - on_validation_end: val_mover_mean1 = 0.25\n",
            "2024-04-21 22:56:10 lightning_base - on_validation_end: val_mover_median1 = 0.241\n",
            "Epoch 17: 100% 75/75 [02:38<00:00,  2.11s/it, loss=1.649, lr=2.9e-5]\n",
            "                                               \u001b[A\n",
            "  0% 0/200 [00:00<?, ?it/s]\u001b[A\n",
            " 34% 68/200 [00:00<00:00, 645.03it/s]\u001b[A\n",
            " 66% 133/200 [00:00<00:00, 510.60it/s]\u001b[A\n",
            "100% 200/200 [00:00<00:00, 490.04it/s]\n",
            "\n",
            "  0% 0/200 [00:00<?, ?it/s]\u001b[A\n",
            "100% 200/200 [00:00<00:00, 1901.56it/s]\n",
            "2024-04-21 22:56:27 train_table2text_t5 - get_dataloader: loading train dataloader...\n",
            "2024-04-21 22:56:27 train_table2text_t5 - get_dataloader: done\n",
            "Epoch 18:  33% 25/75 [00:21<00:43,  1.15it/s, loss=1.530, lr=2.9e-5]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 18:  35% 26/75 [00:24<00:46,  1.06it/s, loss=1.530, lr=2.9e-5]\n",
            "Epoch 18:  36% 27/75 [00:26<00:47,  1.01it/s, loss=1.530, lr=2.9e-5]\n",
            "Epoch 18:  37% 28/75 [00:27<00:46,  1.00it/s, loss=1.530, lr=2.9e-5]\n",
            "Epoch 18:  39% 29/75 [00:31<00:49,  1.07s/it, loss=1.530, lr=2.9e-5]\n",
            "Epoch 18:  40% 30/75 [00:33<00:50,  1.12s/it, loss=1.530, lr=2.9e-5]\n",
            "Epoch 18:  41% 31/75 [00:35<00:49,  1.14s/it, loss=1.530, lr=2.9e-5]\n",
            "Epoch 18:  43% 32/75 [00:37<00:50,  1.17s/it, loss=1.530, lr=2.9e-5]\n",
            "Epoch 18:  44% 33/75 [00:39<00:50,  1.20s/it, loss=1.530, lr=2.9e-5]\n",
            "Epoch 18:  45% 34/75 [00:41<00:49,  1.22s/it, loss=1.530, lr=2.9e-5]\n",
            "Epoch 18:  47% 35/75 [00:44<00:50,  1.26s/it, loss=1.530, lr=2.9e-5]\n",
            "Epoch 18:  48% 36/75 [00:47<00:50,  1.31s/it, loss=1.530, lr=2.9e-5]\n",
            "Epoch 18:  49% 37/75 [00:49<00:50,  1.33s/it, loss=1.530, lr=2.9e-5]\n",
            "Epoch 18:  51% 38/75 [00:51<00:49,  1.34s/it, loss=1.530, lr=2.9e-5]\n",
            "Epoch 18:  52% 39/75 [00:53<00:49,  1.36s/it, loss=1.530, lr=2.9e-5]\n",
            "Epoch 18:  53% 40/75 [00:55<00:48,  1.39s/it, loss=1.530, lr=2.9e-5]\n",
            "Epoch 18:  55% 41/75 [00:58<00:48,  1.43s/it, loss=1.530, lr=2.9e-5]\n",
            "Epoch 18:  56% 42/75 [01:00<00:47,  1.43s/it, loss=1.530, lr=2.9e-5]\n",
            "Epoch 18:  57% 43/75 [01:01<00:46,  1.44s/it, loss=1.530, lr=2.9e-5]\n",
            "Epoch 18:  59% 44/75 [01:03<00:45,  1.45s/it, loss=1.530, lr=2.9e-5]\n",
            "Epoch 18:  60% 45/75 [01:07<00:45,  1.50s/it, loss=1.530, lr=2.9e-5]\n",
            "Epoch 18:  61% 46/75 [01:09<00:43,  1.51s/it, loss=1.530, lr=2.9e-5]\n",
            "Epoch 18:  63% 47/75 [01:11<00:42,  1.53s/it, loss=1.530, lr=2.9e-5]\n",
            "Epoch 18:  64% 48/75 [01:13<00:41,  1.53s/it, loss=1.530, lr=2.9e-5]\n",
            "Epoch 18:  65% 49/75 [01:21<00:43,  1.66s/it, loss=1.530, lr=2.9e-5]\n",
            "Epoch 18:  67% 50/75 [01:27<00:43,  1.75s/it, loss=1.530, lr=2.9e-5]\n",
            "Epoch 18:  68% 51/75 [01:29<00:42,  1.75s/it, loss=1.530, lr=2.9e-5]\n",
            "Epoch 18:  69% 52/75 [01:30<00:40,  1.74s/it, loss=1.530, lr=2.9e-5]\n",
            "Epoch 18:  71% 53/75 [01:35<00:39,  1.80s/it, loss=1.530, lr=2.9e-5]\n",
            "Epoch 18:  72% 54/75 [01:40<00:38,  1.86s/it, loss=1.530, lr=2.9e-5]\n",
            "Epoch 18:  73% 55/75 [01:42<00:37,  1.86s/it, loss=1.530, lr=2.9e-5]\n",
            "Epoch 18:  75% 56/75 [01:45<00:35,  1.88s/it, loss=1.530, lr=2.9e-5]\n",
            "Epoch 18:  76% 57/75 [01:47<00:33,  1.88s/it, loss=1.530, lr=2.9e-5]\n",
            "Epoch 18:  77% 58/75 [01:49<00:31,  1.88s/it, loss=1.530, lr=2.9e-5]\n",
            "Epoch 18:  79% 59/75 [01:50<00:30,  1.88s/it, loss=1.530, lr=2.9e-5]\n",
            "Epoch 18:  80% 60/75 [01:53<00:28,  1.90s/it, loss=1.530, lr=2.9e-5]\n",
            "Epoch 18:  81% 61/75 [01:57<00:26,  1.92s/it, loss=1.530, lr=2.9e-5]\n",
            "Epoch 18:  83% 62/75 [01:59<00:25,  1.93s/it, loss=1.530, lr=2.9e-5]\n",
            "Epoch 18:  84% 63/75 [02:04<00:23,  1.97s/it, loss=1.530, lr=2.9e-5]\n",
            "Epoch 18:  85% 64/75 [02:07<00:21,  1.99s/it, loss=1.530, lr=2.9e-5]\n",
            "Epoch 18:  87% 65/75 [02:08<00:19,  1.98s/it, loss=1.530, lr=2.9e-5]\n",
            "Epoch 18:  88% 66/75 [02:11<00:17,  1.99s/it, loss=1.530, lr=2.9e-5]\n",
            "Epoch 18:  89% 67/75 [02:13<00:15,  2.00s/it, loss=1.530, lr=2.9e-5]\n",
            "Epoch 18:  91% 68/75 [02:15<00:13,  1.99s/it, loss=1.530, lr=2.9e-5]\n",
            "Epoch 18:  92% 69/75 [02:19<00:12,  2.01s/it, loss=1.530, lr=2.9e-5]\n",
            "Epoch 18:  93% 70/75 [02:21<00:10,  2.03s/it, loss=1.530, lr=2.9e-5]\n",
            "Epoch 18:  95% 71/75 [02:25<00:08,  2.04s/it, loss=1.530, lr=2.9e-5]\n",
            "Epoch 18:  96% 72/75 [02:26<00:06,  2.04s/it, loss=1.530, lr=2.9e-5]\n",
            "Epoch 18:  97% 73/75 [02:28<00:04,  2.04s/it, loss=1.530, lr=2.9e-5]\n",
            "Epoch 18:  99% 74/75 [02:30<00:02,  2.04s/it, loss=1.530, lr=2.9e-5]\n",
            "Epoch 18: 100% 75/75 [02:32<00:00,  2.03s/it, loss=1.530, lr=2.9e-5]2024-04-21 22:59:01 bleu - corpus_score: That's 100 lines that end in a tokenized period ('.')\n",
            "2024-04-21 22:59:01 bleu - corpus_score: It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2024-04-21 22:59:01 bleu - corpus_score: If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2024-04-21 22:59:42 train_table2text_t5 - validation_epoch_end: valid epoch: 18\n",
            "2024-04-21 22:59:42 train_table2text_t5 - validation_epoch_end: 18 bleu_info: 18.4061033815603\n",
            "2024-04-21 22:59:42 train_table2text_t5 - validation_epoch_end: 18 mover score: (0.27, 0.255)\n",
            "\n",
            "\n",
            "  0% 0/200 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            " 36% 73/200 [00:00<00:00, 724.44it/s]\u001b[A\u001b[A\n",
            "\n",
            "100% 200/200 [00:00<00:00, 528.15it/s]\n",
            "\n",
            "\n",
            "  0% 0/200 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "100% 200/200 [00:00<00:00, 1793.48it/s]\n",
            "2024-04-21 22:59:43 train_table2text_t5 - get_dataloader: loading dev dataloader...\n",
            "2024-04-21 22:59:43 train_table2text_t5 - get_dataloader: done\n",
            "2024-04-21 22:59:43 lightning_base - on_validation_end: ***** Validation results *****\n",
            "2024-04-21 22:59:43 lightning_base - on_validation_end: avg_val_loss = tensor(0.9183, device='cuda:0')\n",
            "2024-04-21 22:59:43 lightning_base - on_validation_end: epoch = 17\n",
            "2024-04-21 22:59:43 lightning_base - on_validation_end: loss = tensor(1.5033, device='cuda:0')\n",
            "2024-04-21 22:59:43 lightning_base - on_validation_end: step_count = 19\n",
            "2024-04-21 22:59:43 lightning_base - on_validation_end: train_loss = tensor(1.5033, device='cuda:0')\n",
            "2024-04-21 22:59:43 lightning_base - on_validation_end: val_avg_bleu = 18.4061033815603\n",
            "2024-04-21 22:59:43 lightning_base - on_validation_end: val_mover = tensor(0.2700, device='cuda:0')\n",
            "2024-04-21 22:59:43 lightning_base - on_validation_end: val_mover_mean1 = 0.27\n",
            "2024-04-21 22:59:43 lightning_base - on_validation_end: val_mover_median1 = 0.255\n",
            "Epoch 18: 100% 75/75 [03:15<00:00,  2.60s/it, loss=1.530, lr=2.9e-5]\n",
            "                                               \u001b[A\n",
            "  0% 0/200 [00:00<?, ?it/s]\u001b[A\n",
            " 34% 68/200 [00:00<00:00, 664.18it/s]\u001b[A\n",
            " 68% 135/200 [00:00<00:00, 565.70it/s]\u001b[A\n",
            "100% 200/200 [00:00<00:00, 503.74it/s]\n",
            "\n",
            "  0% 0/200 [00:00<?, ?it/s]\u001b[A\n",
            "100% 200/200 [00:00<00:00, 1837.75it/s]\n",
            "2024-04-21 22:59:50 train_table2text_t5 - get_dataloader: loading train dataloader...\n",
            "2024-04-21 22:59:50 train_table2text_t5 - get_dataloader: done\n",
            "Epoch 19:  33% 25/75 [00:21<00:43,  1.14it/s, loss=1.470, lr=2.9e-5]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 19:  35% 26/75 [00:24<00:45,  1.07it/s, loss=1.470, lr=2.9e-5]\n",
            "Epoch 19:  36% 27/75 [00:26<00:47,  1.01it/s, loss=1.470, lr=2.9e-5]\n",
            "Epoch 19:  37% 28/75 [00:28<00:47,  1.00s/it, loss=1.470, lr=2.9e-5]\n",
            "Epoch 19:  39% 29/75 [00:31<00:49,  1.09s/it, loss=1.470, lr=2.9e-5]\n",
            "Epoch 19:  40% 30/75 [00:33<00:50,  1.13s/it, loss=1.470, lr=2.9e-5]\n",
            "Epoch 19:  41% 31/75 [00:35<00:50,  1.15s/it, loss=1.470, lr=2.9e-5]\n",
            "Epoch 19:  43% 32/75 [00:36<00:49,  1.15s/it, loss=1.470, lr=2.9e-5]\n",
            "Epoch 19:  44% 33/75 [00:38<00:48,  1.15s/it, loss=1.470, lr=2.9e-5]\n",
            "Epoch 19:  45% 34/75 [00:40<00:48,  1.18s/it, loss=1.470, lr=2.9e-5]\n",
            "Epoch 19:  47% 35/75 [00:41<00:47,  1.19s/it, loss=1.470, lr=2.9e-5]\n",
            "Epoch 19:  48% 36/75 [00:43<00:47,  1.22s/it, loss=1.470, lr=2.9e-5]\n",
            "Epoch 19:  49% 37/75 [00:45<00:46,  1.24s/it, loss=1.470, lr=2.9e-5]\n",
            "Epoch 19:  51% 38/75 [00:47<00:46,  1.26s/it, loss=1.470, lr=2.9e-5]\n",
            "Epoch 19:  52% 39/75 [00:49<00:45,  1.27s/it, loss=1.470, lr=2.9e-5]\n",
            "Epoch 19:  53% 40/75 [00:51<00:45,  1.30s/it, loss=1.470, lr=2.9e-5]\n",
            "Epoch 19:  55% 41/75 [00:54<00:45,  1.34s/it, loss=1.470, lr=2.9e-5]\n",
            "Epoch 19:  56% 42/75 [00:56<00:44,  1.34s/it, loss=1.470, lr=2.9e-5]\n",
            "Epoch 19:  57% 43/75 [00:57<00:42,  1.34s/it, loss=1.470, lr=2.9e-5]\n",
            "Epoch 19:  59% 44/75 [00:59<00:41,  1.35s/it, loss=1.470, lr=2.9e-5]\n",
            "Epoch 19:  60% 45/75 [01:00<00:40,  1.35s/it, loss=1.470, lr=2.9e-5]\n",
            "Epoch 19:  61% 46/75 [01:02<00:39,  1.36s/it, loss=1.470, lr=2.9e-5]\n",
            "Epoch 19:  63% 47/75 [01:04<00:38,  1.37s/it, loss=1.470, lr=2.9e-5]\n",
            "Epoch 19:  64% 48/75 [01:06<00:37,  1.38s/it, loss=1.470, lr=2.9e-5]\n",
            "Epoch 19:  65% 49/75 [01:09<00:36,  1.41s/it, loss=1.470, lr=2.9e-5]\n",
            "Epoch 19:  67% 50/75 [01:12<00:36,  1.45s/it, loss=1.470, lr=2.9e-5]\n",
            "Epoch 19:  68% 51/75 [01:14<00:34,  1.46s/it, loss=1.470, lr=2.9e-5]\n",
            "Epoch 19:  69% 52/75 [01:15<00:33,  1.45s/it, loss=1.470, lr=2.9e-5]\n",
            "Epoch 19:  71% 53/75 [01:19<00:33,  1.51s/it, loss=1.470, lr=2.9e-5]\n",
            "Epoch 19:  72% 54/75 [01:21<00:31,  1.52s/it, loss=1.470, lr=2.9e-5]\n",
            "Epoch 19:  73% 55/75 [01:23<00:30,  1.53s/it, loss=1.470, lr=2.9e-5]\n",
            "Epoch 19:  75% 56/75 [01:26<00:29,  1.55s/it, loss=1.470, lr=2.9e-5]\n",
            "Epoch 19:  76% 57/75 [01:27<00:27,  1.54s/it, loss=1.470, lr=2.9e-5]\n",
            "Epoch 19:  77% 58/75 [01:30<00:26,  1.57s/it, loss=1.470, lr=2.9e-5]\n",
            "Epoch 19:  79% 59/75 [01:32<00:25,  1.56s/it, loss=1.470, lr=2.9e-5]\n",
            "Epoch 19:  80% 60/75 [01:34<00:23,  1.57s/it, loss=1.470, lr=2.9e-5]\n",
            "Epoch 19:  81% 61/75 [01:35<00:22,  1.57s/it, loss=1.470, lr=2.9e-5]\n",
            "Epoch 19:  83% 62/75 [01:37<00:20,  1.58s/it, loss=1.470, lr=2.9e-5]\n",
            "Epoch 19:  84% 63/75 [01:40<00:19,  1.59s/it, loss=1.470, lr=2.9e-5]\n",
            "Epoch 19:  85% 64/75 [01:42<00:17,  1.61s/it, loss=1.470, lr=2.9e-5]\n",
            "Epoch 19:  87% 65/75 [01:44<00:16,  1.60s/it, loss=1.470, lr=2.9e-5]\n",
            "Epoch 19:  88% 66/75 [01:45<00:14,  1.60s/it, loss=1.470, lr=2.9e-5]\n",
            "Epoch 19:  89% 67/75 [01:47<00:12,  1.60s/it, loss=1.470, lr=2.9e-5]\n",
            "Epoch 19:  91% 68/75 [01:48<00:11,  1.60s/it, loss=1.470, lr=2.9e-5]\n",
            "Epoch 19:  92% 69/75 [01:50<00:09,  1.61s/it, loss=1.470, lr=2.9e-5]\n",
            "Epoch 19:  93% 70/75 [01:53<00:08,  1.62s/it, loss=1.470, lr=2.9e-5]\n",
            "Epoch 19:  95% 71/75 [01:56<00:06,  1.64s/it, loss=1.470, lr=2.9e-5]\n",
            "Epoch 19:  96% 72/75 [01:57<00:04,  1.64s/it, loss=1.470, lr=2.9e-5]\n",
            "Epoch 19:  97% 73/75 [01:59<00:03,  1.64s/it, loss=1.470, lr=2.9e-5]\n",
            "Epoch 19:  99% 74/75 [02:01<00:01,  1.64s/it, loss=1.470, lr=2.9e-5]\n",
            "Epoch 19: 100% 75/75 [02:02<00:00,  1.64s/it, loss=1.470, lr=2.9e-5]2024-04-21 23:01:54 bleu - corpus_score: That's 100 lines that end in a tokenized period ('.')\n",
            "2024-04-21 23:01:54 bleu - corpus_score: It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2024-04-21 23:01:54 bleu - corpus_score: If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2024-04-21 23:02:22 train_table2text_t5 - validation_epoch_end: valid epoch: 19\n",
            "2024-04-21 23:02:22 train_table2text_t5 - validation_epoch_end: 19 bleu_info: 15.620646129033375\n",
            "2024-04-21 23:02:22 train_table2text_t5 - validation_epoch_end: 19 mover score: (0.273, 0.257)\n",
            "\n",
            "\n",
            "  0% 0/200 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            " 19% 38/200 [00:00<00:00, 372.58it/s]\u001b[A\u001b[A\n",
            "\n",
            " 42% 84/200 [00:00<00:00, 422.57it/s]\u001b[A\u001b[A\n",
            "\n",
            " 64% 127/200 [00:00<00:00, 306.37it/s]\u001b[A\u001b[A\n",
            "\n",
            " 80% 161/200 [00:00<00:00, 271.61it/s]\u001b[A\u001b[A\n",
            "\n",
            "100% 200/200 [00:00<00:00, 290.11it/s]\n",
            "\n",
            "\n",
            "  0% 0/200 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "100% 200/200 [00:00<00:00, 1645.94it/s]\n",
            "2024-04-21 23:02:23 train_table2text_t5 - get_dataloader: loading dev dataloader...\n",
            "2024-04-21 23:02:23 train_table2text_t5 - get_dataloader: done\n",
            "2024-04-21 23:02:23 lightning_base - on_validation_end: ***** Validation results *****\n",
            "2024-04-21 23:02:23 lightning_base - on_validation_end: avg_val_loss = tensor(0.8385, device='cuda:0')\n",
            "2024-04-21 23:02:23 lightning_base - on_validation_end: epoch = 18\n",
            "2024-04-21 23:02:23 lightning_base - on_validation_end: loss = tensor(1.3437, device='cuda:0')\n",
            "2024-04-21 23:02:23 lightning_base - on_validation_end: step_count = 20\n",
            "2024-04-21 23:02:23 lightning_base - on_validation_end: train_loss = tensor(1.3437, device='cuda:0')\n",
            "2024-04-21 23:02:23 lightning_base - on_validation_end: val_avg_bleu = 15.620646129033375\n",
            "2024-04-21 23:02:23 lightning_base - on_validation_end: val_mover = tensor(0.2730, device='cuda:0')\n",
            "2024-04-21 23:02:23 lightning_base - on_validation_end: val_mover_mean1 = 0.273\n",
            "2024-04-21 23:02:23 lightning_base - on_validation_end: val_mover_median1 = 0.257\n",
            "Epoch 19: 100% 75/75 [02:32<00:00,  2.03s/it, loss=1.470, lr=2.9e-5]\n",
            "                                               \u001b[A\n",
            "  0% 0/200 [00:00<?, ?it/s]\u001b[A\n",
            " 26% 51/200 [00:00<00:00, 507.78it/s]\u001b[A\n",
            " 52% 104/200 [00:00<00:00, 518.95it/s]\u001b[A\n",
            "100% 200/200 [00:00<00:00, 436.72it/s]\n",
            "\n",
            "  0% 0/200 [00:00<?, ?it/s]\u001b[A\n",
            "100% 200/200 [00:00<00:00, 1747.56it/s]\n",
            "2024-04-21 23:02:36 train_table2text_t5 - get_dataloader: loading train dataloader...\n",
            "2024-04-21 23:02:36 train_table2text_t5 - get_dataloader: done\n",
            "Epoch 20:  33% 25/75 [00:21<00:43,  1.14it/s, loss=1.367, lr=2.9e-5]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 20:  35% 26/75 [00:24<00:46,  1.04it/s, loss=1.367, lr=2.9e-5]\n",
            "Epoch 20:  36% 27/75 [00:27<00:48,  1.01s/it, loss=1.367, lr=2.9e-5]\n",
            "Epoch 20:  37% 28/75 [00:28<00:47,  1.01s/it, loss=1.367, lr=2.9e-5]\n",
            "Epoch 20:  39% 29/75 [00:30<00:48,  1.04s/it, loss=1.367, lr=2.9e-5]\n",
            "Epoch 20:  40% 30/75 [00:32<00:48,  1.09s/it, loss=1.367, lr=2.9e-5]\n",
            "Epoch 20:  41% 31/75 [00:34<00:49,  1.12s/it, loss=1.367, lr=2.9e-5]\n",
            "Epoch 20:  43% 32/75 [00:36<00:49,  1.14s/it, loss=1.367, lr=2.9e-5]\n",
            "Epoch 20:  44% 33/75 [00:37<00:48,  1.14s/it, loss=1.367, lr=2.9e-5]\n",
            "Epoch 20:  45% 34/75 [00:39<00:47,  1.15s/it, loss=1.367, lr=2.9e-5]\n",
            "Epoch 20:  47% 35/75 [00:40<00:46,  1.15s/it, loss=1.367, lr=2.9e-5]\n",
            "Epoch 20:  48% 36/75 [00:42<00:46,  1.19s/it, loss=1.367, lr=2.9e-5]\n",
            "Epoch 20:  49% 37/75 [00:45<00:46,  1.22s/it, loss=1.367, lr=2.9e-5]\n",
            "Epoch 20:  51% 38/75 [00:48<00:46,  1.27s/it, loss=1.367, lr=2.9e-5]\n",
            "Epoch 20:  52% 39/75 [00:50<00:46,  1.29s/it, loss=1.367, lr=2.9e-5]\n",
            "Epoch 20:  53% 40/75 [00:52<00:45,  1.31s/it, loss=1.367, lr=2.9e-5]\n",
            "Epoch 20:  55% 41/75 [00:54<00:45,  1.33s/it, loss=1.367, lr=2.9e-5]\n",
            "Epoch 20:  56% 42/75 [00:55<00:43,  1.33s/it, loss=1.367, lr=2.9e-5]\n",
            "Epoch 20:  57% 43/75 [00:57<00:42,  1.34s/it, loss=1.367, lr=2.9e-5]\n",
            "Epoch 20:  59% 44/75 [01:00<00:42,  1.37s/it, loss=1.367, lr=2.9e-5]\n",
            "Epoch 20:  60% 45/75 [01:01<00:41,  1.38s/it, loss=1.367, lr=2.9e-5]\n",
            "Epoch 20:  61% 46/75 [01:03<00:39,  1.38s/it, loss=1.367, lr=2.9e-5]\n",
            "Epoch 20:  63% 47/75 [01:04<00:38,  1.38s/it, loss=1.367, lr=2.9e-5]\n",
            "Epoch 20:  64% 48/75 [01:06<00:37,  1.38s/it, loss=1.367, lr=2.9e-5]\n",
            "Epoch 20:  65% 49/75 [01:13<00:38,  1.49s/it, loss=1.367, lr=2.9e-5]\n",
            "Epoch 20:  67% 50/75 [01:17<00:38,  1.55s/it, loss=1.367, lr=2.9e-5]\n",
            "Epoch 20:  68% 51/75 [01:19<00:37,  1.56s/it, loss=1.367, lr=2.9e-5]\n",
            "Epoch 20:  69% 52/75 [01:20<00:35,  1.55s/it, loss=1.367, lr=2.9e-5]\n",
            "Epoch 20:  71% 53/75 [01:23<00:34,  1.58s/it, loss=1.367, lr=2.9e-5]\n",
            "Epoch 20:  72% 54/75 [01:26<00:33,  1.60s/it, loss=1.367, lr=2.9e-5]\n",
            "Epoch 20:  73% 55/75 [01:28<00:32,  1.61s/it, loss=1.367, lr=2.9e-5]\n",
            "Epoch 20:  75% 56/75 [01:30<00:30,  1.61s/it, loss=1.367, lr=2.9e-5]\n",
            "Epoch 20:  76% 57/75 [01:31<00:28,  1.61s/it, loss=1.367, lr=2.9e-5]\n",
            "Epoch 20:  77% 58/75 [01:33<00:27,  1.61s/it, loss=1.367, lr=2.9e-5]\n",
            "Epoch 20:  79% 59/75 [01:34<00:25,  1.61s/it, loss=1.367, lr=2.9e-5]\n",
            "Epoch 20:  80% 60/75 [01:37<00:24,  1.62s/it, loss=1.367, lr=2.9e-5]\n",
            "Epoch 20:  81% 61/75 [01:39<00:22,  1.63s/it, loss=1.367, lr=2.9e-5]\n",
            "Epoch 20:  83% 62/75 [01:40<00:21,  1.62s/it, loss=1.367, lr=2.9e-5]\n",
            "Epoch 20:  84% 63/75 [01:43<00:19,  1.64s/it, loss=1.367, lr=2.9e-5]\n",
            "Epoch 20:  85% 64/75 [01:45<00:18,  1.65s/it, loss=1.367, lr=2.9e-5]\n",
            "Epoch 20:  87% 65/75 [01:47<00:16,  1.65s/it, loss=1.367, lr=2.9e-5]\n",
            "Epoch 20:  88% 66/75 [01:48<00:14,  1.64s/it, loss=1.367, lr=2.9e-5]\n",
            "Epoch 20:  89% 67/75 [01:49<00:13,  1.64s/it, loss=1.367, lr=2.9e-5]\n",
            "Epoch 20:  91% 68/75 [01:51<00:11,  1.64s/it, loss=1.367, lr=2.9e-5]\n",
            "Epoch 20:  92% 69/75 [01:54<00:09,  1.65s/it, loss=1.367, lr=2.9e-5]\n",
            "Epoch 20:  93% 70/75 [01:56<00:08,  1.66s/it, loss=1.367, lr=2.9e-5]\n",
            "Epoch 20:  95% 71/75 [01:58<00:06,  1.67s/it, loss=1.367, lr=2.9e-5]\n",
            "Epoch 20:  96% 72/75 [01:59<00:04,  1.66s/it, loss=1.367, lr=2.9e-5]\n",
            "Epoch 20:  97% 73/75 [02:00<00:03,  1.65s/it, loss=1.367, lr=2.9e-5]\n",
            "Epoch 20:  99% 74/75 [02:02<00:01,  1.66s/it, loss=1.367, lr=2.9e-5]\n",
            "Epoch 20: 100% 75/75 [02:04<00:00,  1.66s/it, loss=1.367, lr=2.9e-5]2024-04-21 23:04:41 bleu - corpus_score: That's 100 lines that end in a tokenized period ('.')\n",
            "2024-04-21 23:04:41 bleu - corpus_score: It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2024-04-21 23:04:41 bleu - corpus_score: If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2024-04-21 23:05:12 train_table2text_t5 - validation_epoch_end: valid epoch: 20\n",
            "2024-04-21 23:05:12 train_table2text_t5 - validation_epoch_end: 20 bleu_info: 18.007687923791643\n",
            "2024-04-21 23:05:12 train_table2text_t5 - validation_epoch_end: 20 mover score: (0.296, 0.267)\n",
            "\n",
            "\n",
            "  0% 0/200 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            " 38% 75/200 [00:00<00:00, 743.77it/s]\u001b[A\u001b[A\n",
            "\n",
            "100% 200/200 [00:00<00:00, 532.88it/s]\n",
            "\n",
            "\n",
            "  0% 0/200 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "100% 200/200 [00:00<00:00, 1890.22it/s]\n",
            "2024-04-21 23:05:12 train_table2text_t5 - get_dataloader: loading dev dataloader...\n",
            "2024-04-21 23:05:12 train_table2text_t5 - get_dataloader: done\n",
            "2024-04-21 23:05:12 lightning_base - on_validation_end: ***** Validation results *****\n",
            "2024-04-21 23:05:12 lightning_base - on_validation_end: avg_val_loss = tensor(0.7558, device='cuda:0')\n",
            "2024-04-21 23:05:12 lightning_base - on_validation_end: epoch = 19\n",
            "2024-04-21 23:05:12 lightning_base - on_validation_end: loss = tensor(1.2518, device='cuda:0')\n",
            "2024-04-21 23:05:12 lightning_base - on_validation_end: step_count = 21\n",
            "2024-04-21 23:05:12 lightning_base - on_validation_end: train_loss = tensor(1.2518, device='cuda:0')\n",
            "2024-04-21 23:05:12 lightning_base - on_validation_end: val_avg_bleu = 18.007687923791643\n",
            "2024-04-21 23:05:12 lightning_base - on_validation_end: val_mover = tensor(0.2960, device='cuda:0')\n",
            "2024-04-21 23:05:12 lightning_base - on_validation_end: val_mover_mean1 = 0.296\n",
            "2024-04-21 23:05:12 lightning_base - on_validation_end: val_mover_median1 = 0.267\n",
            "Epoch 20: 100% 75/75 [02:36<00:00,  2.09s/it, loss=1.367, lr=2.9e-5]\n",
            "                                               \u001b[A\n",
            "  0% 0/200 [00:00<?, ?it/s]\u001b[A\n",
            " 34% 68/200 [00:00<00:00, 663.19it/s]\u001b[A\n",
            " 68% 135/200 [00:00<00:00, 533.40it/s]\u001b[A\n",
            "100% 200/200 [00:00<00:00, 505.12it/s]\n",
            "\n",
            "  0% 0/200 [00:00<?, ?it/s]\u001b[A\n",
            "100% 200/200 [00:00<00:00, 1785.53it/s]\n",
            "2024-04-21 23:05:20 train_table2text_t5 - get_dataloader: loading train dataloader...\n",
            "2024-04-21 23:05:20 train_table2text_t5 - get_dataloader: done\n",
            "Epoch 21:  33% 25/75 [00:22<00:44,  1.12it/s, loss=1.278, lr=2.9e-5]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 21:  35% 26/75 [00:24<00:46,  1.05it/s, loss=1.278, lr=2.9e-5]\n",
            "Epoch 21:  36% 27/75 [00:26<00:46,  1.03it/s, loss=1.278, lr=2.9e-5]\n",
            "Epoch 21:  37% 28/75 [00:27<00:45,  1.03it/s, loss=1.278, lr=2.9e-5]\n",
            "Epoch 21:  39% 29/75 [00:29<00:47,  1.03s/it, loss=1.278, lr=2.9e-5]\n",
            "Epoch 21:  40% 30/75 [00:31<00:47,  1.05s/it, loss=1.278, lr=2.9e-5]\n",
            "Epoch 21:  41% 31/75 [00:33<00:47,  1.09s/it, loss=1.278, lr=2.9e-5]\n",
            "Epoch 21:  43% 32/75 [00:35<00:47,  1.10s/it, loss=1.278, lr=2.9e-5]\n",
            "Epoch 21:  44% 33/75 [00:36<00:46,  1.12s/it, loss=1.278, lr=2.9e-5]\n",
            "Epoch 21:  45% 34/75 [00:38<00:46,  1.13s/it, loss=1.278, lr=2.9e-5]\n",
            "Epoch 21:  47% 35/75 [00:39<00:45,  1.13s/it, loss=1.278, lr=2.9e-5]\n",
            "Epoch 21:  48% 36/75 [00:41<00:44,  1.15s/it, loss=1.278, lr=2.9e-5]\n",
            "Epoch 21:  49% 37/75 [00:42<00:44,  1.16s/it, loss=1.278, lr=2.9e-5]\n",
            "Epoch 21:  51% 38/75 [00:45<00:44,  1.20s/it, loss=1.278, lr=2.9e-5]\n",
            "Epoch 21:  52% 39/75 [00:47<00:43,  1.22s/it, loss=1.278, lr=2.9e-5]\n",
            "Epoch 21:  53% 40/75 [00:49<00:43,  1.23s/it, loss=1.278, lr=2.9e-5]\n",
            "Epoch 21:  55% 41/75 [00:51<00:42,  1.26s/it, loss=1.278, lr=2.9e-5]\n",
            "Epoch 21:  56% 42/75 [00:52<00:41,  1.25s/it, loss=1.278, lr=2.9e-5]\n",
            "Epoch 21:  57% 43/75 [00:54<00:40,  1.26s/it, loss=1.278, lr=2.9e-5]\n",
            "Epoch 21:  59% 44/75 [00:56<00:39,  1.29s/it, loss=1.278, lr=2.9e-5]\n",
            "Epoch 21:  60% 45/75 [00:58<00:39,  1.31s/it, loss=1.278, lr=2.9e-5]\n",
            "Epoch 21:  61% 46/75 [01:00<00:37,  1.31s/it, loss=1.278, lr=2.9e-5]\n",
            "Epoch 21:  63% 47/75 [01:01<00:36,  1.32s/it, loss=1.278, lr=2.9e-5]\n",
            "Epoch 21:  64% 48/75 [01:03<00:35,  1.32s/it, loss=1.278, lr=2.9e-5]\n",
            "Epoch 21:  65% 49/75 [01:06<00:35,  1.35s/it, loss=1.278, lr=2.9e-5]\n",
            "Epoch 21:  67% 50/75 [01:10<00:35,  1.41s/it, loss=1.278, lr=2.9e-5]\n",
            "Epoch 21:  68% 51/75 [01:11<00:33,  1.41s/it, loss=1.278, lr=2.9e-5]\n",
            "Epoch 21:  69% 52/75 [01:13<00:32,  1.40s/it, loss=1.278, lr=2.9e-5]\n",
            "Epoch 21:  71% 53/75 [01:15<00:31,  1.42s/it, loss=1.278, lr=2.9e-5]\n",
            "Epoch 21:  72% 54/75 [01:16<00:29,  1.42s/it, loss=1.278, lr=2.9e-5]\n",
            "Epoch 21:  73% 55/75 [01:18<00:28,  1.44s/it, loss=1.278, lr=2.9e-5]\n",
            "Epoch 21:  75% 56/75 [01:21<00:27,  1.45s/it, loss=1.278, lr=2.9e-5]\n",
            "Epoch 21:  76% 57/75 [01:22<00:25,  1.44s/it, loss=1.278, lr=2.9e-5]\n",
            "Epoch 21:  77% 58/75 [01:24<00:24,  1.45s/it, loss=1.278, lr=2.9e-5]\n",
            "Epoch 21:  79% 59/75 [01:25<00:23,  1.44s/it, loss=1.278, lr=2.9e-5]\n",
            "Epoch 21:  80% 60/75 [01:26<00:21,  1.45s/it, loss=1.278, lr=2.9e-5]\n",
            "Epoch 21:  81% 61/75 [01:28<00:20,  1.45s/it, loss=1.278, lr=2.9e-5]\n",
            "Epoch 21:  83% 62/75 [01:29<00:18,  1.45s/it, loss=1.278, lr=2.9e-5]\n",
            "Epoch 21:  84% 63/75 [01:32<00:17,  1.47s/it, loss=1.278, lr=2.9e-5]\n",
            "Epoch 21:  85% 64/75 [01:34<00:16,  1.48s/it, loss=1.278, lr=2.9e-5]\n",
            "Epoch 21:  87% 65/75 [01:36<00:14,  1.49s/it, loss=1.278, lr=2.9e-5]\n",
            "Epoch 21:  88% 66/75 [01:38<00:13,  1.49s/it, loss=1.278, lr=2.9e-5]\n",
            "Epoch 21:  89% 67/75 [01:39<00:11,  1.48s/it, loss=1.278, lr=2.9e-5]\n",
            "Epoch 21:  91% 68/75 [01:40<00:10,  1.48s/it, loss=1.278, lr=2.9e-5]\n",
            "Epoch 21:  92% 69/75 [01:42<00:08,  1.49s/it, loss=1.278, lr=2.9e-5]\n",
            "Epoch 21:  93% 70/75 [01:45<00:07,  1.51s/it, loss=1.278, lr=2.9e-5]\n",
            "Epoch 21:  95% 71/75 [01:48<00:06,  1.52s/it, loss=1.278, lr=2.9e-5]\n",
            "Epoch 21:  96% 72/75 [01:49<00:04,  1.52s/it, loss=1.278, lr=2.9e-5]\n",
            "Epoch 21:  97% 73/75 [01:51<00:03,  1.52s/it, loss=1.278, lr=2.9e-5]\n",
            "Epoch 21:  99% 74/75 [01:52<00:01,  1.52s/it, loss=1.278, lr=2.9e-5]\n",
            "Epoch 21: 100% 75/75 [01:54<00:00,  1.53s/it, loss=1.278, lr=2.9e-5]2024-04-21 23:07:15 bleu - corpus_score: That's 100 lines that end in a tokenized period ('.')\n",
            "2024-04-21 23:07:15 bleu - corpus_score: It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2024-04-21 23:07:15 bleu - corpus_score: If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2024-04-21 23:07:41 train_table2text_t5 - validation_epoch_end: valid epoch: 21\n",
            "2024-04-21 23:07:41 train_table2text_t5 - validation_epoch_end: 21 bleu_info: 17.08455472131035\n",
            "2024-04-21 23:07:41 train_table2text_t5 - validation_epoch_end: 21 mover score: (0.298, 0.272)\n",
            "\n",
            "\n",
            "  0% 0/200 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            " 36% 72/200 [00:00<00:00, 717.07it/s]\u001b[A\u001b[A\n",
            "\n",
            " 72% 144/200 [00:00<00:00, 496.08it/s]\u001b[A\u001b[A\n",
            "\n",
            "100% 200/200 [00:00<00:00, 508.19it/s]\n",
            "\n",
            "\n",
            "  0% 0/200 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "100% 200/200 [00:00<00:00, 1840.91it/s]\n",
            "2024-04-21 23:07:42 train_table2text_t5 - get_dataloader: loading dev dataloader...\n",
            "2024-04-21 23:07:42 train_table2text_t5 - get_dataloader: done\n",
            "2024-04-21 23:07:42 lightning_base - on_validation_end: ***** Validation results *****\n",
            "2024-04-21 23:07:42 lightning_base - on_validation_end: avg_val_loss = tensor(0.6702, device='cuda:0')\n",
            "2024-04-21 23:07:42 lightning_base - on_validation_end: epoch = 20\n",
            "2024-04-21 23:07:42 lightning_base - on_validation_end: loss = tensor(1.5123, device='cuda:0')\n",
            "2024-04-21 23:07:42 lightning_base - on_validation_end: step_count = 22\n",
            "2024-04-21 23:07:42 lightning_base - on_validation_end: train_loss = tensor(1.5123, device='cuda:0')\n",
            "2024-04-21 23:07:42 lightning_base - on_validation_end: val_avg_bleu = 17.08455472131035\n",
            "2024-04-21 23:07:42 lightning_base - on_validation_end: val_mover = tensor(0.2980, device='cuda:0')\n",
            "2024-04-21 23:07:42 lightning_base - on_validation_end: val_mover_mean1 = 0.298\n",
            "2024-04-21 23:07:42 lightning_base - on_validation_end: val_mover_median1 = 0.272\n",
            "Epoch 21: 100% 75/75 [02:22<00:00,  1.89s/it, loss=1.278, lr=2.9e-5]\n",
            "                                               \u001b[A\n",
            "  0% 0/200 [00:00<?, ?it/s]\u001b[A\n",
            " 34% 68/200 [00:00<00:00, 661.66it/s]\u001b[A\n",
            " 68% 135/200 [00:00<00:00, 564.35it/s]\u001b[A\n",
            "100% 200/200 [00:00<00:00, 504.42it/s]\n",
            "\n",
            "  0% 0/200 [00:00<?, ?it/s]\u001b[A\n",
            "100% 200/200 [00:00<00:00, 1959.25it/s]\n",
            "2024-04-21 23:07:58 train_table2text_t5 - get_dataloader: loading train dataloader...\n",
            "2024-04-21 23:07:58 train_table2text_t5 - get_dataloader: done\n",
            "Epoch 22:  33% 25/75 [00:21<00:42,  1.17it/s, loss=1.204, lr=2.9e-5]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 22:  35% 26/75 [00:24<00:45,  1.07it/s, loss=1.204, lr=2.9e-5]\n",
            "Epoch 22:  36% 27/75 [00:25<00:45,  1.04it/s, loss=1.204, lr=2.9e-5]\n",
            "Epoch 22:  37% 28/75 [00:27<00:45,  1.02it/s, loss=1.204, lr=2.9e-5]\n",
            "Epoch 22:  39% 29/75 [00:30<00:47,  1.04s/it, loss=1.204, lr=2.9e-5]\n",
            "Epoch 22:  40% 30/75 [00:31<00:47,  1.06s/it, loss=1.204, lr=2.9e-5]\n",
            "Epoch 22:  41% 31/75 [00:33<00:46,  1.07s/it, loss=1.204, lr=2.9e-5]\n",
            "Epoch 22:  43% 32/75 [00:34<00:46,  1.09s/it, loss=1.204, lr=2.9e-5]\n",
            "Epoch 22:  44% 33/75 [00:36<00:46,  1.10s/it, loss=1.204, lr=2.9e-5]\n",
            "Epoch 22:  45% 34/75 [00:37<00:45,  1.10s/it, loss=1.204, lr=2.9e-5]\n",
            "Epoch 22:  47% 35/75 [00:38<00:44,  1.11s/it, loss=1.204, lr=2.9e-5]\n",
            "Epoch 22:  48% 36/75 [00:40<00:44,  1.13s/it, loss=1.204, lr=2.9e-5]\n",
            "Epoch 22:  49% 37/75 [00:42<00:43,  1.15s/it, loss=1.204, lr=2.9e-5]\n",
            "Epoch 22:  51% 38/75 [00:44<00:43,  1.17s/it, loss=1.204, lr=2.9e-5]\n",
            "Epoch 22:  52% 39/75 [00:46<00:42,  1.19s/it, loss=1.204, lr=2.9e-5]\n",
            "Epoch 22:  53% 40/75 [00:49<00:42,  1.23s/it, loss=1.204, lr=2.9e-5]\n",
            "Epoch 22:  55% 41/75 [00:51<00:42,  1.25s/it, loss=1.204, lr=2.9e-5]\n",
            "Epoch 22:  56% 42/75 [00:52<00:41,  1.24s/it, loss=1.204, lr=2.9e-5]\n",
            "Epoch 22:  57% 43/75 [00:53<00:39,  1.24s/it, loss=1.204, lr=2.9e-5]\n",
            "Epoch 22:  59% 44/75 [00:55<00:39,  1.26s/it, loss=1.204, lr=2.9e-5]\n",
            "Epoch 22:  60% 45/75 [00:56<00:37,  1.26s/it, loss=1.204, lr=2.9e-5]\n",
            "Epoch 22:  61% 46/75 [00:57<00:36,  1.26s/it, loss=1.204, lr=2.9e-5]\n",
            "Epoch 22:  63% 47/75 [01:00<00:35,  1.28s/it, loss=1.204, lr=2.9e-5]\n",
            "Epoch 22:  64% 48/75 [01:01<00:34,  1.28s/it, loss=1.204, lr=2.9e-5]\n",
            "Epoch 22:  65% 49/75 [01:04<00:34,  1.31s/it, loss=1.204, lr=2.9e-5]\n",
            "Epoch 22:  67% 50/75 [01:06<00:33,  1.34s/it, loss=1.204, lr=2.9e-5]\n",
            "Epoch 22:  68% 51/75 [01:08<00:32,  1.34s/it, loss=1.204, lr=2.9e-5]\n",
            "Epoch 22:  69% 52/75 [01:09<00:30,  1.34s/it, loss=1.204, lr=2.9e-5]\n",
            "Epoch 22:  71% 53/75 [01:14<00:31,  1.41s/it, loss=1.204, lr=2.9e-5]\n",
            "Epoch 22:  72% 54/75 [01:16<00:29,  1.42s/it, loss=1.204, lr=2.9e-5]\n",
            "Epoch 22:  73% 55/75 [01:18<00:28,  1.43s/it, loss=1.204, lr=2.9e-5]\n",
            "Epoch 22:  75% 56/75 [01:21<00:27,  1.45s/it, loss=1.204, lr=2.9e-5]\n",
            "Epoch 22:  76% 57/75 [01:22<00:25,  1.44s/it, loss=1.204, lr=2.9e-5]\n",
            "Epoch 22:  77% 58/75 [01:23<00:24,  1.43s/it, loss=1.204, lr=2.9e-5]\n",
            "Epoch 22:  79% 59/75 [01:24<00:22,  1.43s/it, loss=1.204, lr=2.9e-5]\n",
            "Epoch 22:  80% 60/75 [01:26<00:21,  1.44s/it, loss=1.204, lr=2.9e-5]\n",
            "Epoch 22:  81% 61/75 [01:27<00:20,  1.44s/it, loss=1.204, lr=2.9e-5]\n",
            "Epoch 22:  83% 62/75 [01:29<00:18,  1.44s/it, loss=1.204, lr=2.9e-5]\n",
            "Epoch 22:  84% 63/75 [01:32<00:17,  1.46s/it, loss=1.204, lr=2.9e-5]\n",
            "Epoch 22:  85% 64/75 [01:34<00:16,  1.48s/it, loss=1.204, lr=2.9e-5]\n",
            "Epoch 22:  87% 65/75 [01:35<00:14,  1.47s/it, loss=1.204, lr=2.9e-5]\n",
            "Epoch 22:  88% 66/75 [01:37<00:13,  1.48s/it, loss=1.204, lr=2.9e-5]\n",
            "Epoch 22:  89% 67/75 [01:39<00:11,  1.48s/it, loss=1.204, lr=2.9e-5]\n",
            "Epoch 22:  91% 68/75 [01:40<00:10,  1.47s/it, loss=1.204, lr=2.9e-5]\n",
            "Epoch 22:  92% 69/75 [01:42<00:08,  1.48s/it, loss=1.204, lr=2.9e-5]\n",
            "Epoch 22:  93% 70/75 [01:44<00:07,  1.49s/it, loss=1.204, lr=2.9e-5]\n",
            "Epoch 22:  95% 71/75 [01:46<00:05,  1.50s/it, loss=1.204, lr=2.9e-5]\n",
            "Epoch 22:  96% 72/75 [01:47<00:04,  1.49s/it, loss=1.204, lr=2.9e-5]\n",
            "Epoch 22:  97% 73/75 [01:49<00:02,  1.50s/it, loss=1.204, lr=2.9e-5]\n",
            "Epoch 22:  99% 74/75 [01:51<00:01,  1.50s/it, loss=1.204, lr=2.9e-5]\n",
            "Epoch 22: 100% 75/75 [01:53<00:00,  1.51s/it, loss=1.204, lr=2.9e-5]2024-04-21 23:09:52 bleu - corpus_score: That's 100 lines that end in a tokenized period ('.')\n",
            "2024-04-21 23:09:52 bleu - corpus_score: It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2024-04-21 23:09:52 bleu - corpus_score: If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2024-04-21 23:10:18 train_table2text_t5 - validation_epoch_end: valid epoch: 22\n",
            "2024-04-21 23:10:18 train_table2text_t5 - validation_epoch_end: 22 bleu_info: 17.372852547923642\n",
            "2024-04-21 23:10:18 train_table2text_t5 - validation_epoch_end: 22 mover score: (0.319, 0.284)\n",
            "\n",
            "\n",
            "  0% 0/200 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            " 34% 69/200 [00:00<00:00, 688.58it/s]\u001b[A\u001b[A\n",
            "\n",
            " 69% 138/200 [00:00<00:00, 500.69it/s]\u001b[A\u001b[A\n",
            "\n",
            "100% 200/200 [00:00<00:00, 500.99it/s]\n",
            "\n",
            "\n",
            "  0% 0/200 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "100% 200/200 [00:00<00:00, 1860.96it/s]\n",
            "2024-04-21 23:10:19 train_table2text_t5 - get_dataloader: loading dev dataloader...\n",
            "2024-04-21 23:10:19 train_table2text_t5 - get_dataloader: done\n",
            "2024-04-21 23:10:19 lightning_base - on_validation_end: ***** Validation results *****\n",
            "2024-04-21 23:10:19 lightning_base - on_validation_end: avg_val_loss = tensor(0.5991, device='cuda:0')\n",
            "2024-04-21 23:10:19 lightning_base - on_validation_end: epoch = 21\n",
            "2024-04-21 23:10:19 lightning_base - on_validation_end: loss = tensor(1.4137, device='cuda:0')\n",
            "2024-04-21 23:10:19 lightning_base - on_validation_end: step_count = 23\n",
            "2024-04-21 23:10:19 lightning_base - on_validation_end: train_loss = tensor(1.4137, device='cuda:0')\n",
            "2024-04-21 23:10:19 lightning_base - on_validation_end: val_avg_bleu = 17.372852547923642\n",
            "2024-04-21 23:10:19 lightning_base - on_validation_end: val_mover = tensor(0.3190, device='cuda:0')\n",
            "2024-04-21 23:10:19 lightning_base - on_validation_end: val_mover_mean1 = 0.319\n",
            "2024-04-21 23:10:19 lightning_base - on_validation_end: val_mover_median1 = 0.284\n",
            "Epoch 22: 100% 75/75 [02:20<00:00,  1.87s/it, loss=1.204, lr=2.9e-5]\n",
            "                                               \u001b[A\n",
            "  0% 0/200 [00:00<?, ?it/s]\u001b[A\n",
            "  7% 14/200 [00:00<00:01, 137.03it/s]\u001b[A\n",
            " 28% 55/200 [00:00<00:00, 295.43it/s]\u001b[A\n",
            " 44% 87/200 [00:00<00:00, 306.51it/s]\u001b[A\n",
            " 59% 118/200 [00:00<00:00, 258.18it/s]\u001b[A\n",
            " 72% 145/200 [00:00<00:00, 245.96it/s]\u001b[A\n",
            "100% 200/200 [00:00<00:00, 258.73it/s]\n",
            "\n",
            "  0% 0/200 [00:00<?, ?it/s]\u001b[A\n",
            "100% 200/200 [00:00<00:00, 1015.44it/s]\n",
            "2024-04-21 23:10:37 train_table2text_t5 - get_dataloader: loading train dataloader...\n",
            "2024-04-21 23:10:37 train_table2text_t5 - get_dataloader: done\n",
            "Epoch 23:  33% 25/75 [00:21<00:43,  1.14it/s, loss=1.114, lr=2.9e-5]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 23:  35% 26/75 [00:25<00:47,  1.04it/s, loss=1.114, lr=2.9e-5]\n",
            "Epoch 23:  36% 27/75 [00:27<00:48,  1.01s/it, loss=1.114, lr=2.9e-5]\n",
            "Epoch 23:  37% 28/75 [00:28<00:48,  1.03s/it, loss=1.114, lr=2.9e-5]\n",
            "Epoch 23:  39% 29/75 [00:31<00:50,  1.09s/it, loss=1.114, lr=2.9e-5]\n",
            "Epoch 23:  40% 30/75 [00:34<00:52,  1.16s/it, loss=1.114, lr=2.9e-5]\n",
            "Epoch 23:  41% 31/75 [00:36<00:52,  1.19s/it, loss=1.114, lr=2.9e-5]\n",
            "Epoch 23:  43% 32/75 [00:39<00:52,  1.22s/it, loss=1.114, lr=2.9e-5]\n",
            "Epoch 23:  44% 33/75 [00:40<00:51,  1.23s/it, loss=1.114, lr=2.9e-5]\n",
            "Epoch 23:  45% 34/75 [00:42<00:50,  1.24s/it, loss=1.114, lr=2.9e-5]\n",
            "Epoch 23:  47% 35/75 [00:43<00:49,  1.24s/it, loss=1.114, lr=2.9e-5]\n",
            "Epoch 23:  48% 36/75 [00:44<00:48,  1.25s/it, loss=1.114, lr=2.9e-5]\n",
            "Epoch 23:  49% 37/75 [00:47<00:48,  1.29s/it, loss=1.114, lr=2.9e-5]\n",
            "Epoch 23:  51% 38/75 [00:50<00:49,  1.33s/it, loss=1.114, lr=2.9e-5]\n",
            "Epoch 23:  52% 39/75 [00:52<00:48,  1.36s/it, loss=1.114, lr=2.9e-5]\n",
            "Epoch 23:  53% 40/75 [00:55<00:48,  1.38s/it, loss=1.114, lr=2.9e-5]\n",
            "Epoch 23:  55% 41/75 [00:57<00:47,  1.40s/it, loss=1.114, lr=2.9e-5]\n",
            "Epoch 23:  56% 42/75 [00:58<00:46,  1.40s/it, loss=1.114, lr=2.9e-5]\n",
            "Epoch 23:  57% 43/75 [01:00<00:45,  1.41s/it, loss=1.114, lr=2.9e-5]\n",
            "Epoch 23:  59% 44/75 [01:03<00:44,  1.44s/it, loss=1.114, lr=2.9e-5]\n",
            "Epoch 23:  60% 45/75 [01:05<00:43,  1.45s/it, loss=1.114, lr=2.9e-5]\n",
            "Epoch 23:  61% 46/75 [01:06<00:42,  1.45s/it, loss=1.114, lr=2.9e-5]\n",
            "Epoch 23:  63% 47/75 [01:09<00:41,  1.47s/it, loss=1.114, lr=2.9e-5]\n",
            "Epoch 23:  64% 48/75 [01:10<00:39,  1.47s/it, loss=1.114, lr=2.9e-5]\n",
            "Epoch 23:  65% 49/75 [01:14<00:39,  1.52s/it, loss=1.114, lr=2.9e-5]\n",
            "Epoch 23:  67% 50/75 [01:18<00:39,  1.57s/it, loss=1.114, lr=2.9e-5]\n",
            "Epoch 23:  68% 51/75 [01:20<00:38,  1.59s/it, loss=1.114, lr=2.9e-5]\n",
            "Epoch 23:  69% 52/75 [01:22<00:36,  1.58s/it, loss=1.114, lr=2.9e-5]\n",
            "Epoch 23:  71% 53/75 [01:27<00:36,  1.65s/it, loss=1.114, lr=2.9e-5]\n",
            "Epoch 23:  72% 54/75 [01:29<00:34,  1.66s/it, loss=1.114, lr=2.9e-5]\n",
            "Epoch 23:  73% 55/75 [01:32<00:33,  1.68s/it, loss=1.114, lr=2.9e-5]\n",
            "Epoch 23:  75% 56/75 [01:34<00:32,  1.69s/it, loss=1.114, lr=2.9e-5]\n",
            "Epoch 23:  76% 57/75 [01:36<00:30,  1.69s/it, loss=1.114, lr=2.9e-5]\n",
            "Epoch 23:  77% 58/75 [01:37<00:28,  1.68s/it, loss=1.114, lr=2.9e-5]\n",
            "Epoch 23:  79% 59/75 [01:38<00:26,  1.68s/it, loss=1.114, lr=2.9e-5]\n",
            "Epoch 23:  80% 60/75 [01:41<00:25,  1.69s/it, loss=1.114, lr=2.9e-5]\n",
            "Epoch 23:  81% 61/75 [01:43<00:23,  1.69s/it, loss=1.114, lr=2.9e-5]\n",
            "Epoch 23:  83% 62/75 [01:44<00:21,  1.69s/it, loss=1.114, lr=2.9e-5]\n",
            "Epoch 23:  84% 63/75 [01:48<00:20,  1.72s/it, loss=1.114, lr=2.9e-5]\n",
            "Epoch 23:  85% 64/75 [01:50<00:18,  1.72s/it, loss=1.114, lr=2.9e-5]\n",
            "Epoch 23:  87% 65/75 [01:52<00:17,  1.73s/it, loss=1.114, lr=2.9e-5]\n",
            "Epoch 23:  88% 66/75 [01:54<00:15,  1.73s/it, loss=1.114, lr=2.9e-5]\n",
            "Epoch 23:  89% 67/75 [01:55<00:13,  1.73s/it, loss=1.114, lr=2.9e-5]\n",
            "Epoch 23:  91% 68/75 [01:57<00:12,  1.72s/it, loss=1.114, lr=2.9e-5]\n",
            "Epoch 23:  92% 69/75 [01:59<00:10,  1.74s/it, loss=1.114, lr=2.9e-5]\n",
            "Epoch 23:  93% 70/75 [02:02<00:08,  1.75s/it, loss=1.114, lr=2.9e-5]\n",
            "Epoch 23:  95% 71/75 [02:04<00:07,  1.76s/it, loss=1.114, lr=2.9e-5]\n",
            "Epoch 23:  96% 72/75 [02:06<00:05,  1.76s/it, loss=1.114, lr=2.9e-5]\n",
            "Epoch 23:  97% 73/75 [02:09<00:03,  1.77s/it, loss=1.114, lr=2.9e-5]\n",
            "Epoch 23:  99% 74/75 [02:10<00:01,  1.76s/it, loss=1.114, lr=2.9e-5]\n",
            "Epoch 23: 100% 75/75 [02:12<00:00,  1.76s/it, loss=1.114, lr=2.9e-5]2024-04-21 23:12:50 bleu - corpus_score: That's 100 lines that end in a tokenized period ('.')\n",
            "2024-04-21 23:12:50 bleu - corpus_score: It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2024-04-21 23:12:50 bleu - corpus_score: If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2024-04-21 23:13:22 train_table2text_t5 - validation_epoch_end: valid epoch: 23\n",
            "2024-04-21 23:13:22 train_table2text_t5 - validation_epoch_end: 23 bleu_info: 24.586209638612925\n",
            "2024-04-21 23:13:22 train_table2text_t5 - validation_epoch_end: 23 mover score: (0.355, 0.322)\n",
            "\n",
            "\n",
            "  0% 0/200 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            " 34% 68/200 [00:00<00:00, 676.47it/s]\u001b[A\u001b[A\n",
            "\n",
            " 68% 136/200 [00:00<00:00, 590.96it/s]\u001b[A\u001b[A\n",
            "\n",
            "100% 200/200 [00:00<00:00, 516.41it/s]\n",
            "\n",
            "\n",
            "  0% 0/200 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "100% 200/200 [00:00<00:00, 1630.97it/s]\n",
            "2024-04-21 23:13:22 train_table2text_t5 - get_dataloader: loading dev dataloader...\n",
            "2024-04-21 23:13:22 train_table2text_t5 - get_dataloader: done\n",
            "2024-04-21 23:13:22 lightning_base - on_validation_end: ***** Validation results *****\n",
            "2024-04-21 23:13:22 lightning_base - on_validation_end: avg_val_loss = tensor(0.5307, device='cuda:0')\n",
            "2024-04-21 23:13:22 lightning_base - on_validation_end: epoch = 22\n",
            "2024-04-21 23:13:22 lightning_base - on_validation_end: loss = tensor(1.1908, device='cuda:0')\n",
            "2024-04-21 23:13:22 lightning_base - on_validation_end: step_count = 24\n",
            "2024-04-21 23:13:22 lightning_base - on_validation_end: train_loss = tensor(1.1908, device='cuda:0')\n",
            "2024-04-21 23:13:22 lightning_base - on_validation_end: val_avg_bleu = 24.586209638612925\n",
            "2024-04-21 23:13:22 lightning_base - on_validation_end: val_mover = tensor(0.3550, device='cuda:0')\n",
            "2024-04-21 23:13:22 lightning_base - on_validation_end: val_mover_mean1 = 0.355\n",
            "2024-04-21 23:13:22 lightning_base - on_validation_end: val_mover_median1 = 0.322\n",
            "Epoch 23: 100% 75/75 [02:45<00:00,  2.20s/it, loss=1.114, lr=2.9e-5]\n",
            "                                               \u001b[A\n",
            "  0% 0/200 [00:00<?, ?it/s]\u001b[A\n",
            " 34% 68/200 [00:00<00:00, 656.26it/s]\u001b[A\n",
            " 67% 134/200 [00:00<00:00, 543.24it/s]\u001b[A\n",
            "100% 200/200 [00:00<00:00, 501.86it/s]\n",
            "\n",
            "  0% 0/200 [00:00<?, ?it/s]\u001b[A\n",
            "100% 200/200 [00:00<00:00, 1888.85it/s]\n",
            "2024-04-21 23:13:39 train_table2text_t5 - get_dataloader: loading train dataloader...\n",
            "2024-04-21 23:13:39 train_table2text_t5 - get_dataloader: done\n",
            "Epoch 24:  33% 25/75 [00:20<00:41,  1.20it/s, loss=1.040, lr=2.9e-5]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 24:  35% 26/75 [00:23<00:43,  1.11it/s, loss=1.040, lr=2.9e-5]\n",
            "Epoch 24:  36% 27/75 [00:25<00:45,  1.05it/s, loss=1.040, lr=2.9e-5]\n",
            "Epoch 24:  37% 28/75 [00:27<00:45,  1.04it/s, loss=1.040, lr=2.9e-5]\n",
            "Epoch 24:  39% 29/75 [00:29<00:47,  1.03s/it, loss=1.040, lr=2.9e-5]\n",
            "Epoch 24:  40% 30/75 [00:32<00:49,  1.10s/it, loss=1.040, lr=2.9e-5]\n",
            "Epoch 24:  41% 31/75 [00:34<00:48,  1.11s/it, loss=1.040, lr=2.9e-5]\n",
            "Epoch 24:  43% 32/75 [00:35<00:48,  1.12s/it, loss=1.040, lr=2.9e-5]\n",
            "Epoch 24:  44% 33/75 [00:37<00:47,  1.13s/it, loss=1.040, lr=2.9e-5]\n",
            "Epoch 24:  45% 34/75 [00:38<00:45,  1.12s/it, loss=1.040, lr=2.9e-5]\n",
            "Epoch 24:  47% 35/75 [00:39<00:44,  1.12s/it, loss=1.040, lr=2.9e-5]\n",
            "Epoch 24:  48% 36/75 [00:40<00:44,  1.13s/it, loss=1.040, lr=2.9e-5]\n",
            "Epoch 24:  49% 37/75 [00:42<00:44,  1.16s/it, loss=1.040, lr=2.9e-5]\n",
            "Epoch 24:  51% 38/75 [00:44<00:43,  1.18s/it, loss=1.040, lr=2.9e-5]\n",
            "Epoch 24:  52% 39/75 [00:46<00:43,  1.20s/it, loss=1.040, lr=2.9e-5]\n",
            "Epoch 24:  53% 40/75 [00:48<00:42,  1.22s/it, loss=1.040, lr=2.9e-5]\n",
            "Epoch 24:  55% 41/75 [00:50<00:42,  1.24s/it, loss=1.040, lr=2.9e-5]\n",
            "Epoch 24:  56% 42/75 [00:51<00:40,  1.24s/it, loss=1.040, lr=2.9e-5]\n",
            "Epoch 24:  57% 43/75 [00:53<00:39,  1.25s/it, loss=1.040, lr=2.9e-5]\n",
            "Epoch 24:  59% 44/75 [00:57<00:40,  1.30s/it, loss=1.040, lr=2.9e-5]\n",
            "Epoch 24:  60% 45/75 [00:59<00:39,  1.32s/it, loss=1.040, lr=2.9e-5]\n",
            "Epoch 24:  61% 46/75 [01:00<00:38,  1.32s/it, loss=1.040, lr=2.9e-5]\n",
            "Epoch 24:  63% 47/75 [01:03<00:37,  1.35s/it, loss=1.040, lr=2.9e-5]\n",
            "Epoch 24:  64% 48/75 [01:05<00:36,  1.36s/it, loss=1.040, lr=2.9e-5]\n",
            "Epoch 24:  65% 49/75 [01:12<00:38,  1.47s/it, loss=1.040, lr=2.9e-5]\n",
            "Epoch 24:  67% 50/75 [01:17<00:38,  1.54s/it, loss=1.040, lr=2.9e-5]\n",
            "Epoch 24:  68% 51/75 [01:19<00:37,  1.55s/it, loss=1.040, lr=2.9e-5]\n",
            "Epoch 24:  69% 52/75 [01:20<00:35,  1.55s/it, loss=1.040, lr=2.9e-5]\n",
            "Epoch 24:  71% 53/75 [01:29<00:36,  1.68s/it, loss=1.040, lr=2.9e-5]\n",
            "Epoch 24:  72% 54/75 [01:31<00:35,  1.69s/it, loss=1.040, lr=2.9e-5]\n",
            "Epoch 24:  73% 55/75 [01:34<00:34,  1.71s/it, loss=1.040, lr=2.9e-5]\n",
            "Epoch 24:  75% 56/75 [01:37<00:33,  1.74s/it, loss=1.040, lr=2.9e-5]\n",
            "Epoch 24:  76% 57/75 [01:38<00:31,  1.73s/it, loss=1.040, lr=2.9e-5]\n",
            "Epoch 24:  77% 58/75 [01:40<00:29,  1.72s/it, loss=1.040, lr=2.9e-5]\n",
            "Epoch 24:  79% 59/75 [01:41<00:27,  1.72s/it, loss=1.040, lr=2.9e-5]\n",
            "Epoch 24:  80% 60/75 [01:43<00:25,  1.72s/it, loss=1.040, lr=2.9e-5]\n",
            "Epoch 24:  81% 61/75 [01:45<00:24,  1.73s/it, loss=1.040, lr=2.9e-5]\n",
            "Epoch 24:  83% 62/75 [01:47<00:22,  1.73s/it, loss=1.040, lr=2.9e-5]\n",
            "Epoch 24:  84% 63/75 [01:49<00:20,  1.75s/it, loss=1.040, lr=2.9e-5]\n",
            "Epoch 24:  85% 64/75 [01:52<00:19,  1.76s/it, loss=1.040, lr=2.9e-5]\n",
            "Epoch 24:  87% 65/75 [01:53<00:17,  1.75s/it, loss=1.040, lr=2.9e-5]\n",
            "Epoch 24:  88% 66/75 [01:55<00:15,  1.75s/it, loss=1.040, lr=2.9e-5]\n",
            "Epoch 24:  89% 67/75 [01:56<00:13,  1.74s/it, loss=1.040, lr=2.9e-5]\n",
            "Epoch 24:  91% 68/75 [01:58<00:12,  1.74s/it, loss=1.040, lr=2.9e-5]\n",
            "Epoch 24:  92% 69/75 [02:01<00:10,  1.76s/it, loss=1.040, lr=2.9e-5]\n",
            "Epoch 24:  93% 70/75 [02:02<00:08,  1.76s/it, loss=1.040, lr=2.9e-5]\n",
            "Epoch 24:  95% 71/75 [02:04<00:07,  1.76s/it, loss=1.040, lr=2.9e-5]\n",
            "Epoch 24:  96% 72/75 [02:06<00:05,  1.75s/it, loss=1.040, lr=2.9e-5]\n",
            "Epoch 24:  97% 73/75 [02:07<00:03,  1.75s/it, loss=1.040, lr=2.9e-5]\n",
            "Epoch 24:  99% 74/75 [02:09<00:01,  1.76s/it, loss=1.040, lr=2.9e-5]\n",
            "Epoch 24: 100% 75/75 [02:12<00:00,  1.76s/it, loss=1.040, lr=2.9e-5]2024-04-21 23:15:52 bleu - corpus_score: That's 100 lines that end in a tokenized period ('.')\n",
            "2024-04-21 23:15:52 bleu - corpus_score: It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2024-04-21 23:15:52 bleu - corpus_score: If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2024-04-21 23:16:29 train_table2text_t5 - validation_epoch_end: valid epoch: 24\n",
            "2024-04-21 23:16:29 train_table2text_t5 - validation_epoch_end: 24 bleu_info: 26.336867323442423\n",
            "2024-04-21 23:16:29 train_table2text_t5 - validation_epoch_end: 24 mover score: (0.372, 0.336)\n",
            "\n",
            "\n",
            "  0% 0/200 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            " 36% 71/200 [00:00<00:00, 707.67it/s]\u001b[A\u001b[A\n",
            "\n",
            " 71% 142/200 [00:00<00:00, 526.50it/s]\u001b[A\u001b[A\n",
            "\n",
            "100% 200/200 [00:00<00:00, 516.82it/s]\n",
            "\n",
            "\n",
            "  0% 0/200 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "100% 200/200 [00:00<00:00, 1873.02it/s]\n",
            "2024-04-21 23:16:29 train_table2text_t5 - get_dataloader: loading dev dataloader...\n",
            "2024-04-21 23:16:29 train_table2text_t5 - get_dataloader: done\n",
            "2024-04-21 23:16:29 lightning_base - on_validation_end: ***** Validation results *****\n",
            "2024-04-21 23:16:29 lightning_base - on_validation_end: avg_val_loss = tensor(0.4726, device='cuda:0')\n",
            "2024-04-21 23:16:29 lightning_base - on_validation_end: epoch = 23\n",
            "2024-04-21 23:16:29 lightning_base - on_validation_end: loss = tensor(1.0339, device='cuda:0')\n",
            "2024-04-21 23:16:29 lightning_base - on_validation_end: step_count = 25\n",
            "2024-04-21 23:16:29 lightning_base - on_validation_end: train_loss = tensor(1.0339, device='cuda:0')\n",
            "2024-04-21 23:16:29 lightning_base - on_validation_end: val_avg_bleu = 26.336867323442423\n",
            "2024-04-21 23:16:29 lightning_base - on_validation_end: val_mover = tensor(0.3720, device='cuda:0')\n",
            "2024-04-21 23:16:29 lightning_base - on_validation_end: val_mover_mean1 = 0.372\n",
            "2024-04-21 23:16:29 lightning_base - on_validation_end: val_mover_median1 = 0.336\n",
            "Epoch 24: 100% 75/75 [02:49<00:00,  2.27s/it, loss=1.040, lr=2.9e-5]\n",
            "                                               \u001b[A\n",
            "  0% 0/200 [00:00<?, ?it/s]\u001b[A\n",
            " 31% 62/200 [00:00<00:00, 613.96it/s]\u001b[A\n",
            " 62% 124/200 [00:00<00:00, 522.39it/s]\u001b[A\n",
            "100% 200/200 [00:00<00:00, 485.53it/s]\n",
            "\n",
            "  0% 0/200 [00:00<?, ?it/s]\u001b[A\n",
            "100% 200/200 [00:00<00:00, 1651.37it/s]\n",
            "2024-04-21 23:16:55 train_table2text_t5 - get_dataloader: loading train dataloader...\n",
            "2024-04-21 23:16:55 train_table2text_t5 - get_dataloader: done\n",
            "Epoch 25:  33% 25/75 [00:20<00:40,  1.24it/s, loss=0.948, lr=2.9e-5]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 25:  35% 26/75 [00:22<00:43,  1.14it/s, loss=0.948, lr=2.9e-5]\n",
            "Epoch 25:  36% 27/75 [00:25<00:45,  1.05it/s, loss=0.948, lr=2.9e-5]\n",
            "Epoch 25:  37% 28/75 [00:27<00:45,  1.03it/s, loss=0.948, lr=2.9e-5]\n",
            "Epoch 25:  39% 29/75 [00:30<00:48,  1.05s/it, loss=0.948, lr=2.9e-5]\n",
            "Epoch 25:  40% 30/75 [00:33<00:50,  1.13s/it, loss=0.948, lr=2.9e-5]\n",
            "Epoch 25:  41% 31/75 [00:35<00:50,  1.16s/it, loss=0.948, lr=2.9e-5]\n",
            "Epoch 25:  43% 32/75 [00:37<00:50,  1.17s/it, loss=0.948, lr=2.9e-5]\n",
            "Epoch 25:  44% 33/75 [00:40<00:51,  1.23s/it, loss=0.948, lr=2.9e-5]\n",
            "Epoch 25:  45% 34/75 [00:42<00:50,  1.24s/it, loss=0.948, lr=2.9e-5]\n",
            "Epoch 25:  47% 35/75 [00:44<00:50,  1.27s/it, loss=0.948, lr=2.9e-5]\n",
            "Epoch 25:  48% 36/75 [00:46<00:50,  1.30s/it, loss=0.948, lr=2.9e-5]\n",
            "Epoch 25:  49% 37/75 [00:49<00:51,  1.35s/it, loss=0.948, lr=2.9e-5]\n",
            "Epoch 25:  51% 38/75 [00:53<00:51,  1.40s/it, loss=0.948, lr=2.9e-5]\n",
            "Epoch 25:  52% 39/75 [00:56<00:51,  1.44s/it, loss=0.948, lr=2.9e-5]\n",
            "Epoch 25:  53% 40/75 [00:58<00:51,  1.46s/it, loss=0.948, lr=2.9e-5]\n",
            "Epoch 25:  55% 41/75 [01:00<00:50,  1.48s/it, loss=0.948, lr=2.9e-5]\n",
            "Epoch 25:  56% 42/75 [01:02<00:48,  1.48s/it, loss=0.948, lr=2.9e-5]\n",
            "Epoch 25:  57% 43/75 [01:03<00:47,  1.48s/it, loss=0.948, lr=2.9e-5]\n",
            "Epoch 25:  59% 44/75 [01:07<00:47,  1.53s/it, loss=0.948, lr=2.9e-5]\n",
            "Epoch 25:  60% 45/75 [01:10<00:46,  1.56s/it, loss=0.948, lr=2.9e-5]\n",
            "Epoch 25:  61% 46/75 [01:11<00:45,  1.56s/it, loss=0.948, lr=2.9e-5]\n",
            "Epoch 25:  63% 47/75 [01:14<00:44,  1.58s/it, loss=0.948, lr=2.9e-5]\n",
            "Epoch 25:  64% 48/75 [01:16<00:42,  1.59s/it, loss=0.948, lr=2.9e-5]\n",
            "Epoch 25:  65% 49/75 [01:22<00:44,  1.69s/it, loss=0.948, lr=2.9e-5]\n",
            "Epoch 25:  67% 50/75 [01:31<00:45,  1.83s/it, loss=0.948, lr=2.9e-5]\n",
            "Epoch 25:  68% 51/75 [01:33<00:43,  1.83s/it, loss=0.948, lr=2.9e-5]\n",
            "Epoch 25:  69% 52/75 [01:34<00:42,  1.83s/it, loss=0.948, lr=2.9e-5]\n",
            "Epoch 25:  71% 53/75 [01:43<00:43,  1.96s/it, loss=0.948, lr=2.9e-5]\n",
            "Epoch 25:  72% 54/75 [01:48<00:42,  2.01s/it, loss=0.948, lr=2.9e-5]\n",
            "Epoch 25:  73% 55/75 [01:50<00:40,  2.01s/it, loss=0.948, lr=2.9e-5]\n",
            "Epoch 25:  75% 56/75 [01:54<00:38,  2.04s/it, loss=0.948, lr=2.9e-5]\n",
            "Epoch 25:  76% 57/75 [01:55<00:36,  2.03s/it, loss=0.948, lr=2.9e-5]\n",
            "Epoch 25:  77% 58/75 [01:57<00:34,  2.03s/it, loss=0.948, lr=2.9e-5]\n",
            "Epoch 25:  79% 59/75 [01:59<00:32,  2.03s/it, loss=0.948, lr=2.9e-5]\n",
            "Epoch 25:  80% 60/75 [02:02<00:30,  2.04s/it, loss=0.948, lr=2.9e-5]\n",
            "Epoch 25:  81% 61/75 [02:03<00:28,  2.03s/it, loss=0.948, lr=2.9e-5]\n",
            "Epoch 25:  83% 62/75 [02:05<00:26,  2.02s/it, loss=0.948, lr=2.9e-5]\n",
            "Epoch 25:  84% 63/75 [02:08<00:24,  2.03s/it, loss=0.948, lr=2.9e-5]\n",
            "Epoch 25:  85% 64/75 [02:10<00:22,  2.04s/it, loss=0.948, lr=2.9e-5]\n",
            "Epoch 25:  87% 65/75 [02:12<00:20,  2.04s/it, loss=0.948, lr=2.9e-5]\n",
            "Epoch 25:  88% 66/75 [02:14<00:18,  2.04s/it, loss=0.948, lr=2.9e-5]\n",
            "Epoch 25:  89% 67/75 [02:15<00:16,  2.02s/it, loss=0.948, lr=2.9e-5]\n",
            "Epoch 25:  91% 68/75 [02:16<00:14,  2.01s/it, loss=0.948, lr=2.9e-5]\n",
            "Epoch 25:  92% 69/75 [02:19<00:12,  2.03s/it, loss=0.948, lr=2.9e-5]\n",
            "Epoch 25:  93% 70/75 [02:24<00:10,  2.06s/it, loss=0.948, lr=2.9e-5]\n",
            "Epoch 25:  95% 71/75 [02:27<00:08,  2.07s/it, loss=0.948, lr=2.9e-5]\n",
            "Epoch 25:  96% 72/75 [02:28<00:06,  2.07s/it, loss=0.948, lr=2.9e-5]\n",
            "Epoch 25:  97% 73/75 [02:31<00:04,  2.07s/it, loss=0.948, lr=2.9e-5]\n",
            "Epoch 25:  99% 74/75 [02:32<00:02,  2.06s/it, loss=0.948, lr=2.9e-5]\n",
            "Epoch 25: 100% 75/75 [02:35<00:00,  2.07s/it, loss=0.948, lr=2.9e-5]2024-04-21 23:19:32 bleu - corpus_score: That's 100 lines that end in a tokenized period ('.')\n",
            "2024-04-21 23:19:32 bleu - corpus_score: It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2024-04-21 23:19:32 bleu - corpus_score: If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2024-04-21 23:20:14 train_table2text_t5 - validation_epoch_end: valid epoch: 25\n",
            "2024-04-21 23:20:14 train_table2text_t5 - validation_epoch_end: 25 bleu_info: 35.1892598355859\n",
            "2024-04-21 23:20:14 train_table2text_t5 - validation_epoch_end: 25 mover score: (0.41, 0.374)\n",
            "\n",
            "\n",
            "  0% 0/200 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            " 38% 75/200 [00:00<00:00, 743.20it/s]\u001b[A\u001b[A\n",
            "\n",
            "100% 200/200 [00:00<00:00, 529.22it/s]\n",
            "\n",
            "\n",
            "  0% 0/200 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "100% 200/200 [00:00<00:00, 1508.72it/s]\n",
            "2024-04-21 23:20:15 train_table2text_t5 - get_dataloader: loading dev dataloader...\n",
            "2024-04-21 23:20:15 train_table2text_t5 - get_dataloader: done\n",
            "2024-04-21 23:20:15 lightning_base - on_validation_end: ***** Validation results *****\n",
            "2024-04-21 23:20:15 lightning_base - on_validation_end: avg_val_loss = tensor(0.4191, device='cuda:0')\n",
            "2024-04-21 23:20:15 lightning_base - on_validation_end: epoch = 24\n",
            "2024-04-21 23:20:15 lightning_base - on_validation_end: loss = tensor(1.0360, device='cuda:0')\n",
            "2024-04-21 23:20:15 lightning_base - on_validation_end: step_count = 26\n",
            "2024-04-21 23:20:15 lightning_base - on_validation_end: train_loss = tensor(1.0360, device='cuda:0')\n",
            "2024-04-21 23:20:15 lightning_base - on_validation_end: val_avg_bleu = 35.1892598355859\n",
            "2024-04-21 23:20:15 lightning_base - on_validation_end: val_mover = tensor(0.4100, device='cuda:0')\n",
            "2024-04-21 23:20:15 lightning_base - on_validation_end: val_mover_mean1 = 0.41\n",
            "2024-04-21 23:20:15 lightning_base - on_validation_end: val_mover_median1 = 0.374\n",
            "Epoch 25: 100% 75/75 [03:19<00:00,  2.66s/it, loss=0.948, lr=2.9e-5]\n",
            "                                               \u001b[A\n",
            "  0% 0/200 [00:00<?, ?it/s]\u001b[A\n",
            " 34% 68/200 [00:00<00:00, 646.37it/s]\u001b[A\n",
            " 66% 133/200 [00:00<00:00, 553.19it/s]\u001b[A\n",
            "100% 200/200 [00:00<00:00, 497.12it/s]\n",
            "\n",
            "  0% 0/200 [00:00<?, ?it/s]\u001b[A\n",
            "100% 200/200 [00:00<00:00, 1566.41it/s]\n",
            "2024-04-21 23:20:36 train_table2text_t5 - get_dataloader: loading train dataloader...\n",
            "2024-04-21 23:20:36 train_table2text_t5 - get_dataloader: done\n",
            "Epoch 26:  33% 25/75 [00:20<00:41,  1.20it/s, loss=0.929, lr=2.9e-5]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 26:  35% 26/75 [00:24<00:45,  1.07it/s, loss=0.929, lr=2.9e-5]\n",
            "Epoch 26:  36% 27/75 [00:26<00:47,  1.01it/s, loss=0.929, lr=2.9e-5]\n",
            "Epoch 26:  37% 28/75 [00:28<00:47,  1.01s/it, loss=0.929, lr=2.9e-5]\n",
            "Epoch 26:  39% 29/75 [00:31<00:49,  1.07s/it, loss=0.929, lr=2.9e-5]\n",
            "Epoch 26:  40% 30/75 [00:34<00:52,  1.16s/it, loss=0.929, lr=2.9e-5]\n",
            "Epoch 26:  41% 31/75 [00:37<00:52,  1.20s/it, loss=0.929, lr=2.9e-5]\n",
            "Epoch 26:  43% 32/75 [00:39<00:53,  1.25s/it, loss=0.929, lr=2.9e-5]\n",
            "Epoch 26:  44% 33/75 [00:43<00:55,  1.31s/it, loss=0.929, lr=2.9e-5]\n",
            "Epoch 26:  45% 34/75 [00:44<00:54,  1.32s/it, loss=0.929, lr=2.9e-5]\n",
            "Epoch 26:  47% 35/75 [00:46<00:53,  1.33s/it, loss=0.929, lr=2.9e-5]\n",
            "Epoch 26:  48% 36/75 [00:50<00:54,  1.39s/it, loss=0.929, lr=2.9e-5]\n",
            "Epoch 26:  49% 37/75 [00:52<00:54,  1.42s/it, loss=0.929, lr=2.9e-5]\n",
            "Epoch 26:  51% 38/75 [00:55<00:53,  1.45s/it, loss=0.929, lr=2.9e-5]\n",
            "Epoch 26:  52% 39/75 [00:57<00:52,  1.47s/it, loss=0.929, lr=2.9e-5]\n",
            "Epoch 26:  53% 40/75 [00:59<00:52,  1.50s/it, loss=0.929, lr=2.9e-5]\n",
            "Epoch 26:  55% 41/75 [01:02<00:51,  1.53s/it, loss=0.929, lr=2.9e-5]\n",
            "Epoch 26:  56% 42/75 [01:03<00:50,  1.52s/it, loss=0.929, lr=2.9e-5]\n",
            "Epoch 26:  57% 43/75 [01:05<00:48,  1.52s/it, loss=0.929, lr=2.9e-5]\n",
            "Epoch 26:  59% 44/75 [01:08<00:48,  1.56s/it, loss=0.929, lr=2.9e-5]\n",
            "Epoch 26:  60% 45/75 [01:10<00:47,  1.58s/it, loss=0.929, lr=2.9e-5]\n",
            "Epoch 26:  61% 46/75 [01:13<00:46,  1.59s/it, loss=0.929, lr=2.9e-5]\n",
            "Epoch 26:  63% 47/75 [01:16<00:45,  1.63s/it, loss=0.929, lr=2.9e-5]\n",
            "Epoch 26:  64% 48/75 [01:19<00:44,  1.65s/it, loss=0.929, lr=2.9e-5]\n",
            "Epoch 26:  65% 49/75 [01:25<00:45,  1.75s/it, loss=0.929, lr=2.9e-5]\n",
            "Epoch 26:  67% 50/75 [01:30<00:45,  1.81s/it, loss=0.929, lr=2.9e-5]\n",
            "Epoch 26:  68% 51/75 [01:32<00:43,  1.81s/it, loss=0.929, lr=2.9e-5]\n",
            "Epoch 26:  69% 52/75 [01:33<00:41,  1.80s/it, loss=0.929, lr=2.9e-5]\n",
            "Epoch 26:  71% 53/75 [01:41<00:42,  1.91s/it, loss=0.929, lr=2.9e-5]\n",
            "Epoch 26:  72% 54/75 [01:43<00:40,  1.91s/it, loss=0.929, lr=2.9e-5]\n",
            "Epoch 26:  73% 55/75 [01:45<00:38,  1.91s/it, loss=0.929, lr=2.9e-5]\n",
            "Epoch 26:  75% 56/75 [01:48<00:36,  1.94s/it, loss=0.929, lr=2.9e-5]\n",
            "Epoch 26:  76% 57/75 [01:50<00:35,  1.95s/it, loss=0.929, lr=2.9e-5]\n",
            "Epoch 26:  77% 58/75 [01:52<00:32,  1.94s/it, loss=0.929, lr=2.9e-5]\n",
            "Epoch 26:  79% 59/75 [01:53<00:30,  1.93s/it, loss=0.929, lr=2.9e-5]\n",
            "Epoch 26:  80% 60/75 [01:57<00:29,  1.95s/it, loss=0.929, lr=2.9e-5]\n",
            "Epoch 26:  81% 61/75 [01:58<00:27,  1.95s/it, loss=0.929, lr=2.9e-5]\n",
            "Epoch 26:  83% 62/75 [02:00<00:25,  1.94s/it, loss=0.929, lr=2.9e-5]\n",
            "Epoch 26:  84% 63/75 [02:05<00:23,  1.99s/it, loss=0.929, lr=2.9e-5]\n",
            "Epoch 26:  85% 64/75 [02:08<00:22,  2.00s/it, loss=0.929, lr=2.9e-5]\n",
            "Epoch 26:  87% 65/75 [02:09<00:19,  1.99s/it, loss=0.929, lr=2.9e-5]\n",
            "Epoch 26:  88% 66/75 [02:11<00:17,  1.99s/it, loss=0.929, lr=2.9e-5]\n",
            "Epoch 26:  89% 67/75 [02:12<00:15,  1.98s/it, loss=0.929, lr=2.9e-5]\n",
            "Epoch 26:  91% 68/75 [02:14<00:13,  1.97s/it, loss=0.929, lr=2.9e-5]\n",
            "Epoch 26:  92% 69/75 [02:17<00:11,  1.99s/it, loss=0.929, lr=2.9e-5]\n",
            "Epoch 26:  93% 70/75 [02:20<00:10,  2.00s/it, loss=0.929, lr=2.9e-5]\n",
            "Epoch 26:  95% 71/75 [02:22<00:08,  2.01s/it, loss=0.929, lr=2.9e-5]\n",
            "Epoch 26:  96% 72/75 [02:24<00:06,  2.00s/it, loss=0.929, lr=2.9e-5]\n",
            "Epoch 26:  97% 73/75 [02:25<00:03,  2.00s/it, loss=0.929, lr=2.9e-5]\n",
            "Epoch 26:  99% 74/75 [02:28<00:02,  2.00s/it, loss=0.929, lr=2.9e-5]\n",
            "Epoch 26: 100% 75/75 [02:30<00:00,  2.01s/it, loss=0.929, lr=2.9e-5]2024-04-21 23:23:08 bleu - corpus_score: That's 100 lines that end in a tokenized period ('.')\n",
            "2024-04-21 23:23:08 bleu - corpus_score: It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2024-04-21 23:23:08 bleu - corpus_score: If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2024-04-21 23:23:49 train_table2text_t5 - validation_epoch_end: valid epoch: 26\n",
            "2024-04-21 23:23:49 train_table2text_t5 - validation_epoch_end: 26 bleu_info: 36.508838515234615\n",
            "2024-04-21 23:23:49 train_table2text_t5 - validation_epoch_end: 26 mover score: (0.44, 0.407)\n",
            "\n",
            "\n",
            "  0% 0/200 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "  8% 15/200 [00:00<00:01, 140.93it/s]\u001b[A\u001b[A\n",
            "\n",
            " 22% 44/200 [00:00<00:00, 224.55it/s]\u001b[A\u001b[A\n",
            "\n",
            " 34% 67/200 [00:00<00:00, 184.32it/s]\u001b[A\u001b[A\n",
            "\n",
            " 46% 93/200 [00:00<00:00, 202.88it/s]\u001b[A\u001b[A\n",
            "\n",
            " 57% 114/200 [00:00<00:00, 185.04it/s]\u001b[A\u001b[A\n",
            "\n",
            " 67% 134/200 [00:00<00:00, 185.86it/s]\u001b[A\u001b[A\n",
            "\n",
            " 76% 153/200 [00:00<00:00, 182.54it/s]\u001b[A\u001b[A\n",
            "\n",
            "100% 200/200 [00:00<00:00, 205.14it/s]\n",
            "\n",
            "\n",
            "  0% 0/200 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            " 30% 59/200 [00:00<00:00, 569.84it/s]\u001b[A\u001b[A\n",
            "\n",
            "100% 200/200 [00:00<00:00, 714.25it/s]\n",
            "2024-04-21 23:23:51 train_table2text_t5 - get_dataloader: loading dev dataloader...\n",
            "2024-04-21 23:23:51 train_table2text_t5 - get_dataloader: done\n",
            "2024-04-21 23:23:51 lightning_base - on_validation_end: ***** Validation results *****\n",
            "2024-04-21 23:23:51 lightning_base - on_validation_end: avg_val_loss = tensor(0.3751, device='cuda:0')\n",
            "2024-04-21 23:23:51 lightning_base - on_validation_end: epoch = 25\n",
            "2024-04-21 23:23:51 lightning_base - on_validation_end: loss = tensor(0.8595, device='cuda:0')\n",
            "2024-04-21 23:23:51 lightning_base - on_validation_end: step_count = 27\n",
            "2024-04-21 23:23:51 lightning_base - on_validation_end: train_loss = tensor(0.8595, device='cuda:0')\n",
            "2024-04-21 23:23:51 lightning_base - on_validation_end: val_avg_bleu = 36.508838515234615\n",
            "2024-04-21 23:23:51 lightning_base - on_validation_end: val_mover = tensor(0.4400, device='cuda:0')\n",
            "2024-04-21 23:23:51 lightning_base - on_validation_end: val_mover_mean1 = 0.44\n",
            "2024-04-21 23:23:51 lightning_base - on_validation_end: val_mover_median1 = 0.407\n",
            "Epoch 26: 100% 75/75 [03:14<00:00,  2.59s/it, loss=0.929, lr=2.9e-5]\n",
            "                                               \u001b[A\n",
            "  0% 0/200 [00:00<?, ?it/s]\u001b[A\n",
            " 34% 68/200 [00:00<00:00, 660.34it/s]\u001b[A\n",
            " 68% 135/200 [00:00<00:00, 566.78it/s]\u001b[A\n",
            "100% 200/200 [00:00<00:00, 498.16it/s]\n",
            "\n",
            "  0% 0/200 [00:00<?, ?it/s]\u001b[A\n",
            "100% 200/200 [00:00<00:00, 1688.01it/s]\n",
            "2024-04-21 23:24:09 train_table2text_t5 - get_dataloader: loading train dataloader...\n",
            "2024-04-21 23:24:09 train_table2text_t5 - get_dataloader: done\n",
            "Epoch 27:  33% 25/75 [00:21<00:42,  1.18it/s, loss=0.869, lr=2.9e-5]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 27:  35% 26/75 [00:24<00:45,  1.07it/s, loss=0.869, lr=2.9e-5]\n",
            "Epoch 27:  36% 27/75 [00:26<00:47,  1.01it/s, loss=0.869, lr=2.9e-5]\n",
            "Epoch 27:  37% 28/75 [00:28<00:48,  1.03s/it, loss=0.869, lr=2.9e-5]\n",
            "Epoch 27:  39% 29/75 [00:33<00:52,  1.15s/it, loss=0.869, lr=2.9e-5]\n",
            "Epoch 27:  40% 30/75 [00:37<00:56,  1.25s/it, loss=0.869, lr=2.9e-5]\n",
            "Epoch 27:  41% 31/75 [00:39<00:55,  1.27s/it, loss=0.869, lr=2.9e-5]\n",
            "Epoch 27:  43% 32/75 [00:40<00:55,  1.28s/it, loss=0.869, lr=2.9e-5]\n",
            "Epoch 27:  44% 33/75 [00:42<00:53,  1.28s/it, loss=0.869, lr=2.9e-5]\n",
            "Epoch 27:  45% 34/75 [00:43<00:52,  1.28s/it, loss=0.869, lr=2.9e-5]\n",
            "Epoch 27:  47% 35/75 [00:46<00:52,  1.32s/it, loss=0.869, lr=2.9e-5]\n",
            "Epoch 27:  48% 36/75 [00:48<00:52,  1.35s/it, loss=0.869, lr=2.9e-5]\n",
            "Epoch 27:  49% 37/75 [00:51<00:52,  1.38s/it, loss=0.869, lr=2.9e-5]\n",
            "Epoch 27:  51% 38/75 [00:54<00:52,  1.43s/it, loss=0.869, lr=2.9e-5]\n",
            "Epoch 27:  52% 39/75 [00:56<00:52,  1.46s/it, loss=0.869, lr=2.9e-5]\n",
            "Epoch 27:  53% 40/75 [01:00<00:52,  1.51s/it, loss=0.869, lr=2.9e-5]\n",
            "Epoch 27:  55% 41/75 [01:02<00:52,  1.53s/it, loss=0.869, lr=2.9e-5]\n",
            "Epoch 27:  56% 42/75 [01:04<00:50,  1.53s/it, loss=0.869, lr=2.9e-5]\n",
            "Epoch 27:  57% 43/75 [01:06<00:49,  1.54s/it, loss=0.869, lr=2.9e-5]\n",
            "Epoch 27:  59% 44/75 [01:09<00:49,  1.59s/it, loss=0.869, lr=2.9e-5]\n",
            "Epoch 27:  60% 45/75 [01:12<00:48,  1.62s/it, loss=0.869, lr=2.9e-5]\n",
            "Epoch 27:  61% 46/75 [01:14<00:46,  1.61s/it, loss=0.869, lr=2.9e-5]\n",
            "Epoch 27:  63% 47/75 [01:17<00:46,  1.65s/it, loss=0.869, lr=2.9e-5]\n",
            "Epoch 27:  64% 48/75 [01:20<00:45,  1.67s/it, loss=0.869, lr=2.9e-5]\n",
            "Epoch 27:  65% 49/75 [01:28<00:47,  1.81s/it, loss=0.869, lr=2.9e-5]\n",
            "Epoch 27:  67% 50/75 [01:32<00:46,  1.86s/it, loss=0.869, lr=2.9e-5]\n",
            "Epoch 27:  68% 51/75 [01:35<00:44,  1.87s/it, loss=0.869, lr=2.9e-5]\n",
            "Epoch 27:  69% 52/75 [01:36<00:42,  1.86s/it, loss=0.869, lr=2.9e-5]\n",
            "Epoch 27:  71% 53/75 [01:47<00:44,  2.02s/it, loss=0.869, lr=2.9e-5]\n",
            "Epoch 27:  72% 54/75 [01:50<00:42,  2.04s/it, loss=0.869, lr=2.9e-5]\n",
            "Epoch 27:  73% 55/75 [01:52<00:40,  2.04s/it, loss=0.869, lr=2.9e-5]\n",
            "Epoch 27:  75% 56/75 [01:55<00:39,  2.06s/it, loss=0.869, lr=2.9e-5]\n",
            "Epoch 27:  76% 57/75 [01:57<00:36,  2.05s/it, loss=0.869, lr=2.9e-5]\n",
            "Epoch 27:  77% 58/75 [01:58<00:34,  2.05s/it, loss=0.869, lr=2.9e-5]\n",
            "Epoch 27:  79% 59/75 [02:00<00:32,  2.05s/it, loss=0.869, lr=2.9e-5]\n",
            "Epoch 27:  80% 60/75 [02:04<00:31,  2.07s/it, loss=0.869, lr=2.9e-5]\n",
            "Epoch 27:  81% 61/75 [02:05<00:28,  2.06s/it, loss=0.869, lr=2.9e-5]\n",
            "Epoch 27:  83% 62/75 [02:07<00:26,  2.05s/it, loss=0.869, lr=2.9e-5]\n",
            "Epoch 27:  84% 63/75 [02:11<00:25,  2.09s/it, loss=0.869, lr=2.9e-5]\n",
            "Epoch 27:  85% 64/75 [02:15<00:23,  2.11s/it, loss=0.869, lr=2.9e-5]\n",
            "Epoch 27:  87% 65/75 [02:16<00:21,  2.10s/it, loss=0.869, lr=2.9e-5]\n",
            "Epoch 27:  88% 66/75 [02:17<00:18,  2.09s/it, loss=0.869, lr=2.9e-5]\n",
            "Epoch 27:  89% 67/75 [02:18<00:16,  2.07s/it, loss=0.869, lr=2.9e-5]\n",
            "Epoch 27:  91% 68/75 [02:19<00:14,  2.06s/it, loss=0.869, lr=2.9e-5]\n",
            "Epoch 27:  92% 69/75 [02:23<00:12,  2.07s/it, loss=0.869, lr=2.9e-5]\n",
            "Epoch 27:  93% 70/75 [02:26<00:10,  2.09s/it, loss=0.869, lr=2.9e-5]\n",
            "Epoch 27:  95% 71/75 [02:29<00:08,  2.10s/it, loss=0.869, lr=2.9e-5]\n",
            "Epoch 27:  96% 72/75 [02:30<00:06,  2.09s/it, loss=0.869, lr=2.9e-5]\n",
            "Epoch 27:  97% 73/75 [02:32<00:04,  2.09s/it, loss=0.869, lr=2.9e-5]\n",
            "Epoch 27:  99% 74/75 [02:33<00:02,  2.08s/it, loss=0.869, lr=2.9e-5]\n",
            "Epoch 27: 100% 75/75 [02:35<00:00,  2.08s/it, loss=0.869, lr=2.9e-5]2024-04-21 23:26:47 bleu - corpus_score: That's 100 lines that end in a tokenized period ('.')\n",
            "2024-04-21 23:26:47 bleu - corpus_score: It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2024-04-21 23:26:47 bleu - corpus_score: If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2024-04-21 23:27:32 train_table2text_t5 - validation_epoch_end: valid epoch: 27\n",
            "2024-04-21 23:27:32 train_table2text_t5 - validation_epoch_end: 27 bleu_info: 40.257979209231195\n",
            "2024-04-21 23:27:32 train_table2text_t5 - validation_epoch_end: 27 mover score: (0.467, 0.435)\n",
            "\n",
            "\n",
            "  0% 0/200 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            " 38% 75/200 [00:00<00:00, 744.70it/s]\u001b[A\u001b[A\n",
            "\n",
            "100% 200/200 [00:00<00:00, 529.34it/s]\n",
            "\n",
            "\n",
            "  0% 0/200 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "100% 200/200 [00:00<00:00, 1925.49it/s]\n",
            "2024-04-21 23:27:32 train_table2text_t5 - get_dataloader: loading dev dataloader...\n",
            "2024-04-21 23:27:32 train_table2text_t5 - get_dataloader: done\n",
            "2024-04-21 23:27:32 lightning_base - on_validation_end: ***** Validation results *****\n",
            "2024-04-21 23:27:32 lightning_base - on_validation_end: avg_val_loss = tensor(0.3328, device='cuda:0')\n",
            "2024-04-21 23:27:32 lightning_base - on_validation_end: epoch = 26\n",
            "2024-04-21 23:27:32 lightning_base - on_validation_end: loss = tensor(0.9758, device='cuda:0')\n",
            "2024-04-21 23:27:32 lightning_base - on_validation_end: step_count = 28\n",
            "2024-04-21 23:27:32 lightning_base - on_validation_end: train_loss = tensor(0.9758, device='cuda:0')\n",
            "2024-04-21 23:27:32 lightning_base - on_validation_end: val_avg_bleu = 40.257979209231195\n",
            "2024-04-21 23:27:32 lightning_base - on_validation_end: val_mover = tensor(0.4670, device='cuda:0')\n",
            "2024-04-21 23:27:32 lightning_base - on_validation_end: val_mover_mean1 = 0.467\n",
            "2024-04-21 23:27:32 lightning_base - on_validation_end: val_mover_median1 = 0.435\n",
            "Epoch 27: 100% 75/75 [03:23<00:00,  2.71s/it, loss=0.869, lr=2.9e-5]\n",
            "                                               \u001b[A\n",
            "  0% 0/200 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 58/200 [00:00<00:00, 575.85it/s]\u001b[A\n",
            " 58% 116/200 [00:00<00:00, 520.55it/s]\u001b[A\n",
            "100% 200/200 [00:00<00:00, 467.17it/s]\n",
            "\n",
            "  0% 0/200 [00:00<?, ?it/s]\u001b[A\n",
            "100% 200/200 [00:00<00:00, 1701.63it/s]\n",
            "2024-04-21 23:27:44 train_table2text_t5 - get_dataloader: loading train dataloader...\n",
            "2024-04-21 23:27:44 train_table2text_t5 - get_dataloader: done\n",
            "Epoch 28:  33% 25/75 [00:21<00:42,  1.18it/s, loss=0.824, lr=2.9e-5]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 28:  35% 26/75 [00:23<00:45,  1.08it/s, loss=0.824, lr=2.9e-5]\n",
            "Epoch 28:  36% 27/75 [00:26<00:47,  1.02it/s, loss=0.824, lr=2.9e-5]\n",
            "Epoch 28:  37% 28/75 [00:28<00:47,  1.00s/it, loss=0.824, lr=2.9e-5]\n",
            "Epoch 28:  39% 29/75 [00:30<00:49,  1.07s/it, loss=0.824, lr=2.9e-5]\n",
            "Epoch 28:  40% 30/75 [00:33<00:49,  1.11s/it, loss=0.824, lr=2.9e-5]\n",
            "Epoch 28:  41% 31/75 [00:35<00:49,  1.14s/it, loss=0.824, lr=2.9e-5]\n",
            "Epoch 28:  43% 32/75 [00:36<00:49,  1.15s/it, loss=0.824, lr=2.9e-5]\n",
            "Epoch 28:  44% 33/75 [00:37<00:48,  1.15s/it, loss=0.824, lr=2.9e-5]\n",
            "Epoch 28:  45% 34/75 [00:39<00:47,  1.15s/it, loss=0.824, lr=2.9e-5]\n",
            "Epoch 28:  47% 35/75 [00:40<00:46,  1.16s/it, loss=0.824, lr=2.9e-5]\n",
            "Epoch 28:  48% 36/75 [00:44<00:47,  1.23s/it, loss=0.824, lr=2.9e-5]\n",
            "Epoch 28:  49% 37/75 [00:46<00:47,  1.26s/it, loss=0.824, lr=2.9e-5]\n",
            "Epoch 28:  51% 38/75 [00:48<00:47,  1.28s/it, loss=0.824, lr=2.9e-5]\n",
            "Epoch 28:  52% 39/75 [00:50<00:46,  1.30s/it, loss=0.824, lr=2.9e-5]\n",
            "Epoch 28:  53% 40/75 [00:52<00:46,  1.32s/it, loss=0.824, lr=2.9e-5]\n",
            "Epoch 28:  55% 41/75 [00:56<00:46,  1.37s/it, loss=0.824, lr=2.9e-5]\n",
            "Epoch 28:  56% 42/75 [00:57<00:45,  1.36s/it, loss=0.824, lr=2.9e-5]\n",
            "Epoch 28:  57% 43/75 [00:59<00:44,  1.39s/it, loss=0.824, lr=2.9e-5]\n",
            "Epoch 28:  59% 44/75 [01:02<00:43,  1.42s/it, loss=0.824, lr=2.9e-5]\n",
            "Epoch 28:  60% 45/75 [01:04<00:42,  1.43s/it, loss=0.824, lr=2.9e-5]\n",
            "Epoch 28:  61% 46/75 [01:06<00:41,  1.43s/it, loss=0.824, lr=2.9e-5]\n",
            "Epoch 28:  63% 47/75 [01:09<00:41,  1.48s/it, loss=0.824, lr=2.9e-5]\n",
            "Epoch 28:  64% 48/75 [01:11<00:40,  1.49s/it, loss=0.824, lr=2.9e-5]\n",
            "Epoch 28:  65% 49/75 [01:16<00:40,  1.57s/it, loss=0.824, lr=2.9e-5]\n",
            "Epoch 28:  67% 50/75 [01:21<00:40,  1.63s/it, loss=0.824, lr=2.9e-5]\n",
            "Epoch 28:  68% 51/75 [01:23<00:39,  1.63s/it, loss=0.824, lr=2.9e-5]\n",
            "Epoch 28:  69% 52/75 [01:24<00:37,  1.62s/it, loss=0.824, lr=2.9e-5]\n",
            "Epoch 28:  71% 53/75 [01:28<00:36,  1.68s/it, loss=0.824, lr=2.9e-5]\n",
            "Epoch 28:  72% 54/75 [01:31<00:35,  1.69s/it, loss=0.824, lr=2.9e-5]\n",
            "Epoch 28:  73% 55/75 [01:34<00:34,  1.72s/it, loss=0.824, lr=2.9e-5]\n",
            "Epoch 28:  75% 56/75 [01:37<00:33,  1.75s/it, loss=0.824, lr=2.9e-5]\n",
            "Epoch 28:  76% 57/75 [01:39<00:31,  1.75s/it, loss=0.824, lr=2.9e-5]\n",
            "Epoch 28:  77% 58/75 [01:41<00:29,  1.75s/it, loss=0.824, lr=2.9e-5]\n",
            "Epoch 28:  79% 59/75 [01:42<00:27,  1.73s/it, loss=0.824, lr=2.9e-5]\n",
            "Epoch 28:  80% 60/75 [01:45<00:26,  1.75s/it, loss=0.824, lr=2.9e-5]\n",
            "Epoch 28:  81% 61/75 [01:46<00:24,  1.75s/it, loss=0.824, lr=2.9e-5]\n",
            "Epoch 28:  83% 62/75 [01:48<00:22,  1.75s/it, loss=0.824, lr=2.9e-5]\n",
            "Epoch 28:  84% 63/75 [01:51<00:21,  1.77s/it, loss=0.824, lr=2.9e-5]\n",
            "Epoch 28:  85% 64/75 [01:53<00:19,  1.78s/it, loss=0.824, lr=2.9e-5]\n",
            "Epoch 28:  87% 65/75 [01:54<00:17,  1.77s/it, loss=0.824, lr=2.9e-5]\n",
            "Epoch 28:  88% 66/75 [01:56<00:15,  1.76s/it, loss=0.824, lr=2.9e-5]\n",
            "Epoch 28:  89% 67/75 [01:57<00:14,  1.76s/it, loss=0.824, lr=2.9e-5]\n",
            "Epoch 28:  91% 68/75 [01:59<00:12,  1.75s/it, loss=0.824, lr=2.9e-5]\n",
            "Epoch 28:  92% 69/75 [02:02<00:10,  1.78s/it, loss=0.824, lr=2.9e-5]\n",
            "Epoch 28:  93% 70/75 [02:04<00:08,  1.79s/it, loss=0.824, lr=2.9e-5]\n",
            "Epoch 28:  95% 71/75 [02:06<00:07,  1.79s/it, loss=0.824, lr=2.9e-5]\n",
            "Epoch 28:  96% 72/75 [02:08<00:05,  1.79s/it, loss=0.824, lr=2.9e-5]\n",
            "Epoch 28:  97% 73/75 [02:11<00:03,  1.80s/it, loss=0.824, lr=2.9e-5]\n",
            "Epoch 28:  99% 74/75 [02:13<00:01,  1.80s/it, loss=0.824, lr=2.9e-5]\n",
            "Epoch 28: 100% 75/75 [02:15<00:00,  1.80s/it, loss=0.824, lr=2.9e-5]2024-04-21 23:30:00 bleu - corpus_score: That's 100 lines that end in a tokenized period ('.')\n",
            "2024-04-21 23:30:00 bleu - corpus_score: It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2024-04-21 23:30:00 bleu - corpus_score: If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2024-04-21 23:30:30 train_table2text_t5 - validation_epoch_end: valid epoch: 28\n",
            "2024-04-21 23:30:30 train_table2text_t5 - validation_epoch_end: 28 bleu_info: 39.34291579978816\n",
            "2024-04-21 23:30:30 train_table2text_t5 - validation_epoch_end: 28 mover score: (0.491, 0.48)\n",
            "\n",
            "\n",
            "  0% 0/200 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            " 19% 38/200 [00:00<00:00, 368.09it/s]\u001b[A\u001b[A\n",
            "\n",
            " 42% 84/200 [00:00<00:00, 419.27it/s]\u001b[A\u001b[A\n",
            "\n",
            " 63% 126/200 [00:00<00:00, 319.12it/s]\u001b[A\u001b[A\n",
            "\n",
            " 80% 161/200 [00:00<00:00, 282.78it/s]\u001b[A\u001b[A\n",
            "\n",
            "100% 200/200 [00:00<00:00, 307.56it/s]\n",
            "\n",
            "\n",
            "  0% 0/200 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "100% 200/200 [00:00<00:00, 999.50it/s]\n",
            "2024-04-21 23:30:31 train_table2text_t5 - get_dataloader: loading dev dataloader...\n",
            "2024-04-21 23:30:31 train_table2text_t5 - get_dataloader: done\n",
            "2024-04-21 23:30:31 lightning_base - on_validation_end: ***** Validation results *****\n",
            "2024-04-21 23:30:31 lightning_base - on_validation_end: avg_val_loss = tensor(0.2990, device='cuda:0')\n",
            "2024-04-21 23:30:31 lightning_base - on_validation_end: epoch = 27\n",
            "2024-04-21 23:30:31 lightning_base - on_validation_end: loss = tensor(0.8340, device='cuda:0')\n",
            "2024-04-21 23:30:31 lightning_base - on_validation_end: step_count = 29\n",
            "2024-04-21 23:30:31 lightning_base - on_validation_end: train_loss = tensor(0.8340, device='cuda:0')\n",
            "2024-04-21 23:30:31 lightning_base - on_validation_end: val_avg_bleu = 39.34291579978816\n",
            "2024-04-21 23:30:31 lightning_base - on_validation_end: val_mover = tensor(0.4910, device='cuda:0')\n",
            "2024-04-21 23:30:31 lightning_base - on_validation_end: val_mover_mean1 = 0.491\n",
            "2024-04-21 23:30:31 lightning_base - on_validation_end: val_mover_median1 = 0.48\n",
            "Epoch 28: 100% 75/75 [02:47<00:00,  2.23s/it, loss=0.824, lr=2.9e-5]\n",
            "                                               \u001b[A\n",
            "  0% 0/200 [00:00<?, ?it/s]\u001b[A\n",
            " 20% 40/200 [00:00<00:00, 398.56it/s]\u001b[A\n",
            " 52% 103/200 [00:00<00:00, 525.31it/s]\u001b[A\n",
            "100% 200/200 [00:00<00:00, 456.05it/s]\n",
            "\n",
            "  0% 0/200 [00:00<?, ?it/s]\u001b[A\n",
            "100% 200/200 [00:00<00:00, 1392.08it/s]\n",
            "2024-04-21 23:30:47 train_table2text_t5 - get_dataloader: loading train dataloader...\n",
            "2024-04-21 23:30:47 train_table2text_t5 - get_dataloader: done\n",
            "Epoch 29:  33% 25/75 [00:21<00:43,  1.16it/s, loss=0.732, lr=2.9e-5]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 29:  35% 26/75 [00:24<00:45,  1.07it/s, loss=0.732, lr=2.9e-5]\n",
            "Epoch 29:  36% 27/75 [00:26<00:47,  1.00it/s, loss=0.732, lr=2.9e-5]\n",
            "Epoch 29:  37% 28/75 [00:29<00:48,  1.04s/it, loss=0.732, lr=2.9e-5]\n",
            "Epoch 29:  39% 29/75 [00:31<00:50,  1.09s/it, loss=0.732, lr=2.9e-5]\n",
            "Epoch 29:  40% 30/75 [00:36<00:54,  1.22s/it, loss=0.732, lr=2.9e-5]\n",
            "Epoch 29:  41% 31/75 [00:38<00:54,  1.25s/it, loss=0.732, lr=2.9e-5]\n",
            "Epoch 29:  43% 32/75 [00:41<00:55,  1.28s/it, loss=0.732, lr=2.9e-5]\n",
            "Epoch 29:  44% 33/75 [00:44<00:56,  1.34s/it, loss=0.732, lr=2.9e-5]\n",
            "Epoch 29:  45% 34/75 [00:46<00:55,  1.36s/it, loss=0.732, lr=2.9e-5]\n",
            "Epoch 29:  47% 35/75 [00:48<00:55,  1.38s/it, loss=0.732, lr=2.9e-5]\n",
            "Epoch 29:  48% 36/75 [00:50<00:55,  1.42s/it, loss=0.732, lr=2.9e-5]\n",
            "Epoch 29:  49% 37/75 [00:53<00:55,  1.45s/it, loss=0.732, lr=2.9e-5]\n",
            "Epoch 29:  51% 38/75 [00:56<00:55,  1.49s/it, loss=0.732, lr=2.9e-5]\n",
            "Epoch 29:  52% 39/75 [01:00<00:55,  1.55s/it, loss=0.732, lr=2.9e-5]\n",
            "Epoch 29:  53% 40/75 [01:02<00:54,  1.56s/it, loss=0.732, lr=2.9e-5]\n",
            "Epoch 29:  55% 41/75 [01:04<00:53,  1.57s/it, loss=0.732, lr=2.9e-5]\n",
            "Epoch 29:  56% 42/75 [01:06<00:52,  1.58s/it, loss=0.732, lr=2.9e-5]\n",
            "Epoch 29:  57% 43/75 [01:09<00:51,  1.61s/it, loss=0.732, lr=2.9e-5]\n",
            "Epoch 29:  59% 44/75 [01:13<00:51,  1.67s/it, loss=0.732, lr=2.9e-5]\n",
            "Epoch 29:  60% 45/75 [01:15<00:50,  1.68s/it, loss=0.732, lr=2.9e-5]\n",
            "Epoch 29:  61% 46/75 [01:17<00:49,  1.70s/it, loss=0.732, lr=2.9e-5]\n",
            "Epoch 29:  63% 47/75 [01:21<00:48,  1.74s/it, loss=0.732, lr=2.9e-5]\n",
            "Epoch 29:  64% 48/75 [01:25<00:48,  1.78s/it, loss=0.732, lr=2.9e-5]\n",
            "Epoch 29:  65% 49/75 [01:31<00:48,  1.86s/it, loss=0.732, lr=2.9e-5]\n",
            "Epoch 29:  67% 50/75 [01:34<00:47,  1.89s/it, loss=0.732, lr=2.9e-5]\n",
            "Epoch 29:  68% 51/75 [01:36<00:45,  1.90s/it, loss=0.732, lr=2.9e-5]\n",
            "Epoch 29:  69% 52/75 [01:38<00:43,  1.89s/it, loss=0.732, lr=2.9e-5]\n",
            "Epoch 29:  71% 53/75 [01:50<00:45,  2.08s/it, loss=0.732, lr=2.9e-5]\n",
            "Epoch 29:  72% 54/75 [01:53<00:44,  2.10s/it, loss=0.732, lr=2.9e-5]\n",
            "Epoch 29:  73% 55/75 [01:55<00:42,  2.10s/it, loss=0.732, lr=2.9e-5]\n",
            "Epoch 29:  75% 56/75 [01:59<00:40,  2.13s/it, loss=0.732, lr=2.9e-5]\n",
            "Epoch 29:  76% 57/75 [02:01<00:38,  2.14s/it, loss=0.732, lr=2.9e-5]\n",
            "Epoch 29:  77% 58/75 [02:03<00:36,  2.13s/it, loss=0.732, lr=2.9e-5]\n",
            "Epoch 29:  79% 59/75 [02:04<00:33,  2.11s/it, loss=0.732, lr=2.9e-5]\n",
            "Epoch 29:  80% 60/75 [02:09<00:32,  2.15s/it, loss=0.732, lr=2.9e-5]\n",
            "Epoch 29:  81% 61/75 [02:10<00:30,  2.14s/it, loss=0.732, lr=2.9e-5]\n",
            "Epoch 29:  83% 62/75 [02:12<00:27,  2.14s/it, loss=0.732, lr=2.9e-5]\n",
            "Epoch 29:  84% 63/75 [02:16<00:25,  2.16s/it, loss=0.732, lr=2.9e-5]\n",
            "Epoch 29:  85% 64/75 [02:19<00:23,  2.18s/it, loss=0.732, lr=2.9e-5]\n",
            "Epoch 29:  87% 65/75 [02:21<00:21,  2.17s/it, loss=0.732, lr=2.9e-5]\n",
            "Epoch 29:  88% 66/75 [02:22<00:19,  2.16s/it, loss=0.732, lr=2.9e-5]\n",
            "Epoch 29:  89% 67/75 [02:23<00:17,  2.15s/it, loss=0.732, lr=2.9e-5]\n",
            "Epoch 29:  91% 68/75 [02:25<00:14,  2.14s/it, loss=0.732, lr=2.9e-5]\n",
            "Epoch 29:  92% 69/75 [02:30<00:13,  2.18s/it, loss=0.732, lr=2.9e-5]\n",
            "Epoch 29:  93% 70/75 [02:32<00:10,  2.18s/it, loss=0.732, lr=2.9e-5]\n",
            "Epoch 29:  95% 71/75 [02:35<00:08,  2.19s/it, loss=0.732, lr=2.9e-5]\n",
            "Epoch 29:  96% 72/75 [02:36<00:06,  2.18s/it, loss=0.732, lr=2.9e-5]\n",
            "Epoch 29:  97% 73/75 [02:39<00:04,  2.19s/it, loss=0.732, lr=2.9e-5]\n",
            "Epoch 29:  99% 74/75 [02:41<00:02,  2.18s/it, loss=0.732, lr=2.9e-5]\n",
            "Epoch 29: 100% 75/75 [02:43<00:00,  2.19s/it, loss=0.732, lr=2.9e-5]2024-04-21 23:33:32 bleu - corpus_score: That's 100 lines that end in a tokenized period ('.')\n",
            "2024-04-21 23:33:32 bleu - corpus_score: It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2024-04-21 23:33:32 bleu - corpus_score: If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2024-04-21 23:34:19 train_table2text_t5 - validation_epoch_end: valid epoch: 29\n",
            "2024-04-21 23:34:19 train_table2text_t5 - validation_epoch_end: 29 bleu_info: 48.53436454259714\n",
            "2024-04-21 23:34:19 train_table2text_t5 - validation_epoch_end: 29 mover score: (0.54, 0.535)\n",
            "\n",
            "\n",
            "  0% 0/200 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            " 37% 74/200 [00:00<00:00, 739.28it/s]\u001b[A\u001b[A\n",
            "\n",
            "100% 200/200 [00:00<00:00, 515.63it/s]\n",
            "\n",
            "\n",
            "  0% 0/200 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "100% 200/200 [00:00<00:00, 1747.39it/s]\n",
            "2024-04-21 23:34:20 train_table2text_t5 - get_dataloader: loading dev dataloader...\n",
            "2024-04-21 23:34:20 train_table2text_t5 - get_dataloader: done\n",
            "2024-04-21 23:34:20 lightning_base - on_validation_end: ***** Validation results *****\n",
            "2024-04-21 23:34:20 lightning_base - on_validation_end: avg_val_loss = tensor(0.2588, device='cuda:0')\n",
            "2024-04-21 23:34:20 lightning_base - on_validation_end: epoch = 28\n",
            "2024-04-21 23:34:20 lightning_base - on_validation_end: loss = tensor(0.6849, device='cuda:0')\n",
            "2024-04-21 23:34:20 lightning_base - on_validation_end: step_count = 30\n",
            "2024-04-21 23:34:20 lightning_base - on_validation_end: train_loss = tensor(0.6849, device='cuda:0')\n",
            "2024-04-21 23:34:20 lightning_base - on_validation_end: val_avg_bleu = 48.53436454259714\n",
            "2024-04-21 23:34:20 lightning_base - on_validation_end: val_mover = tensor(0.5400, device='cuda:0')\n",
            "2024-04-21 23:34:20 lightning_base - on_validation_end: val_mover_mean1 = 0.54\n",
            "2024-04-21 23:34:20 lightning_base - on_validation_end: val_mover_median1 = 0.535\n",
            "Epoch 29: 100% 75/75 [03:33<00:00,  2.84s/it, loss=0.732, lr=2.9e-5]\n",
            "                                               \u001b[A\n",
            "  0% 0/200 [00:00<?, ?it/s]\u001b[A\n",
            " 14% 28/200 [00:00<00:00, 262.41it/s]\u001b[A\n",
            " 33% 66/200 [00:00<00:00, 328.41it/s]\u001b[A\n",
            " 50% 100/200 [00:00<00:00, 332.74it/s]\u001b[A\n",
            " 67% 134/200 [00:00<00:00, 289.94it/s]\u001b[A\n",
            " 82% 164/200 [00:00<00:00, 245.51it/s]\u001b[A\n",
            "100% 200/200 [00:00<00:00, 275.00it/s]\n",
            "\n",
            "  0% 0/200 [00:00<?, ?it/s]\u001b[A\n",
            " 48% 97/200 [00:00<00:00, 961.96it/s]\u001b[A\n",
            "100% 200/200 [00:00<00:00, 890.10it/s]\n",
            "2024-04-21 23:34:38 train_table2text_t5 - get_dataloader: loading train dataloader...\n",
            "2024-04-21 23:34:38 train_table2text_t5 - get_dataloader: done\n",
            "Epoch 30:  33% 25/75 [00:21<00:43,  1.14it/s, loss=0.676, lr=2.9e-5]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 30:  35% 26/75 [00:25<00:47,  1.03it/s, loss=0.676, lr=2.9e-5]\n",
            "Epoch 30:  36% 27/75 [00:27<00:48,  1.02s/it, loss=0.676, lr=2.9e-5]\n",
            "Epoch 30:  37% 28/75 [00:29<00:48,  1.04s/it, loss=0.676, lr=2.9e-5]\n",
            "Epoch 30:  39% 29/75 [00:31<00:50,  1.10s/it, loss=0.676, lr=2.9e-5]\n",
            "Epoch 30:  40% 30/75 [00:36<00:55,  1.23s/it, loss=0.676, lr=2.9e-5]\n",
            "Epoch 30:  41% 31/75 [00:39<00:56,  1.29s/it, loss=0.676, lr=2.9e-5]\n",
            "Epoch 30:  43% 32/75 [00:42<00:57,  1.33s/it, loss=0.676, lr=2.9e-5]\n",
            "Epoch 30:  44% 33/75 [00:44<00:56,  1.35s/it, loss=0.676, lr=2.9e-5]\n",
            "Epoch 30:  45% 34/75 [00:46<00:55,  1.36s/it, loss=0.676, lr=2.9e-5]\n",
            "Epoch 30:  47% 35/75 [00:48<00:55,  1.38s/it, loss=0.676, lr=2.9e-5]\n",
            "Epoch 30:  48% 36/75 [00:52<00:56,  1.45s/it, loss=0.676, lr=2.9e-5]\n",
            "Epoch 30:  49% 37/75 [00:54<00:55,  1.47s/it, loss=0.676, lr=2.9e-5]\n",
            "Epoch 30:  51% 38/75 [00:56<00:55,  1.50s/it, loss=0.676, lr=2.9e-5]\n",
            "Epoch 30:  52% 39/75 [00:59<00:54,  1.51s/it, loss=0.676, lr=2.9e-5]\n",
            "Epoch 30:  53% 40/75 [01:01<00:53,  1.54s/it, loss=0.676, lr=2.9e-5]\n",
            "Epoch 30:  55% 41/75 [01:04<00:53,  1.57s/it, loss=0.676, lr=2.9e-5]\n",
            "Epoch 30:  56% 42/75 [01:05<00:51,  1.57s/it, loss=0.676, lr=2.9e-5]\n",
            "Epoch 30:  57% 43/75 [01:07<00:50,  1.58s/it, loss=0.676, lr=2.9e-5]\n",
            "Epoch 30:  59% 44/75 [01:10<00:49,  1.61s/it, loss=0.676, lr=2.9e-5]\n",
            "Epoch 30:  60% 45/75 [01:12<00:48,  1.61s/it, loss=0.676, lr=2.9e-5]\n",
            "Epoch 30:  61% 46/75 [01:15<00:47,  1.63s/it, loss=0.676, lr=2.9e-5]\n",
            "Epoch 30:  63% 47/75 [01:18<00:46,  1.67s/it, loss=0.676, lr=2.9e-5]\n",
            "Epoch 30:  64% 48/75 [01:21<00:45,  1.70s/it, loss=0.676, lr=2.9e-5]\n",
            "Epoch 30:  65% 49/75 [01:26<00:46,  1.77s/it, loss=0.676, lr=2.9e-5]\n",
            "Epoch 30:  67% 50/75 [01:34<00:47,  1.90s/it, loss=0.676, lr=2.9e-5]\n",
            "Epoch 30:  68% 51/75 [01:36<00:45,  1.89s/it, loss=0.676, lr=2.9e-5]\n",
            "Epoch 30:  69% 52/75 [01:37<00:43,  1.88s/it, loss=0.676, lr=2.9e-5]\n",
            "Epoch 30:  71% 53/75 [01:47<00:44,  2.02s/it, loss=0.676, lr=2.9e-5]\n",
            "Epoch 30:  72% 54/75 [01:49<00:42,  2.02s/it, loss=0.676, lr=2.9e-5]\n",
            "Epoch 30:  73% 55/75 [01:51<00:40,  2.03s/it, loss=0.676, lr=2.9e-5]\n",
            "Epoch 30:  75% 56/75 [01:55<00:39,  2.06s/it, loss=0.676, lr=2.9e-5]\n",
            "Epoch 30:  76% 57/75 [01:56<00:36,  2.05s/it, loss=0.676, lr=2.9e-5]\n",
            "Epoch 30:  77% 58/75 [01:58<00:34,  2.05s/it, loss=0.676, lr=2.9e-5]\n",
            "Epoch 30:  79% 59/75 [01:59<00:32,  2.03s/it, loss=0.676, lr=2.9e-5]\n",
            "Epoch 30:  80% 60/75 [02:02<00:30,  2.05s/it, loss=0.676, lr=2.9e-5]\n",
            "Epoch 30:  81% 61/75 [02:04<00:28,  2.05s/it, loss=0.676, lr=2.9e-5]\n",
            "Epoch 30:  83% 62/75 [02:07<00:26,  2.05s/it, loss=0.676, lr=2.9e-5]\n",
            "Epoch 30:  84% 63/75 [02:10<00:24,  2.07s/it, loss=0.676, lr=2.9e-5]\n",
            "Epoch 30:  85% 64/75 [02:12<00:22,  2.08s/it, loss=0.676, lr=2.9e-5]\n",
            "Epoch 30:  87% 65/75 [02:14<00:20,  2.06s/it, loss=0.676, lr=2.9e-5]\n",
            "Epoch 30:  88% 66/75 [02:15<00:18,  2.06s/it, loss=0.676, lr=2.9e-5]\n",
            "Epoch 30:  89% 67/75 [02:17<00:16,  2.05s/it, loss=0.676, lr=2.9e-5]\n",
            "Epoch 30:  91% 68/75 [02:19<00:14,  2.05s/it, loss=0.676, lr=2.9e-5]\n",
            "Epoch 30:  92% 69/75 [02:23<00:12,  2.08s/it, loss=0.676, lr=2.9e-5]\n",
            "Epoch 30:  93% 70/75 [02:26<00:10,  2.10s/it, loss=0.676, lr=2.9e-5]\n",
            "Epoch 30:  95% 71/75 [02:29<00:08,  2.11s/it, loss=0.676, lr=2.9e-5]\n",
            "Epoch 30:  96% 72/75 [02:31<00:06,  2.10s/it, loss=0.676, lr=2.9e-5]\n",
            "Epoch 30:  97% 73/75 [02:34<00:04,  2.11s/it, loss=0.676, lr=2.9e-5]\n",
            "Epoch 30:  99% 74/75 [02:36<00:02,  2.11s/it, loss=0.676, lr=2.9e-5]\n",
            "Epoch 30: 100% 75/75 [02:37<00:00,  2.10s/it, loss=0.676, lr=2.9e-5]2024-04-21 23:37:17 bleu - corpus_score: That's 100 lines that end in a tokenized period ('.')\n",
            "2024-04-21 23:37:17 bleu - corpus_score: It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2024-04-21 23:37:17 bleu - corpus_score: If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2024-04-21 23:37:57 train_table2text_t5 - validation_epoch_end: valid epoch: 30\n",
            "2024-04-21 23:37:57 train_table2text_t5 - validation_epoch_end: 30 bleu_info: 49.023521212087736\n",
            "2024-04-21 23:37:57 train_table2text_t5 - validation_epoch_end: 30 mover score: (0.563, 0.57)\n",
            "\n",
            "\n",
            "  0% 0/200 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            " 19% 38/200 [00:00<00:00, 375.46it/s]\u001b[A\u001b[A\n",
            "\n",
            " 38% 76/200 [00:00<00:00, 371.88it/s]\u001b[A\u001b[A\n",
            "\n",
            " 57% 114/200 [00:00<00:00, 278.54it/s]\u001b[A\u001b[A\n",
            "\n",
            " 72% 145/200 [00:00<00:00, 158.87it/s]\u001b[A\u001b[A\n",
            "\n",
            " 84% 168/200 [00:00<00:00, 171.30it/s]\u001b[A\u001b[A\n",
            "\n",
            "100% 200/200 [00:00<00:00, 212.66it/s]\n",
            "\n",
            "\n",
            "  0% 0/200 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            " 44% 88/200 [00:00<00:00, 874.71it/s]\u001b[A\u001b[A\n",
            "\n",
            "100% 200/200 [00:00<00:00, 917.90it/s]\n",
            "2024-04-21 23:37:58 train_table2text_t5 - get_dataloader: loading dev dataloader...\n",
            "2024-04-21 23:37:58 train_table2text_t5 - get_dataloader: done\n",
            "2024-04-21 23:37:58 lightning_base - on_validation_end: ***** Validation results *****\n",
            "2024-04-21 23:37:58 lightning_base - on_validation_end: avg_val_loss = tensor(0.2319, device='cuda:0')\n",
            "2024-04-21 23:37:58 lightning_base - on_validation_end: epoch = 29\n",
            "2024-04-21 23:37:58 lightning_base - on_validation_end: loss = tensor(0.6240, device='cuda:0')\n",
            "2024-04-21 23:37:58 lightning_base - on_validation_end: step_count = 31\n",
            "2024-04-21 23:37:58 lightning_base - on_validation_end: train_loss = tensor(0.6240, device='cuda:0')\n",
            "2024-04-21 23:37:58 lightning_base - on_validation_end: val_avg_bleu = 49.023521212087736\n",
            "2024-04-21 23:37:58 lightning_base - on_validation_end: val_mover = tensor(0.5630, device='cuda:0')\n",
            "2024-04-21 23:37:58 lightning_base - on_validation_end: val_mover_mean1 = 0.563\n",
            "2024-04-21 23:37:58 lightning_base - on_validation_end: val_mover_median1 = 0.57\n",
            "Epoch 30: 100% 75/75 [03:20<00:00,  2.67s/it, loss=0.676, lr=2.9e-5]\n",
            "Epoch 30: 100% 75/75 [03:34<00:00,  2.86s/it, loss=0.676, lr=2.9e-5]\n",
            "Loading weights from output_train_large/val_mover=0.56-step_count=31.ckpt\n",
            "2024-04-21 23:38:13 connectionpool - _new_conn: Starting new HTTPS connection (1): s3.amazonaws.com:443\n",
            "2024-04-21 23:38:14 connectionpool - _make_request: https://s3.amazonaws.com:443 \"HEAD /models.huggingface.co/bert/facebook/bart-base/config.json HTTP/1.1\" 200 0\n",
            "2024-04-21 23:38:14 configuration_utils - get_config_dict: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/config.json from cache at /root/.cache/torch/transformers/09f4fcaeaf785dd3b97b085d6e3510c7081f586ec8e75981683c6299c0f81d9d.e8d516ad807436d395effad8c2326786872659b7dd1210827ac67c761198a0eb\n",
            "2024-04-21 23:38:14 configuration_utils - from_dict: Model config BartConfig {\n",
            "  \"activation_dropout\": 0.1,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"add_bias_logits\": false,\n",
            "  \"add_final_layer_norm\": false,\n",
            "  \"architectures\": [\n",
            "    \"BartModel\",\n",
            "    \"BartForConditionalGeneration\",\n",
            "    \"BartForSequenceClassification\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classif_dropout\": 0.0,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_attention_heads\": 12,\n",
            "  \"decoder_ffn_dim\": 3072,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 6,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"dropout\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_heads\": 12,\n",
            "  \"encoder_ffn_dim\": 3072,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 6,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"max_position_embeddings\": 1024,\n",
            "  \"model_type\": \"bart\",\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"normalize_before\": false,\n",
            "  \"normalize_embedding\": true,\n",
            "  \"num_beams\": 4,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"scale_embedding\": false,\n",
            "  \"static_position_embeddings\": false,\n",
            "  \"task_specific_params\": {\n",
            "    \"summarization\": {\n",
            "      \"length_penalty\": 1.0,\n",
            "      \"max_length\": 128,\n",
            "      \"min_length\": 12,\n",
            "      \"num_beams\": 4\n",
            "    },\n",
            "    \"summarization_cnn\": {\n",
            "      \"length_penalty\": 2.0,\n",
            "      \"max_length\": 142,\n",
            "      \"min_length\": 56,\n",
            "      \"num_beams\": 4\n",
            "    },\n",
            "    \"summarization_xsum\": {\n",
            "      \"length_penalty\": 1.0,\n",
            "      \"max_length\": 62,\n",
            "      \"min_length\": 11,\n",
            "      \"num_beams\": 6\n",
            "    }\n",
            "  },\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "2024-04-21 23:38:14 connectionpool - _new_conn: Starting new HTTPS connection (1): s3.amazonaws.com:443\n",
            "2024-04-21 23:38:14 connectionpool - _make_request: https://s3.amazonaws.com:443 \"HEAD /models.huggingface.co/bert/facebook/bart-base/config.json HTTP/1.1\" 200 0\n",
            "2024-04-21 23:38:14 configuration_utils - get_config_dict: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/config.json from cache at /root/.cache/torch/transformers/09f4fcaeaf785dd3b97b085d6e3510c7081f586ec8e75981683c6299c0f81d9d.e8d516ad807436d395effad8c2326786872659b7dd1210827ac67c761198a0eb\n",
            "2024-04-21 23:38:14 configuration_utils - from_dict: Model config BartConfig {\n",
            "  \"activation_dropout\": 0.1,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"add_bias_logits\": false,\n",
            "  \"add_final_layer_norm\": false,\n",
            "  \"architectures\": [\n",
            "    \"BartModel\",\n",
            "    \"BartForConditionalGeneration\",\n",
            "    \"BartForSequenceClassification\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classif_dropout\": 0.0,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_attention_heads\": 12,\n",
            "  \"decoder_ffn_dim\": 3072,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 6,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"dropout\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_heads\": 12,\n",
            "  \"encoder_ffn_dim\": 3072,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 6,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"max_position_embeddings\": 1024,\n",
            "  \"model_type\": \"bart\",\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"normalize_before\": false,\n",
            "  \"normalize_embedding\": true,\n",
            "  \"num_beams\": 4,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"scale_embedding\": false,\n",
            "  \"static_position_embeddings\": false,\n",
            "  \"task_specific_params\": {\n",
            "    \"summarization\": {\n",
            "      \"length_penalty\": 1.0,\n",
            "      \"max_length\": 128,\n",
            "      \"min_length\": 12,\n",
            "      \"num_beams\": 4\n",
            "    },\n",
            "    \"summarization_cnn\": {\n",
            "      \"length_penalty\": 2.0,\n",
            "      \"max_length\": 142,\n",
            "      \"min_length\": 56,\n",
            "      \"num_beams\": 4\n",
            "    },\n",
            "    \"summarization_xsum\": {\n",
            "      \"length_penalty\": 1.0,\n",
            "      \"max_length\": 62,\n",
            "      \"min_length\": 11,\n",
            "      \"num_beams\": 6\n",
            "    }\n",
            "  },\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "2024-04-21 23:38:14 tokenization_utils - _from_pretrained: Model name 'facebook/bart-base' not found in model shortcut name list (bart-large, bart-large-mnli, bart-large-cnn, bart-large-xsum). Assuming 'facebook/bart-base' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "2024-04-21 23:38:14 connectionpool - _new_conn: Starting new HTTPS connection (1): s3.amazonaws.com:443\n",
            "2024-04-21 23:38:14 connectionpool - _make_request: https://s3.amazonaws.com:443 \"HEAD /models.huggingface.co/bert/facebook/bart-base/vocab.json HTTP/1.1\" 200 0\n",
            "2024-04-21 23:38:14 connectionpool - _new_conn: Starting new HTTPS connection (1): s3.amazonaws.com:443\n",
            "2024-04-21 23:38:15 connectionpool - _make_request: https://s3.amazonaws.com:443 \"HEAD /models.huggingface.co/bert/facebook/bart-base/merges.txt HTTP/1.1\" 200 0\n",
            "2024-04-21 23:38:15 connectionpool - _new_conn: Starting new HTTPS connection (1): s3.amazonaws.com:443\n",
            "2024-04-21 23:38:15 connectionpool - _make_request: https://s3.amazonaws.com:443 \"HEAD /models.huggingface.co/bert/facebook/bart-base/added_tokens.json HTTP/1.1\" 404 0\n",
            "2024-04-21 23:38:15 connectionpool - _new_conn: Starting new HTTPS connection (1): s3.amazonaws.com:443\n",
            "2024-04-21 23:38:15 connectionpool - _make_request: https://s3.amazonaws.com:443 \"HEAD /models.huggingface.co/bert/facebook/bart-base/special_tokens_map.json HTTP/1.1\" 404 0\n",
            "2024-04-21 23:38:15 connectionpool - _new_conn: Starting new HTTPS connection (1): s3.amazonaws.com:443\n",
            "2024-04-21 23:38:15 connectionpool - _make_request: https://s3.amazonaws.com:443 \"HEAD /models.huggingface.co/bert/facebook/bart-base/tokenizer_config.json HTTP/1.1\" 404 0\n",
            "2024-04-21 23:38:15 tokenization_utils - _from_pretrained: loading file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/vocab.json from cache at /root/.cache/torch/transformers/7a7fc1746df9ea150ffd06a23351e5854c2105db3a0be1e0307dfd74fbf4a65c.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b\n",
            "2024-04-21 23:38:15 tokenization_utils - _from_pretrained: loading file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/merges.txt from cache at /root/.cache/torch/transformers/3d0d1735635ef097afbf08cf5af21618c94286b8e8b53cfb9bd6cdcfc91d4e14.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
            "2024-04-21 23:38:15 tokenization_utils - _from_pretrained: loading file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/added_tokens.json from cache at None\n",
            "2024-04-21 23:38:15 tokenization_utils - _from_pretrained: loading file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/special_tokens_map.json from cache at None\n",
            "2024-04-21 23:38:15 tokenization_utils - _from_pretrained: loading file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/tokenizer_config.json from cache at None\n",
            "2024-04-21 23:38:15 tokenization_utils - add_tokens: Adding [R] to the vocabulary\n",
            "2024-04-21 23:38:15 tokenization_utils - add_tokens: Adding [C] to the vocabulary\n",
            "2024-04-21 23:38:15 tokenization_utils - add_tokens: Adding [CAP] to the vocabulary\n",
            "2024-04-21 23:38:15 connectionpool - _new_conn: Starting new HTTPS connection (1): cdn.huggingface.co:443\n",
            "2024-04-21 23:38:16 connectionpool - _make_request: https://cdn.huggingface.co:443 \"HEAD /facebook/bart-base/pytorch_model.bin HTTP/1.1\" 200 0\n",
            "2024-04-21 23:38:16 modeling_utils - from_pretrained: loading weights file https://cdn.huggingface.co/facebook/bart-base/pytorch_model.bin from cache at /root/.cache/torch/transformers/566c05fb6983817e8ad7a4fa51e3099fe9caa3b31730f964bc5198d71c677523.0a3d95c18c1e434448941bc25accea7b122882be6526fb67c8e8fb6d5ebc711c\n",
            "2024-04-21 23:38:23 lightning_base - __init__: We have added 3 tokens\n",
            "2024-04-21 23:38:24 train_table2text_t5 - __init__: parameters {'output_dir': 'output_train_large', 'fp16': False, 'fp16_opt_level': 'O1', 'n_gpu': 1, 'n_tpu_cores': 0, 'max_grad_norm': 1.0, 'do_train': True, 'do_predict': True, 'gradient_accumulation_steps': 1, 'seed': 42, 'model_name_or_path': 'facebook/bart-base', 'config_name': '', 'tokenizer_name': '', 'cache_dir': '', 'learning_rate': 3e-05, 'weight_decay': 0.0, 'adam_epsilon': 1e-08, 'warmup_steps': 0, 'num_train_epochs': 30, 'train_batch_size': 8, 'eval_batch_size': 4, 'test_batch_size': 4, 'max_source_length': 384, 'max_target_length': 384, 'data_dir': 'data_few_shot', 'early_stopping_patience': 10, 'checkpoint': None, 'checkpoint_model': None}\n",
            "100% 200/200 [00:00<00:00, 307.00it/s]\n",
            "100% 200/200 [00:00<00:00, 1031.91it/s]\n",
            "2024-04-21 23:38:26 train_table2text_t5 - get_dataloader: loading test dataloader...\n",
            "2024-04-21 23:38:26 train_table2text_t5 - get_dataloader: done\n",
            "Testing: 100% 50/50 [02:11<00:00,  2.12s/it]Traceback (most recent call last):\n",
            "  File \"train_table2text_t5.py\", line 345, in <module>\n",
            "    main(args)\n",
            "  File \"train_table2text_t5.py\", line 334, in main\n",
            "    trainer.test(model)\n",
            "  File \"/usr/local/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py\", line 1179, in test\n",
            "    self.fit(model)\n",
            "  File \"/usr/local/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py\", line 918, in fit\n",
            "    self.single_gpu_train(model)\n",
            "  File \"/usr/local/lib/python3.6/site-packages/pytorch_lightning/trainer/distrib_parts.py\", line 176, in single_gpu_train\n",
            "    self.run_pretrain_routine(model)\n",
            "  File \"/usr/local/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py\", line 1055, in run_pretrain_routine\n",
            "    self.run_evaluation(test_mode=True)\n",
            "  File \"/usr/local/lib/python3.6/site-packages/pytorch_lightning/trainer/evaluation_loop.py\", line 379, in run_evaluation\n",
            "    eval_results = self._evaluate(self.model, dataloaders, max_batches, test_mode)\n",
            "  File \"/usr/local/lib/python3.6/site-packages/pytorch_lightning/trainer/evaluation_loop.py\", line 320, in _evaluate\n",
            "    eval_results = model.test_epoch_end(outputs)\n",
            "  File \"train_table2text_t5.py\", line 134, in test_epoch_end\n",
            "    with open(output_test_predictions_file, \"w\") as p_writer, open(output_test_targets_file, \"w\") as t_writer:\n",
            "FileNotFoundError: [Errno 2] No such file or directory: 'output_train_large/test_predictions_0.txt'\n",
            "Testing: 100%|██████████| 50/50 [02:13<00:00,  2.67s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git add ."
      ],
      "metadata": {
        "id": "s9GXdr1u6bug",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66ed0a65-554f-4f97-eff5-13f2d2b46873"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "4HMAAIdUN71H",
        "outputId": "cbdb7bdb-f6ab-4727-eb66-874d257acfd9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "#drive.mount('/content/drive')\n",
        "\n",
        "# Create a folder in the root directory\n",
        "#!mkdir -p \"/content/drive/My Drive/NLP/baselines/output_train_few_shot\"\n",
        "\n",
        "import shutil\n",
        "\n",
        "source_dir = r\"/content/SciGen/baselines/output_train_few_shot_t5\"\n",
        "destination_dir = r\"/content/drive/My Drive/NLP/SciGen/baselines/output_train_few_shot_t5\"\n",
        "shutil.copytree(source_dir, destination_dir)\n"
      ],
      "metadata": {
        "id": "6uYZBMmwOVCE",
        "outputId": "2a159e35-fc8a-4731-ba85-5b3f8fbfe1b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/My Drive/NLP/SciGen/baselines/output_train_few_shot_t5'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Hzp4MAOWDaE0",
        "outputId": "ab2aaa78-3bba-4742-bd4d-1b900c754b42",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34mbleurt\u001b[0m/                evaluate.py                \u001b[01;34m__pycache__\u001b[0m/              utils.py\n",
            "callbacks.py           lightning_base.py          README.md\n",
            "convert_json_files.py  \u001b[01;34mlightning_logs\u001b[0m/            train_table2text_bart.py\n",
            "\u001b[01;34mdata_few_shot\u001b[0m/         \u001b[01;34moutput_train_few_shot_t5\u001b[0m/  train_table2text_t5.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python evaluate.py \\\n",
        "--pred=output_train_few_shot_t5/validation_predictions_30.txt \\\n",
        "--sys=output_train_few_shot_t5/validation_targets_30.txt"
      ],
      "metadata": {
        "id": "rM1AcK6OAh5O",
        "outputId": "a308b449-82df-43a2-889e-2dbcdd295063",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"evaluate.py\", line 6, in <module>\n",
            "    from moverscore_v2 import get_idf_dict, word_mover_score\n",
            "  File \"/usr/local/lib/python3.6/site-packages/moverscore_v2.py\", line 5, in <module>\n",
            "    from pyemd import emd, emd_with_flow\n",
            "  File \"/usr/local/lib/python3.6/site-packages/pyemd/__init__.py\", line 75, in <module>\n",
            "    from .emd import emd, emd_with_flow, emd_samples\n",
            "ImportError: /usr/local/lib/python3.6/site-packages/pyemd/emd.cpython-36m-x86_64-linux-gnu.so: undefined symbol: _ZSt28__throw_bad_array_new_lengthv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install nltk"
      ],
      "metadata": {
        "id": "xuU9QovxAjoV",
        "outputId": "27c3230c-057b-4bd0-e1ba-f90d076e470e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nltk\n",
            "  Downloading https://files.pythonhosted.org/packages/c5/ea/84c7247f5c96c5a1b619fe822fb44052081ccfbe487a49d4c888306adec7/nltk-3.6.7-py3-none-any.whl (1.5MB)\n",
            "\u001b[K    100% |████████████████████████████████| 1.5MB 918kB/s \n",
            "\u001b[?25hRequirement already satisfied: click in /usr/local/lib/python3.6/site-packages (from nltk)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/site-packages (from nltk)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.6/site-packages (from nltk)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/site-packages (from nltk)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/site-packages (from click->nltk)\n",
            "Requirement already satisfied: importlib-resources; python_version < \"3.7\" in /usr/local/lib/python3.6/site-packages (from tqdm->nltk)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/site-packages (from importlib-metadata; python_version < \"3.8\"->click->nltk)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/site-packages (from importlib-metadata; python_version < \"3.8\"->click->nltk)\n",
            "Installing collected packages: nltk\n",
            "Successfully installed nltk-3.6.7\n"
          ]
        }
      ]
    }
  ]
}